{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0c6083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import recall_score, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4431b769",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\BlackFlameSG\\Documents\\Python Scripts\\ML\\Project\\SVM_Numerical\\HR_comma_sep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a564d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['satisfaction_level', 'last_evaluation', 'number_project',\n",
      "       'average_montly_hours', 'time_spend_company', 'Work_accident', 'left',\n",
      "       'promotion_last_5years', 'Department', 'salary'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14999 entries, 0 to 14998\n",
      "Data columns (total 10 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   satisfaction_level     14999 non-null  float64\n",
      " 1   last_evaluation        14999 non-null  float64\n",
      " 2   number_project         14999 non-null  int64  \n",
      " 3   average_montly_hours   14999 non-null  int64  \n",
      " 4   time_spend_company     14999 non-null  int64  \n",
      " 5   Work_accident          14999 non-null  int64  \n",
      " 6   left                   14999 non-null  int64  \n",
      " 7   promotion_last_5years  14999 non-null  int64  \n",
      " 8   Department             14999 non-null  object \n",
      " 9   salary                 14999 non-null  object \n",
      "dtypes: float64(2), int64(6), object(2)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4511646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>left</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>Department</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.76</td>\n",
       "      <td>4</td>\n",
       "      <td>158</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>IT</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.77</td>\n",
       "      <td>4</td>\n",
       "      <td>180</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.73</td>\n",
       "      <td>4</td>\n",
       "      <td>247</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>management</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.89</td>\n",
       "      <td>5</td>\n",
       "      <td>229</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>support</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.39</td>\n",
       "      <td>5</td>\n",
       "      <td>198</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RandD</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14994</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.83</td>\n",
       "      <td>5</td>\n",
       "      <td>171</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>technical</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.49</td>\n",
       "      <td>2</td>\n",
       "      <td>147</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.57</td>\n",
       "      <td>5</td>\n",
       "      <td>183</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.80</td>\n",
       "      <td>3</td>\n",
       "      <td>232</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>accounting</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.75</td>\n",
       "      <td>4</td>\n",
       "      <td>255</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14999 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       satisfaction_level  last_evaluation  number_project  \\\n",
       "0                    0.96             0.76               4   \n",
       "1                    0.50             0.77               4   \n",
       "2                    0.59             0.73               4   \n",
       "3                    0.74             0.89               5   \n",
       "4                    0.43             0.39               5   \n",
       "...                   ...              ...             ...   \n",
       "14994                0.14             0.83               5   \n",
       "14995                0.41             0.49               2   \n",
       "14996                0.44             0.57               5   \n",
       "14997                0.54             0.80               3   \n",
       "14998                0.58             0.75               4   \n",
       "\n",
       "       average_montly_hours  time_spend_company  Work_accident  left  \\\n",
       "0                       158                   3              0     0   \n",
       "1                       180                   3              0     0   \n",
       "2                       247                  10              0     0   \n",
       "3                       229                   6              0     1   \n",
       "4                       198                   5              0     0   \n",
       "...                     ...                 ...            ...   ...   \n",
       "14994                   171                   6              0     0   \n",
       "14995                   147                   3              0     1   \n",
       "14996                   183                   2              1     0   \n",
       "14997                   232                   2              0     0   \n",
       "14998                   255                   3              0     0   \n",
       "\n",
       "       promotion_last_5years  Department  salary  \n",
       "0                          0          IT     low  \n",
       "1                          0       sales  medium  \n",
       "2                          0  management    high  \n",
       "3                          0     support     low  \n",
       "4                          0       RandD  medium  \n",
       "...                      ...         ...     ...  \n",
       "14994                      0   technical  medium  \n",
       "14995                      0       sales     low  \n",
       "14996                      0       sales     low  \n",
       "14997                      0  accounting    high  \n",
       "14998                      0       sales  medium  \n",
       "\n",
       "[14999 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1, random_state=27).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcbe7d8f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['low' 'medium' 'high']\n",
      "['IT' 'sales' 'management' 'support' 'RandD' 'technical' 'hr' 'marketing'\n",
      " 'product_mng' 'accounting']\n"
     ]
    }
   ],
   "source": [
    "print(df['salary'].unique())\n",
    "print(df['Department'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77345d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace({'salary' :{'low':0,'medium':1,'high':2}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2f64f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(df['salary'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38673c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>left</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>salary</th>\n",
       "      <th>Department_IT</th>\n",
       "      <th>Department_RandD</th>\n",
       "      <th>Department_accounting</th>\n",
       "      <th>Department_hr</th>\n",
       "      <th>Department_management</th>\n",
       "      <th>Department_marketing</th>\n",
       "      <th>Department_product_mng</th>\n",
       "      <th>Department_sales</th>\n",
       "      <th>Department_support</th>\n",
       "      <th>Department_technical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.76</td>\n",
       "      <td>4</td>\n",
       "      <td>158</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.77</td>\n",
       "      <td>4</td>\n",
       "      <td>180</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.73</td>\n",
       "      <td>4</td>\n",
       "      <td>247</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.89</td>\n",
       "      <td>5</td>\n",
       "      <td>229</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.39</td>\n",
       "      <td>5</td>\n",
       "      <td>198</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14994</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.83</td>\n",
       "      <td>5</td>\n",
       "      <td>171</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.49</td>\n",
       "      <td>2</td>\n",
       "      <td>147</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.57</td>\n",
       "      <td>5</td>\n",
       "      <td>183</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.80</td>\n",
       "      <td>3</td>\n",
       "      <td>232</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.75</td>\n",
       "      <td>4</td>\n",
       "      <td>255</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14999 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       satisfaction_level  last_evaluation  number_project  \\\n",
       "0                    0.96             0.76               4   \n",
       "1                    0.50             0.77               4   \n",
       "2                    0.59             0.73               4   \n",
       "3                    0.74             0.89               5   \n",
       "4                    0.43             0.39               5   \n",
       "...                   ...              ...             ...   \n",
       "14994                0.14             0.83               5   \n",
       "14995                0.41             0.49               2   \n",
       "14996                0.44             0.57               5   \n",
       "14997                0.54             0.80               3   \n",
       "14998                0.58             0.75               4   \n",
       "\n",
       "       average_montly_hours  time_spend_company  Work_accident  left  \\\n",
       "0                       158                   3              0     0   \n",
       "1                       180                   3              0     0   \n",
       "2                       247                  10              0     0   \n",
       "3                       229                   6              0     1   \n",
       "4                       198                   5              0     0   \n",
       "...                     ...                 ...            ...   ...   \n",
       "14994                   171                   6              0     0   \n",
       "14995                   147                   3              0     1   \n",
       "14996                   183                   2              1     0   \n",
       "14997                   232                   2              0     0   \n",
       "14998                   255                   3              0     0   \n",
       "\n",
       "       promotion_last_5years  salary  Department_IT  Department_RandD  \\\n",
       "0                          0       0              1                 0   \n",
       "1                          0       1              0                 0   \n",
       "2                          0       2              0                 0   \n",
       "3                          0       0              0                 0   \n",
       "4                          0       1              0                 1   \n",
       "...                      ...     ...            ...               ...   \n",
       "14994                      0       1              0                 0   \n",
       "14995                      0       0              0                 0   \n",
       "14996                      0       0              0                 0   \n",
       "14997                      0       2              0                 0   \n",
       "14998                      0       1              0                 0   \n",
       "\n",
       "       Department_accounting  Department_hr  Department_management  \\\n",
       "0                          0              0                      0   \n",
       "1                          0              0                      0   \n",
       "2                          0              0                      1   \n",
       "3                          0              0                      0   \n",
       "4                          0              0                      0   \n",
       "...                      ...            ...                    ...   \n",
       "14994                      0              0                      0   \n",
       "14995                      0              0                      0   \n",
       "14996                      0              0                      0   \n",
       "14997                      1              0                      0   \n",
       "14998                      0              0                      0   \n",
       "\n",
       "       Department_marketing  Department_product_mng  Department_sales  \\\n",
       "0                         0                       0                 0   \n",
       "1                         0                       0                 1   \n",
       "2                         0                       0                 0   \n",
       "3                         0                       0                 0   \n",
       "4                         0                       0                 0   \n",
       "...                     ...                     ...               ...   \n",
       "14994                     0                       0                 0   \n",
       "14995                     0                       0                 1   \n",
       "14996                     0                       0                 1   \n",
       "14997                     0                       0                 0   \n",
       "14998                     0                       0                 1   \n",
       "\n",
       "       Department_support  Department_technical  \n",
       "0                       0                     0  \n",
       "1                       0                     0  \n",
       "2                       0                     0  \n",
       "3                       1                     0  \n",
       "4                       0                     0  \n",
       "...                   ...                   ...  \n",
       "14994                   0                     1  \n",
       "14995                   0                     0  \n",
       "14996                   0                     0  \n",
       "14997                   0                     0  \n",
       "14998                   0                     0  \n",
       "\n",
       "[14999 rows x 19 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.get_dummies(df, drop_first=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4ffae0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['satisfaction_level', 'last_evaluation', 'number_project',\n",
       "       'average_montly_hours', 'time_spend_company', 'Work_accident', 'left',\n",
       "       'promotion_last_5years', 'salary', 'Department_IT', 'Department_RandD',\n",
       "       'Department_accounting', 'Department_hr', 'Department_management',\n",
       "       'Department_marketing', 'Department_product_mng', 'Department_sales',\n",
       "       'Department_support', 'Department_technical'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb5a4bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Department_RandD', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "327b19fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['satisfaction_level', 'last_evaluation', 'number_project',\n",
    "        'average_montly_hours', 'time_spend_company', 'Work_accident',\n",
    "        'promotion_last_5years', 'salary', 'Department_IT', 'Department_accounting',\n",
    "        'Department_hr', 'Department_management', 'Department_marketing',\n",
    "        'Department_product_mng', 'Department_sales', 'Department_support',\n",
    "        'Department_technical', 'left']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70b1f9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>salary</th>\n",
       "      <th>Department_IT</th>\n",
       "      <th>Department_accounting</th>\n",
       "      <th>Department_hr</th>\n",
       "      <th>Department_management</th>\n",
       "      <th>Department_marketing</th>\n",
       "      <th>Department_product_mng</th>\n",
       "      <th>Department_sales</th>\n",
       "      <th>Department_support</th>\n",
       "      <th>Department_technical</th>\n",
       "      <th>left</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.76</td>\n",
       "      <td>4</td>\n",
       "      <td>158</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.77</td>\n",
       "      <td>4</td>\n",
       "      <td>180</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.73</td>\n",
       "      <td>4</td>\n",
       "      <td>247</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.89</td>\n",
       "      <td>5</td>\n",
       "      <td>229</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.39</td>\n",
       "      <td>5</td>\n",
       "      <td>198</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   satisfaction_level  last_evaluation  number_project  average_montly_hours  \\\n",
       "0                0.96             0.76               4                   158   \n",
       "1                0.50             0.77               4                   180   \n",
       "2                0.59             0.73               4                   247   \n",
       "3                0.74             0.89               5                   229   \n",
       "4                0.43             0.39               5                   198   \n",
       "\n",
       "   time_spend_company  Work_accident  promotion_last_5years  salary  \\\n",
       "0                   3              0                      0       0   \n",
       "1                   3              0                      0       1   \n",
       "2                  10              0                      0       2   \n",
       "3                   6              0                      0       0   \n",
       "4                   5              0                      0       1   \n",
       "\n",
       "   Department_IT  Department_accounting  Department_hr  Department_management  \\\n",
       "0              1                      0              0                      0   \n",
       "1              0                      0              0                      0   \n",
       "2              0                      0              0                      1   \n",
       "3              0                      0              0                      0   \n",
       "4              0                      0              0                      0   \n",
       "\n",
       "   Department_marketing  Department_product_mng  Department_sales  \\\n",
       "0                     0                       0                 0   \n",
       "1                     0                       0                 1   \n",
       "2                     0                       0                 0   \n",
       "3                     0                       0                 0   \n",
       "4                     0                       0                 0   \n",
       "\n",
       "   Department_support  Department_technical  left  \n",
       "0                   0                     0     0  \n",
       "1                   0                     0     0  \n",
       "2                   0                     0     0  \n",
       "3                   1                     0     1  \n",
       "4                   0                     0     0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[cols]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d849159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>salary</th>\n",
       "      <th>Department_IT</th>\n",
       "      <th>Department_accounting</th>\n",
       "      <th>Department_hr</th>\n",
       "      <th>Department_management</th>\n",
       "      <th>Department_marketing</th>\n",
       "      <th>Department_product_mng</th>\n",
       "      <th>Department_sales</th>\n",
       "      <th>Department_support</th>\n",
       "      <th>Department_technical</th>\n",
       "      <th>left</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>satisfaction_level</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.105021</td>\n",
       "      <td>-0.142970</td>\n",
       "      <td>-0.020048</td>\n",
       "      <td>-0.100866</td>\n",
       "      <td>0.058697</td>\n",
       "      <td>0.025605</td>\n",
       "      <td>0.050022</td>\n",
       "      <td>0.006373</td>\n",
       "      <td>-0.028649</td>\n",
       "      <td>-0.012841</td>\n",
       "      <td>0.007172</td>\n",
       "      <td>0.005715</td>\n",
       "      <td>0.006919</td>\n",
       "      <td>0.004007</td>\n",
       "      <td>0.009185</td>\n",
       "      <td>-0.009345</td>\n",
       "      <td>-0.388375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_evaluation</th>\n",
       "      <td>0.105021</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.349333</td>\n",
       "      <td>0.339742</td>\n",
       "      <td>0.131591</td>\n",
       "      <td>-0.007104</td>\n",
       "      <td>-0.008684</td>\n",
       "      <td>-0.013002</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>-0.009645</td>\n",
       "      <td>0.009662</td>\n",
       "      <td>-0.000311</td>\n",
       "      <td>-0.001989</td>\n",
       "      <td>-0.023031</td>\n",
       "      <td>0.017104</td>\n",
       "      <td>0.013742</td>\n",
       "      <td>0.006567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_project</th>\n",
       "      <td>-0.142970</td>\n",
       "      <td>0.349333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.417211</td>\n",
       "      <td>0.196786</td>\n",
       "      <td>-0.004741</td>\n",
       "      <td>-0.006064</td>\n",
       "      <td>-0.001803</td>\n",
       "      <td>0.003287</td>\n",
       "      <td>0.004189</td>\n",
       "      <td>-0.027356</td>\n",
       "      <td>0.009728</td>\n",
       "      <td>-0.023064</td>\n",
       "      <td>0.000829</td>\n",
       "      <td>-0.013388</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.028596</td>\n",
       "      <td>0.023787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_montly_hours</th>\n",
       "      <td>-0.020048</td>\n",
       "      <td>0.339742</td>\n",
       "      <td>0.417211</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.127755</td>\n",
       "      <td>-0.010143</td>\n",
       "      <td>-0.003544</td>\n",
       "      <td>-0.002242</td>\n",
       "      <td>0.006967</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>-0.010783</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>-0.008210</td>\n",
       "      <td>-0.005494</td>\n",
       "      <td>-0.001718</td>\n",
       "      <td>-0.002444</td>\n",
       "      <td>0.013638</td>\n",
       "      <td>0.071287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_spend_company</th>\n",
       "      <td>-0.100866</td>\n",
       "      <td>0.131591</td>\n",
       "      <td>0.196786</td>\n",
       "      <td>0.127755</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002120</td>\n",
       "      <td>0.067433</td>\n",
       "      <td>0.048715</td>\n",
       "      <td>-0.006053</td>\n",
       "      <td>0.003909</td>\n",
       "      <td>-0.022194</td>\n",
       "      <td>0.115436</td>\n",
       "      <td>0.012096</td>\n",
       "      <td>-0.003919</td>\n",
       "      <td>0.015150</td>\n",
       "      <td>-0.030111</td>\n",
       "      <td>-0.027991</td>\n",
       "      <td>0.144822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Work_accident</th>\n",
       "      <td>0.058697</td>\n",
       "      <td>-0.007104</td>\n",
       "      <td>-0.004741</td>\n",
       "      <td>-0.010143</td>\n",
       "      <td>0.002120</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.039245</td>\n",
       "      <td>0.009247</td>\n",
       "      <td>-0.009293</td>\n",
       "      <td>-0.012836</td>\n",
       "      <td>-0.015649</td>\n",
       "      <td>0.011242</td>\n",
       "      <td>0.011367</td>\n",
       "      <td>0.001246</td>\n",
       "      <td>-0.004955</td>\n",
       "      <td>0.012079</td>\n",
       "      <td>-0.006070</td>\n",
       "      <td>-0.154622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <td>0.025605</td>\n",
       "      <td>-0.008684</td>\n",
       "      <td>-0.006064</td>\n",
       "      <td>-0.003544</td>\n",
       "      <td>0.067433</td>\n",
       "      <td>0.039245</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.098119</td>\n",
       "      <td>-0.038942</td>\n",
       "      <td>-0.004852</td>\n",
       "      <td>-0.001531</td>\n",
       "      <td>0.128087</td>\n",
       "      <td>0.049253</td>\n",
       "      <td>-0.037288</td>\n",
       "      <td>0.012353</td>\n",
       "      <td>-0.035605</td>\n",
       "      <td>-0.035799</td>\n",
       "      <td>-0.061788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salary</th>\n",
       "      <td>0.050022</td>\n",
       "      <td>-0.013002</td>\n",
       "      <td>-0.001803</td>\n",
       "      <td>-0.002242</td>\n",
       "      <td>0.048715</td>\n",
       "      <td>0.009247</td>\n",
       "      <td>0.098119</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.010959</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>0.004599</td>\n",
       "      <td>0.156665</td>\n",
       "      <td>0.011599</td>\n",
       "      <td>-0.007669</td>\n",
       "      <td>-0.035599</td>\n",
       "      <td>-0.029888</td>\n",
       "      <td>-0.018630</td>\n",
       "      <td>-0.157898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Department_IT</th>\n",
       "      <td>0.006373</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>0.003287</td>\n",
       "      <td>0.006967</td>\n",
       "      <td>-0.006053</td>\n",
       "      <td>-0.009293</td>\n",
       "      <td>-0.038942</td>\n",
       "      <td>-0.010959</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.069293</td>\n",
       "      <td>-0.067949</td>\n",
       "      <td>-0.062500</td>\n",
       "      <td>-0.073524</td>\n",
       "      <td>-0.075503</td>\n",
       "      <td>-0.184302</td>\n",
       "      <td>-0.124705</td>\n",
       "      <td>-0.140484</td>\n",
       "      <td>-0.010925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Department_accounting</th>\n",
       "      <td>-0.028649</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.004189</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.003909</td>\n",
       "      <td>-0.012836</td>\n",
       "      <td>-0.004852</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>-0.069293</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.052848</td>\n",
       "      <td>-0.048610</td>\n",
       "      <td>-0.057183</td>\n",
       "      <td>-0.058723</td>\n",
       "      <td>-0.143341</td>\n",
       "      <td>-0.096989</td>\n",
       "      <td>-0.109262</td>\n",
       "      <td>0.015201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Department_hr</th>\n",
       "      <td>-0.012841</td>\n",
       "      <td>-0.009645</td>\n",
       "      <td>-0.027356</td>\n",
       "      <td>-0.010783</td>\n",
       "      <td>-0.022194</td>\n",
       "      <td>-0.015649</td>\n",
       "      <td>-0.001531</td>\n",
       "      <td>0.004599</td>\n",
       "      <td>-0.067949</td>\n",
       "      <td>-0.052848</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.047667</td>\n",
       "      <td>-0.056075</td>\n",
       "      <td>-0.057584</td>\n",
       "      <td>-0.140562</td>\n",
       "      <td>-0.095109</td>\n",
       "      <td>-0.107143</td>\n",
       "      <td>0.028249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Department_management</th>\n",
       "      <td>0.007172</td>\n",
       "      <td>0.009662</td>\n",
       "      <td>0.009728</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.115436</td>\n",
       "      <td>0.011242</td>\n",
       "      <td>0.128087</td>\n",
       "      <td>0.156665</td>\n",
       "      <td>-0.062500</td>\n",
       "      <td>-0.048610</td>\n",
       "      <td>-0.047667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.051578</td>\n",
       "      <td>-0.052966</td>\n",
       "      <td>-0.129289</td>\n",
       "      <td>-0.087482</td>\n",
       "      <td>-0.098551</td>\n",
       "      <td>-0.046035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Department_marketing</th>\n",
       "      <td>0.005715</td>\n",
       "      <td>-0.000311</td>\n",
       "      <td>-0.023064</td>\n",
       "      <td>-0.008210</td>\n",
       "      <td>0.012096</td>\n",
       "      <td>0.011367</td>\n",
       "      <td>0.049253</td>\n",
       "      <td>0.011599</td>\n",
       "      <td>-0.073524</td>\n",
       "      <td>-0.057183</td>\n",
       "      <td>-0.056075</td>\n",
       "      <td>-0.051578</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.062308</td>\n",
       "      <td>-0.152093</td>\n",
       "      <td>-0.102911</td>\n",
       "      <td>-0.115933</td>\n",
       "      <td>-0.000859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Department_product_mng</th>\n",
       "      <td>0.006919</td>\n",
       "      <td>-0.001989</td>\n",
       "      <td>0.000829</td>\n",
       "      <td>-0.005494</td>\n",
       "      <td>-0.003919</td>\n",
       "      <td>0.001246</td>\n",
       "      <td>-0.037288</td>\n",
       "      <td>-0.007669</td>\n",
       "      <td>-0.075503</td>\n",
       "      <td>-0.058723</td>\n",
       "      <td>-0.057584</td>\n",
       "      <td>-0.052966</td>\n",
       "      <td>-0.062308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.156187</td>\n",
       "      <td>-0.105682</td>\n",
       "      <td>-0.119054</td>\n",
       "      <td>-0.011029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Department_sales</th>\n",
       "      <td>0.004007</td>\n",
       "      <td>-0.023031</td>\n",
       "      <td>-0.013388</td>\n",
       "      <td>-0.001718</td>\n",
       "      <td>0.015150</td>\n",
       "      <td>-0.004955</td>\n",
       "      <td>0.012353</td>\n",
       "      <td>-0.035599</td>\n",
       "      <td>-0.184302</td>\n",
       "      <td>-0.143341</td>\n",
       "      <td>-0.140562</td>\n",
       "      <td>-0.129289</td>\n",
       "      <td>-0.152093</td>\n",
       "      <td>-0.156187</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.257967</td>\n",
       "      <td>-0.290608</td>\n",
       "      <td>0.009923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Department_support</th>\n",
       "      <td>0.009185</td>\n",
       "      <td>0.017104</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>-0.002444</td>\n",
       "      <td>-0.030111</td>\n",
       "      <td>0.012079</td>\n",
       "      <td>-0.035605</td>\n",
       "      <td>-0.029888</td>\n",
       "      <td>-0.124705</td>\n",
       "      <td>-0.096989</td>\n",
       "      <td>-0.095109</td>\n",
       "      <td>-0.087482</td>\n",
       "      <td>-0.102911</td>\n",
       "      <td>-0.105682</td>\n",
       "      <td>-0.257967</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.196636</td>\n",
       "      <td>0.010700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Department_technical</th>\n",
       "      <td>-0.009345</td>\n",
       "      <td>0.013742</td>\n",
       "      <td>0.028596</td>\n",
       "      <td>0.013638</td>\n",
       "      <td>-0.027991</td>\n",
       "      <td>-0.006070</td>\n",
       "      <td>-0.035799</td>\n",
       "      <td>-0.018630</td>\n",
       "      <td>-0.140484</td>\n",
       "      <td>-0.109262</td>\n",
       "      <td>-0.107143</td>\n",
       "      <td>-0.098551</td>\n",
       "      <td>-0.115933</td>\n",
       "      <td>-0.119054</td>\n",
       "      <td>-0.290608</td>\n",
       "      <td>-0.196636</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.020076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>left</th>\n",
       "      <td>-0.388375</td>\n",
       "      <td>0.006567</td>\n",
       "      <td>0.023787</td>\n",
       "      <td>0.071287</td>\n",
       "      <td>0.144822</td>\n",
       "      <td>-0.154622</td>\n",
       "      <td>-0.061788</td>\n",
       "      <td>-0.157898</td>\n",
       "      <td>-0.010925</td>\n",
       "      <td>0.015201</td>\n",
       "      <td>0.028249</td>\n",
       "      <td>-0.046035</td>\n",
       "      <td>-0.000859</td>\n",
       "      <td>-0.011029</td>\n",
       "      <td>0.009923</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.020076</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        satisfaction_level  last_evaluation  number_project  \\\n",
       "satisfaction_level                1.000000         0.105021       -0.142970   \n",
       "last_evaluation                   0.105021         1.000000        0.349333   \n",
       "number_project                   -0.142970         0.349333        1.000000   \n",
       "average_montly_hours             -0.020048         0.339742        0.417211   \n",
       "time_spend_company               -0.100866         0.131591        0.196786   \n",
       "Work_accident                     0.058697        -0.007104       -0.004741   \n",
       "promotion_last_5years             0.025605        -0.008684       -0.006064   \n",
       "salary                            0.050022        -0.013002       -0.001803   \n",
       "Department_IT                     0.006373         0.001269        0.003287   \n",
       "Department_accounting            -0.028649         0.002193        0.004189   \n",
       "Department_hr                    -0.012841        -0.009645       -0.027356   \n",
       "Department_management             0.007172         0.009662        0.009728   \n",
       "Department_marketing              0.005715        -0.000311       -0.023064   \n",
       "Department_product_mng            0.006919        -0.001989        0.000829   \n",
       "Department_sales                  0.004007        -0.023031       -0.013388   \n",
       "Department_support                0.009185         0.017104        0.000303   \n",
       "Department_technical             -0.009345         0.013742        0.028596   \n",
       "left                             -0.388375         0.006567        0.023787   \n",
       "\n",
       "                        average_montly_hours  time_spend_company  \\\n",
       "satisfaction_level                 -0.020048           -0.100866   \n",
       "last_evaluation                     0.339742            0.131591   \n",
       "number_project                      0.417211            0.196786   \n",
       "average_montly_hours                1.000000            0.127755   \n",
       "time_spend_company                  0.127755            1.000000   \n",
       "Work_accident                      -0.010143            0.002120   \n",
       "promotion_last_5years              -0.003544            0.067433   \n",
       "salary                             -0.002242            0.048715   \n",
       "Department_IT                       0.006967           -0.006053   \n",
       "Department_accounting               0.000524            0.003909   \n",
       "Department_hr                      -0.010783           -0.022194   \n",
       "Department_management               0.000834            0.115436   \n",
       "Department_marketing               -0.008210            0.012096   \n",
       "Department_product_mng             -0.005494           -0.003919   \n",
       "Department_sales                   -0.001718            0.015150   \n",
       "Department_support                 -0.002444           -0.030111   \n",
       "Department_technical                0.013638           -0.027991   \n",
       "left                                0.071287            0.144822   \n",
       "\n",
       "                        Work_accident  promotion_last_5years    salary  \\\n",
       "satisfaction_level           0.058697               0.025605  0.050022   \n",
       "last_evaluation             -0.007104              -0.008684 -0.013002   \n",
       "number_project              -0.004741              -0.006064 -0.001803   \n",
       "average_montly_hours        -0.010143              -0.003544 -0.002242   \n",
       "time_spend_company           0.002120               0.067433  0.048715   \n",
       "Work_accident                1.000000               0.039245  0.009247   \n",
       "promotion_last_5years        0.039245               1.000000  0.098119   \n",
       "salary                       0.009247               0.098119  1.000000   \n",
       "Department_IT               -0.009293              -0.038942 -0.010959   \n",
       "Department_accounting       -0.012836              -0.004852  0.012759   \n",
       "Department_hr               -0.015649              -0.001531  0.004599   \n",
       "Department_management        0.011242               0.128087  0.156665   \n",
       "Department_marketing         0.011367               0.049253  0.011599   \n",
       "Department_product_mng       0.001246              -0.037288 -0.007669   \n",
       "Department_sales            -0.004955               0.012353 -0.035599   \n",
       "Department_support           0.012079              -0.035605 -0.029888   \n",
       "Department_technical        -0.006070              -0.035799 -0.018630   \n",
       "left                        -0.154622              -0.061788 -0.157898   \n",
       "\n",
       "                        Department_IT  Department_accounting  Department_hr  \\\n",
       "satisfaction_level           0.006373              -0.028649      -0.012841   \n",
       "last_evaluation              0.001269               0.002193      -0.009645   \n",
       "number_project               0.003287               0.004189      -0.027356   \n",
       "average_montly_hours         0.006967               0.000524      -0.010783   \n",
       "time_spend_company          -0.006053               0.003909      -0.022194   \n",
       "Work_accident               -0.009293              -0.012836      -0.015649   \n",
       "promotion_last_5years       -0.038942              -0.004852      -0.001531   \n",
       "salary                      -0.010959               0.012759       0.004599   \n",
       "Department_IT                1.000000              -0.069293      -0.067949   \n",
       "Department_accounting       -0.069293               1.000000      -0.052848   \n",
       "Department_hr               -0.067949              -0.052848       1.000000   \n",
       "Department_management       -0.062500              -0.048610      -0.047667   \n",
       "Department_marketing        -0.073524              -0.057183      -0.056075   \n",
       "Department_product_mng      -0.075503              -0.058723      -0.057584   \n",
       "Department_sales            -0.184302              -0.143341      -0.140562   \n",
       "Department_support          -0.124705              -0.096989      -0.095109   \n",
       "Department_technical        -0.140484              -0.109262      -0.107143   \n",
       "left                        -0.010925               0.015201       0.028249   \n",
       "\n",
       "                        Department_management  Department_marketing  \\\n",
       "satisfaction_level                   0.007172              0.005715   \n",
       "last_evaluation                      0.009662             -0.000311   \n",
       "number_project                       0.009728             -0.023064   \n",
       "average_montly_hours                 0.000834             -0.008210   \n",
       "time_spend_company                   0.115436              0.012096   \n",
       "Work_accident                        0.011242              0.011367   \n",
       "promotion_last_5years                0.128087              0.049253   \n",
       "salary                               0.156665              0.011599   \n",
       "Department_IT                       -0.062500             -0.073524   \n",
       "Department_accounting               -0.048610             -0.057183   \n",
       "Department_hr                       -0.047667             -0.056075   \n",
       "Department_management                1.000000             -0.051578   \n",
       "Department_marketing                -0.051578              1.000000   \n",
       "Department_product_mng              -0.052966             -0.062308   \n",
       "Department_sales                    -0.129289             -0.152093   \n",
       "Department_support                  -0.087482             -0.102911   \n",
       "Department_technical                -0.098551             -0.115933   \n",
       "left                                -0.046035             -0.000859   \n",
       "\n",
       "                        Department_product_mng  Department_sales  \\\n",
       "satisfaction_level                    0.006919          0.004007   \n",
       "last_evaluation                      -0.001989         -0.023031   \n",
       "number_project                        0.000829         -0.013388   \n",
       "average_montly_hours                 -0.005494         -0.001718   \n",
       "time_spend_company                   -0.003919          0.015150   \n",
       "Work_accident                         0.001246         -0.004955   \n",
       "promotion_last_5years                -0.037288          0.012353   \n",
       "salary                               -0.007669         -0.035599   \n",
       "Department_IT                        -0.075503         -0.184302   \n",
       "Department_accounting                -0.058723         -0.143341   \n",
       "Department_hr                        -0.057584         -0.140562   \n",
       "Department_management                -0.052966         -0.129289   \n",
       "Department_marketing                 -0.062308         -0.152093   \n",
       "Department_product_mng                1.000000         -0.156187   \n",
       "Department_sales                     -0.156187          1.000000   \n",
       "Department_support                   -0.105682         -0.257967   \n",
       "Department_technical                 -0.119054         -0.290608   \n",
       "left                                 -0.011029          0.009923   \n",
       "\n",
       "                        Department_support  Department_technical      left  \n",
       "satisfaction_level                0.009185             -0.009345 -0.388375  \n",
       "last_evaluation                   0.017104              0.013742  0.006567  \n",
       "number_project                    0.000303              0.028596  0.023787  \n",
       "average_montly_hours             -0.002444              0.013638  0.071287  \n",
       "time_spend_company               -0.030111             -0.027991  0.144822  \n",
       "Work_accident                     0.012079             -0.006070 -0.154622  \n",
       "promotion_last_5years            -0.035605             -0.035799 -0.061788  \n",
       "salary                           -0.029888             -0.018630 -0.157898  \n",
       "Department_IT                    -0.124705             -0.140484 -0.010925  \n",
       "Department_accounting            -0.096989             -0.109262  0.015201  \n",
       "Department_hr                    -0.095109             -0.107143  0.028249  \n",
       "Department_management            -0.087482             -0.098551 -0.046035  \n",
       "Department_marketing             -0.102911             -0.115933 -0.000859  \n",
       "Department_product_mng           -0.105682             -0.119054 -0.011029  \n",
       "Department_sales                 -0.257967             -0.290608  0.009923  \n",
       "Department_support                1.000000             -0.196636  0.010700  \n",
       "Department_technical             -0.196636              1.000000  0.020076  \n",
       "left                              0.010700              0.020076  1.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrM = df.corr()\n",
    "corrM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebfb6f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(corrM, annot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fea1792b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "satisfaction_level       -0.476360\n",
      "last_evaluation          -0.026622\n",
      "number_project            0.337706\n",
      "average_montly_hours      0.052842\n",
      "time_spend_company        1.853319\n",
      "Work_accident             2.021149\n",
      "promotion_last_5years     6.636968\n",
      "salary                    0.598882\n",
      "Department_IT             3.052062\n",
      "Department_accounting     4.075859\n",
      "Department_hr             4.165530\n",
      "Department_management     4.566832\n",
      "Department_marketing      3.813782\n",
      "Department_product_mng    3.700720\n",
      "Department_sales          1.002197\n",
      "Department_support        1.975945\n",
      "Department_technical      1.654207\n",
      "left                      1.230043\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df[['satisfaction_level', 'last_evaluation', 'number_project',\n",
    "       'average_montly_hours', 'time_spend_company', 'Work_accident',\n",
    "       'promotion_last_5years', 'salary', 'Department_IT',\n",
    "       'Department_accounting', 'Department_hr', 'Department_management',\n",
    "       'Department_marketing', 'Department_product_mng', 'Department_sales',\n",
    "       'Department_support', 'Department_technical', 'left']].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34cc9581",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    14999.000000\n",
      "mean         3.498233\n",
      "std          1.460136\n",
      "min          2.000000\n",
      "25%          3.000000\n",
      "50%          3.000000\n",
      "75%          4.000000\n",
      "max         10.000000\n",
      "Name: time_spend_company, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df['time_spend_company'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e021096d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.time_spend_company.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3163954f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(df[\"time_spend_company\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fadf53fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    14999.000000\n",
      "mean         0.144610\n",
      "std          0.351719\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max          1.000000\n",
      "Name: Work_accident, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df['Work_accident'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d7c4328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.Work_accident.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5574a7f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.promotion_last_5years.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f16a515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    14999.000000\n",
      "mean         0.021268\n",
      "std          0.144281\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max          1.000000\n",
      "Name: promotion_last_5years, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df['promotion_last_5years'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8676ebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier(feature):\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    Min=Q1-1.5*IQR\n",
    "    Max=Q3+1.5*IQR\n",
    "    Floor=df[feature].quantile(0.10)\n",
    "    Cap=df[feature].quantile(0.90)\n",
    "    df[feature]=np.where(df[feature] < Min,Floor,df[feature])\n",
    "    df[feature]=np.where(df[feature] > Max,Cap,df[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3f6511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier('time_spend_company')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5180479f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.time_spend_company.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc29a574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAMN0lEQVR4nO3d36td9ZnH8fenJtAy1QkhZ8aQH+ZiZC5asMrBH3jjSBnUSr3xwotWkIGgeGGhUGgvbPsPlKIOhtAOU2lnSqFVROIwQitVmFhO0vir8SIMFkMCHh0bG5SC7TMX2R2OJ3tnr33cJ7s+837B5qy91nfv/SCHt4uVtTmpKiRJH3+fWPQAkqT5MOiS1IRBl6QmDLokNWHQJamJLYv64B07dtS+ffsW9fGS9LF05MiRt6pqadyxhQV93759rKysLOrjJeljKclvJx3zkoskNWHQJakJgy5JTRh0SWrCoEtSE4OCnuT1JC8nOZbkvFtTcs5DSU4keSnJNfMfVZJ0IbPctvgPVfXWhGO3AleOHtcBj45+SpIuknldcrkDeKzOOQxsS7JzTu8tSRpgaNAL+M8kR5LsH3N8F/DGmucnR/s+JMn+JCtJVlZXV2efVtqAJBflIS3a0EsuN1bVqSR/AzyT5LWq+uWa4+N+m8/7yxlVdRA4CLC8vOxf1tBFsZE/4pJkQ6+TFmnQGXpVnRr9fBN4HLh23ZKTwJ41z3cDp+YxoCRpmKlBT/JXSS798zbwj8Ar65Y9Cdw9utvleuBMVZ2e+7SSpImGXHL5W+Dx0TXCLcC/VdV/JLkXoKoOAIeA24ATwHvAPZszriRpkqlBr6r/Bq4as//Amu0C7p/vaJKkWfhNUUlqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0MDnqSS5L8OslTY47dlORMkmOjx4PzHVOSNM2WGdY+ABwHLptw/Lmquv2jjyRJ2ohBZ+hJdgNfAL63ueNIkjZq6CWX7wJfA/50gTU3JHkxydNJPjNuQZL9SVaSrKyurs44qiTpQqYGPcntwJtVdeQCy44CV1TVVcDDwBPjFlXVwaparqrlpaWljcwrSZpgyBn6jcAXk7wO/Bi4OckP1y6oqner6uxo+xCwNcmOeQ8rSZpsatCr6utVtbuq9gF3AT+vqi+tXZPk8iQZbV87et+3N2FeSdIEs9zl8iFJ7gWoqgPAncB9ST4A3gfuqqqaz4iSpCGyqO4uLy/XysrKQj5bmiYJnpPoL1GSI1W1PO6Y3xSVpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUxOCgJ7kkya+TPDXmWJI8lOREkpeSXDPfMSVJ08xyhv4AcHzCsVuBK0eP/cCjH3EuSdKMBgU9yW7gC8D3Jiy5A3iszjkMbEuyc04zSpIGGHqG/l3ga8CfJhzfBbyx5vnJ0b4PSbI/yUqSldXV1VnmlADYvn07STb9AWz6Z2zfvn3B/zXVzZZpC5LcDrxZVUeS3DRp2Zh9dd6OqoPAQYDl5eXzjkvTvPPOO1T1+NX58/84pHkZcoZ+I/DFJK8DPwZuTvLDdWtOAnvWPN8NnJrLhJKkQaYGvaq+XlW7q2ofcBfw86r60rplTwJ3j+52uR44U1Wn5z+uJGmSqZdcJklyL0BVHQAOAbcBJ4D3gHvmMp0kabCZgl5VzwLPjrYPrNlfwP3zHEySNBu/KSpJTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJamJqUFP8skkv0ryYpJXk3x7zJqbkpxJcmz0eHBzxpUkTbJlwJo/ADdX1dkkW4HnkzxdVYfXrXuuqm6f/4iSpCGmBr2qCjg7erp19KjNHEqSNLtB19CTXJLkGPAm8ExVvTBm2Q2jyzJPJ/nMhPfZn2Qlycrq6urGp5YknWdQ0Kvqj1X1OWA3cG2Sz65bchS4oqquAh4GnpjwPgerarmqlpeWljY+tSTpPDPd5VJVvwOeBW5Zt//dqjo72j4EbE2yY04zSpIGGHKXy1KSbaPtTwGfB15bt+byJBltXzt637fnPq0kaaIhd7nsBH6Q5BLOhfonVfVUknsBquoAcCdwX5IPgPeBu0b/mCpJukiG3OXyEnD1mP0H1mw/Ajwy39EkSbPwm6KS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKamBr0JJ9M8qskLyZ5Ncm3x6xJkoeSnEjyUpJrNmdcSdIkWwas+QNwc1WdTbIVeD7J01V1eM2aW4ErR4/rgEdHPyVJF8nUM/Q65+zo6dbRo9YtuwN4bLT2MLAtyc75jipJupAhZ+gkuQQ4Avwd8M9V9cK6JbuAN9Y8Pznad3rd++wH9gPs3bt3gyPr/7P65mXwrb9e9BhzUd+8bNEjqJlBQa+qPwKfS7INeDzJZ6vqlTVLMu5lY97nIHAQYHl5+bzj0lTfOnNRPiYJVf6K6uNlprtcqup3wLPALesOnQT2rHm+Gzj1UQaTJM1myF0uS6Mzc5J8Cvg88Nq6ZU8Cd4/udrkeOFNVp5EkXTRDLrnsBH4wuo7+CeAnVfVUknsBquoAcAi4DTgBvAfcs0nzSpImmBr0qnoJuHrM/gNrtgu4f76jSZJm4TdFJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNTE16En2JPlFkuNJXk3ywJg1NyU5k+TY6PHg5owrSZpky4A1HwBfraqjSS4FjiR5pqp+s27dc1V1+/xHlCQNMfUMvapOV9XR0fbvgePArs0eTJI0m5muoSfZB1wNvDDm8A1JXkzydJLPTHj9/iQrSVZWV1dnn1aSNNHgoCf5NPBT4CtV9e66w0eBK6rqKuBh4Ilx71FVB6tquaqWl5aWNjiyJGmcQUFPspVzMf9RVf1s/fGqereqzo62DwFbk+yY66SSpAsacpdLgO8Dx6vqOxPWXD5aR5JrR+/79jwHlSRd2JC7XG4Evgy8nOTYaN83gL0AVXUAuBO4L8kHwPvAXVVV8x9XkjTJ1KBX1fNApqx5BHhkXkNJkmbnN0UlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1MTXoSfYk+UWS40leTfLAmDVJ8lCSE0leSnLN5owrSZpky4A1HwBfraqjSS4FjiR5pqp+s2bNrcCVo8d1wKOjn5Kki2TqGXpVna6qo6Pt3wPHgV3rlt0BPFbnHAa2Jdk592klSRMNOUP/P0n2AVcDL6w7tAt4Y83zk6N9p9e9fj+wH2Dv3r0zjiptTJKL8rqq2tDnSPMy+B9Fk3wa+Cnwlap6d/3hMS8577e7qg5W1XJVLS8tLc02qbRBVXVRHtKiDQp6kq2ci/mPqupnY5acBPaseb4bOPXRx5MkDTXkLpcA3weOV9V3Jix7Erh7dLfL9cCZqjo9Ya0kaRMMuYZ+I/Bl4OUkx0b7vgHsBaiqA8Ah4DbgBPAecM/cJ5UkXdDUoFfV84y/Rr52TQH3z2soSdLs/KaoJDVh0CWpCYMuSU0YdElqIov6QkSSVeC3C/lwabodwFuLHkIa44qqGvvNzIUFXfpLlmSlqpYXPYc0Cy+5SFITBl2SmjDo0ngHFz2ANCuvoUtSE56hS1ITBl2SmjDo0hpJ/iXJm0leWfQs0qwMuvRh/wrcsughpI0w6NIaVfVL4H8WPYe0EQZdkpow6JLUhEGXpCYMuiQ1YdClNZL8O/BfwN8nOZnknxY9kzSUX/2XpCY8Q5ekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKa+F86K0BIFUGE6QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(df[\"time_spend_company\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fb82ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop( axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fbbfd506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['satisfaction_level', 'last_evaluation', 'number_project',\n",
       "       'average_montly_hours', 'time_spend_company', 'Work_accident',\n",
       "       'promotion_last_5years', 'salary', 'Department_IT',\n",
       "       'Department_accounting', 'Department_hr', 'Department_management',\n",
       "       'Department_marketing', 'Department_product_mng', 'Department_sales',\n",
       "       'Department_support', 'Department_technical', 'left'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a86441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature= df[['satisfaction_level', 'last_evaluation', 'number_project',\n",
    "       'average_montly_hours', 'time_spend_company', 'Work_accident', 'salary',\n",
    "       'Department_IT', 'Department_accounting', 'Department_hr',\n",
    "       'promotion_last_5years', 'Department_management', 'Department_marketing',\n",
    "       'Department_product_mng', 'Department_sales', 'Department_support',\n",
    "       'Department_technical']]\n",
    "X=np.asarray(feature)\n",
    "y=np.asarray(df['left'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "250bf19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11999, 17)\n",
      "(11999,)\n",
      "(3000, 17)\n",
      "(3000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.2, random_state=0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "1199\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e0a26fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a1d80b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_dim=17, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "opti = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=opti,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "891e7050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "315/338 [==========================>...] - ETA: 0s - loss: 0.6677 - accuracy: 0.6445\n",
      "Epoch 00001: val_loss improved from inf to 0.53488, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.6594 - accuracy: 0.6503 - val_loss: 0.5349 - val_accuracy: 0.7625\n",
      "Epoch 2/500\n",
      "301/338 [=========================>....] - ETA: 0s - loss: 0.4917 - accuracy: 0.7557\n",
      "Epoch 00002: val_loss improved from 0.53488 to 0.42664, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.4875 - accuracy: 0.7564 - val_loss: 0.4266 - val_accuracy: 0.7783\n",
      "Epoch 3/500\n",
      "326/338 [===========================>..] - ETA: 0s - loss: 0.4016 - accuracy: 0.7788\n",
      "Epoch 00003: val_loss improved from 0.42664 to 0.33976, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.4004 - accuracy: 0.7790 - val_loss: 0.3398 - val_accuracy: 0.8292\n",
      "Epoch 4/500\n",
      "309/338 [==========================>...] - ETA: 0s - loss: 0.3262 - accuracy: 0.8585\n",
      "Epoch 00004: val_loss improved from 0.33976 to 0.26968, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.3219 - accuracy: 0.8611 - val_loss: 0.2697 - val_accuracy: 0.8908\n",
      "Epoch 5/500\n",
      "334/338 [============================>.] - ETA: 0s - loss: 0.2582 - accuracy: 0.9025\n",
      "Epoch 00005: val_loss improved from 0.26968 to 0.22124, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.2577 - accuracy: 0.9026 - val_loss: 0.2212 - val_accuracy: 0.9125\n",
      "Epoch 6/500\n",
      "303/338 [=========================>....] - ETA: 0s - loss: 0.2185 - accuracy: 0.9289\n",
      "Epoch 00006: val_loss improved from 0.22124 to 0.19409, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.2188 - accuracy: 0.9279 - val_loss: 0.1941 - val_accuracy: 0.9300\n",
      "Epoch 7/500\n",
      "330/338 [============================>.] - ETA: 0s - loss: 0.1980 - accuracy: 0.9389\n",
      "Epoch 00007: val_loss improved from 0.19409 to 0.18023, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1979 - accuracy: 0.9392 - val_loss: 0.1802 - val_accuracy: 0.9358\n",
      "Epoch 8/500\n",
      "314/338 [==========================>...] - ETA: 0s - loss: 0.1854 - accuracy: 0.9448\n",
      "Epoch 00008: val_loss improved from 0.18023 to 0.17233, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1856 - accuracy: 0.9451 - val_loss: 0.1723 - val_accuracy: 0.9375\n",
      "Epoch 9/500\n",
      "325/338 [===========================>..] - ETA: 0s - loss: 0.1770 - accuracy: 0.9477\n",
      "Epoch 00009: val_loss improved from 0.17233 to 0.16807, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1773 - accuracy: 0.9476 - val_loss: 0.1681 - val_accuracy: 0.9392\n",
      "Epoch 10/500\n",
      "334/338 [============================>.] - ETA: 0s - loss: 0.1718 - accuracy: 0.9497\n",
      "Epoch 00010: val_loss improved from 0.16807 to 0.16323, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1711 - accuracy: 0.9499 - val_loss: 0.1632 - val_accuracy: 0.9408\n",
      "Epoch 11/500\n",
      "332/338 [============================>.] - ETA: 0s - loss: 0.1668 - accuracy: 0.9507\n",
      "Epoch 00011: val_loss improved from 0.16323 to 0.16135, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1665 - accuracy: 0.9507 - val_loss: 0.1613 - val_accuracy: 0.9433\n",
      "Epoch 12/500\n",
      "309/338 [==========================>...] - ETA: 0s - loss: 0.1624 - accuracy: 0.9520\n",
      "Epoch 00012: val_loss improved from 0.16135 to 0.15987, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1635 - accuracy: 0.9512 - val_loss: 0.1599 - val_accuracy: 0.9433\n",
      "Epoch 13/500\n",
      "327/338 [============================>.] - ETA: 0s - loss: 0.1596 - accuracy: 0.9535\n",
      "Epoch 00013: val_loss did not improve from 0.15987\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1610 - accuracy: 0.9529 - val_loss: 0.1599 - val_accuracy: 0.9433\n",
      "Epoch 14/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1600 - accuracy: 0.9536\n",
      "Epoch 00014: val_loss improved from 0.15987 to 0.15800, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1595 - accuracy: 0.9536 - val_loss: 0.1580 - val_accuracy: 0.9467\n",
      "Epoch 15/500\n",
      "332/338 [============================>.] - ETA: 0s - loss: 0.1592 - accuracy: 0.9527\n",
      "Epoch 00015: val_loss improved from 0.15800 to 0.15687, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1582 - accuracy: 0.9531 - val_loss: 0.1569 - val_accuracy: 0.9467\n",
      "Epoch 16/500\n",
      "318/338 [===========================>..] - ETA: 0s - loss: 0.1560 - accuracy: 0.9535\n",
      "Epoch 00016: val_loss improved from 0.15687 to 0.15371, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1567 - accuracy: 0.9533 - val_loss: 0.1537 - val_accuracy: 0.9483\n",
      "Epoch 17/500\n",
      "306/338 [==========================>...] - ETA: 0s - loss: 0.1567 - accuracy: 0.9531\n",
      "Epoch 00017: val_loss did not improve from 0.15371\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1553 - accuracy: 0.9534 - val_loss: 0.1544 - val_accuracy: 0.9475\n",
      "Epoch 18/500\n",
      "321/338 [===========================>..] - ETA: 0s - loss: 0.1543 - accuracy: 0.9548\n",
      "Epoch 00018: val_loss improved from 0.15371 to 0.15151, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1542 - accuracy: 0.9548 - val_loss: 0.1515 - val_accuracy: 0.9500\n",
      "Epoch 19/500\n",
      "337/338 [============================>.] - ETA: 0s - loss: 0.1525 - accuracy: 0.9556\n",
      "Epoch 00019: val_loss improved from 0.15151 to 0.15098, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1525 - accuracy: 0.9555 - val_loss: 0.1510 - val_accuracy: 0.9500\n",
      "Epoch 20/500\n",
      "309/338 [==========================>...] - ETA: 0s - loss: 0.1531 - accuracy: 0.9552\n",
      "Epoch 00020: val_loss improved from 0.15098 to 0.15019, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1517 - accuracy: 0.9556 - val_loss: 0.1502 - val_accuracy: 0.9500\n",
      "Epoch 21/500\n",
      "315/338 [==========================>...] - ETA: 0s - loss: 0.1515 - accuracy: 0.9550\n",
      "Epoch 00021: val_loss improved from 0.15019 to 0.14816, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1504 - accuracy: 0.9552 - val_loss: 0.1482 - val_accuracy: 0.9533\n",
      "Epoch 22/500\n",
      "330/338 [============================>.] - ETA: 0s - loss: 0.1498 - accuracy: 0.9571\n",
      "Epoch 00022: val_loss did not improve from 0.14816\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1492 - accuracy: 0.9570 - val_loss: 0.1489 - val_accuracy: 0.9492\n",
      "Epoch 23/500\n",
      "316/338 [===========================>..] - ETA: 0s - loss: 0.1499 - accuracy: 0.9553\n",
      "Epoch 00023: val_loss did not improve from 0.14816\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1480 - accuracy: 0.9561 - val_loss: 0.1487 - val_accuracy: 0.9500\n",
      "Epoch 24/500\n",
      "330/338 [============================>.] - ETA: 0s - loss: 0.1475 - accuracy: 0.9563\n",
      "Epoch 00024: val_loss improved from 0.14816 to 0.14465, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1464 - accuracy: 0.9568 - val_loss: 0.1447 - val_accuracy: 0.9550\n",
      "Epoch 25/500\n",
      "314/338 [==========================>...] - ETA: 0s - loss: 0.1422 - accuracy: 0.9586\n",
      "Epoch 00025: val_loss did not improve from 0.14465\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1450 - accuracy: 0.9581 - val_loss: 0.1486 - val_accuracy: 0.9492\n",
      "Epoch 26/500\n",
      "313/338 [==========================>...] - ETA: 0s - loss: 0.1430 - accuracy: 0.9589\n",
      "Epoch 00026: val_loss improved from 0.14465 to 0.14464, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1440 - accuracy: 0.9581 - val_loss: 0.1446 - val_accuracy: 0.9533\n",
      "Epoch 27/500\n",
      "338/338 [==============================] - ETA: 0s - loss: 0.1427 - accuracy: 0.9588\n",
      "Epoch 00027: val_loss improved from 0.14464 to 0.14443, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1427 - accuracy: 0.9588 - val_loss: 0.1444 - val_accuracy: 0.9517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "315/338 [==========================>...] - ETA: 0s - loss: 0.1401 - accuracy: 0.9589\n",
      "Epoch 00028: val_loss improved from 0.14443 to 0.14421, saving model to weights.h5\n",
      "338/338 [==============================] - 7s 22ms/step - loss: 0.1415 - accuracy: 0.9588 - val_loss: 0.1442 - val_accuracy: 0.9542\n",
      "Epoch 29/500\n",
      "317/338 [===========================>..] - ETA: 0s - loss: 0.1387 - accuracy: 0.9600\n",
      "Epoch 00029: val_loss improved from 0.14421 to 0.14307, saving model to weights.h5\n",
      "338/338 [==============================] - 4s 12ms/step - loss: 0.1407 - accuracy: 0.9594 - val_loss: 0.1431 - val_accuracy: 0.9558\n",
      "Epoch 30/500\n",
      "308/338 [==========================>...] - ETA: 0s - loss: 0.1367 - accuracy: 0.9611\n",
      "Epoch 00030: val_loss improved from 0.14307 to 0.14185, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1398 - accuracy: 0.9599 - val_loss: 0.1419 - val_accuracy: 0.9550\n",
      "Epoch 31/500\n",
      "336/338 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.9595\n",
      "Epoch 00031: val_loss improved from 0.14185 to 0.14102, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1391 - accuracy: 0.9597 - val_loss: 0.1410 - val_accuracy: 0.9575\n",
      "Epoch 32/500\n",
      "330/338 [============================>.] - ETA: 0s - loss: 0.1372 - accuracy: 0.9616\n",
      "Epoch 00032: val_loss did not improve from 0.14102\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1381 - accuracy: 0.9613 - val_loss: 0.1421 - val_accuracy: 0.9558\n",
      "Epoch 33/500\n",
      "328/338 [============================>.] - ETA: 0s - loss: 0.1375 - accuracy: 0.9608\n",
      "Epoch 00033: val_loss improved from 0.14102 to 0.14096, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1376 - accuracy: 0.9607 - val_loss: 0.1410 - val_accuracy: 0.9542\n",
      "Epoch 34/500\n",
      "325/338 [===========================>..] - ETA: 0s - loss: 0.1369 - accuracy: 0.9610\n",
      "Epoch 00034: val_loss improved from 0.14096 to 0.13919, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1371 - accuracy: 0.9608 - val_loss: 0.1392 - val_accuracy: 0.9567\n",
      "Epoch 35/500\n",
      "328/338 [============================>.] - ETA: 0s - loss: 0.1367 - accuracy: 0.9609\n",
      "Epoch 00035: val_loss did not improve from 0.13919\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1366 - accuracy: 0.9607 - val_loss: 0.1419 - val_accuracy: 0.9558\n",
      "Epoch 36/500\n",
      "323/338 [===========================>..] - ETA: 0s - loss: 0.1361 - accuracy: 0.9608\n",
      "Epoch 00036: val_loss did not improve from 0.13919\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1361 - accuracy: 0.9613 - val_loss: 0.1406 - val_accuracy: 0.9583\n",
      "Epoch 37/500\n",
      "331/338 [============================>.] - ETA: 0s - loss: 0.1355 - accuracy: 0.9613\n",
      "Epoch 00037: val_loss did not improve from 0.13919\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1358 - accuracy: 0.9615 - val_loss: 0.1414 - val_accuracy: 0.9567\n",
      "Epoch 38/500\n",
      "314/338 [==========================>...] - ETA: 0s - loss: 0.1321 - accuracy: 0.9623\n",
      "Epoch 00038: val_loss improved from 0.13919 to 0.13815, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1347 - accuracy: 0.9613 - val_loss: 0.1382 - val_accuracy: 0.9575\n",
      "Epoch 39/500\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 0.1329 - accuracy: 0.9621\n",
      "Epoch 00039: val_loss improved from 0.13815 to 0.13731, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1344 - accuracy: 0.9618 - val_loss: 0.1373 - val_accuracy: 0.9567\n",
      "Epoch 40/500\n",
      "315/338 [==========================>...] - ETA: 0s - loss: 0.1360 - accuracy: 0.9611\n",
      "Epoch 00040: val_loss improved from 0.13731 to 0.13730, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1344 - accuracy: 0.9616 - val_loss: 0.1373 - val_accuracy: 0.9567\n",
      "Epoch 41/500\n",
      "309/338 [==========================>...] - ETA: 0s - loss: 0.1326 - accuracy: 0.9617\n",
      "Epoch 00041: val_loss did not improve from 0.13730\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1336 - accuracy: 0.9617 - val_loss: 0.1402 - val_accuracy: 0.9558\n",
      "Epoch 42/500\n",
      "328/338 [============================>.] - ETA: 0s - loss: 0.1326 - accuracy: 0.9617\n",
      "Epoch 00042: val_loss did not improve from 0.13730\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1338 - accuracy: 0.9616 - val_loss: 0.1380 - val_accuracy: 0.9550\n",
      "Epoch 43/500\n",
      "330/338 [============================>.] - ETA: 0s - loss: 0.1339 - accuracy: 0.9615\n",
      "Epoch 00043: val_loss did not improve from 0.13730\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1332 - accuracy: 0.9617 - val_loss: 0.1378 - val_accuracy: 0.9558\n",
      "Epoch 44/500\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 0.1328 - accuracy: 0.9617\n",
      "Epoch 00044: val_loss improved from 0.13730 to 0.13621, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1330 - accuracy: 0.9618 - val_loss: 0.1362 - val_accuracy: 0.9592\n",
      "Epoch 45/500\n",
      "314/338 [==========================>...] - ETA: 0s - loss: 0.1331 - accuracy: 0.9616\n",
      "Epoch 00045: val_loss did not improve from 0.13621\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1325 - accuracy: 0.9622 - val_loss: 0.1370 - val_accuracy: 0.9575\n",
      "Epoch 46/500\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 0.1320 - accuracy: 0.9623\n",
      "Epoch 00046: val_loss did not improve from 0.13621\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1324 - accuracy: 0.9622 - val_loss: 0.1367 - val_accuracy: 0.9575\n",
      "Epoch 47/500\n",
      "337/338 [============================>.] - ETA: 0s - loss: 0.1324 - accuracy: 0.9623\n",
      "Epoch 00047: val_loss did not improve from 0.13621\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1323 - accuracy: 0.9623 - val_loss: 0.1381 - val_accuracy: 0.9567\n",
      "Epoch 48/500\n",
      "313/338 [==========================>...] - ETA: 0s - loss: 0.1285 - accuracy: 0.9639\n",
      "Epoch 00048: val_loss did not improve from 0.13621\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1319 - accuracy: 0.9625 - val_loss: 0.1377 - val_accuracy: 0.9583\n",
      "Epoch 49/500\n",
      "333/338 [============================>.] - ETA: 0s - loss: 0.1325 - accuracy: 0.9615\n",
      "Epoch 00049: val_loss improved from 0.13621 to 0.13613, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1319 - accuracy: 0.9618 - val_loss: 0.1361 - val_accuracy: 0.9583\n",
      "Epoch 50/500\n",
      "337/338 [============================>.] - ETA: 0s - loss: 0.1314 - accuracy: 0.9627\n",
      "Epoch 00050: val_loss did not improve from 0.13613\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1312 - accuracy: 0.9628 - val_loss: 0.1396 - val_accuracy: 0.9567\n",
      "Epoch 51/500\n",
      "333/338 [============================>.] - ETA: 0s - loss: 0.1312 - accuracy: 0.9624\n",
      "Epoch 00051: val_loss did not improve from 0.13613\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1314 - accuracy: 0.9623 - val_loss: 0.1382 - val_accuracy: 0.9583\n",
      "Epoch 52/500\n",
      "314/338 [==========================>...] - ETA: 0s - loss: 0.1299 - accuracy: 0.9636\n",
      "Epoch 00052: val_loss did not improve from 0.13613\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1309 - accuracy: 0.9628 - val_loss: 0.1381 - val_accuracy: 0.9567\n",
      "Epoch 53/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1313 - accuracy: 0.9623\n",
      "Epoch 00053: val_loss did not improve from 0.13613\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1311 - accuracy: 0.9624 - val_loss: 0.1371 - val_accuracy: 0.9575\n",
      "Epoch 54/500\n",
      "334/338 [============================>.] - ETA: 0s - loss: 0.1303 - accuracy: 0.9628\n",
      "Epoch 00054: val_loss improved from 0.13613 to 0.13541, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1307 - accuracy: 0.9627 - val_loss: 0.1354 - val_accuracy: 0.9592\n",
      "Epoch 55/500\n",
      "305/338 [==========================>...] - ETA: 0s - loss: 0.1320 - accuracy: 0.9628\n",
      "Epoch 00055: val_loss did not improve from 0.13541\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1305 - accuracy: 0.9635 - val_loss: 0.1368 - val_accuracy: 0.9575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "331/338 [============================>.] - ETA: 0s - loss: 0.1298 - accuracy: 0.9637\n",
      "Epoch 00056: val_loss did not improve from 0.13541\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1305 - accuracy: 0.9634 - val_loss: 0.1377 - val_accuracy: 0.9575\n",
      "Epoch 57/500\n",
      "308/338 [==========================>...] - ETA: 0s - loss: 0.1293 - accuracy: 0.9635\n",
      "Epoch 00057: val_loss improved from 0.13541 to 0.13517, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1305 - accuracy: 0.9631 - val_loss: 0.1352 - val_accuracy: 0.9592\n",
      "Epoch 58/500\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 0.1280 - accuracy: 0.9640\n",
      "Epoch 00058: val_loss did not improve from 0.13517\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1301 - accuracy: 0.9631 - val_loss: 0.1399 - val_accuracy: 0.9583\n",
      "Epoch 59/500\n",
      "331/338 [============================>.] - ETA: 0s - loss: 0.1311 - accuracy: 0.9628\n",
      "Epoch 00059: val_loss did not improve from 0.13517\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1301 - accuracy: 0.9631 - val_loss: 0.1372 - val_accuracy: 0.9600\n",
      "Epoch 60/500\n",
      "329/338 [============================>.] - ETA: 0s - loss: 0.1290 - accuracy: 0.9631\n",
      "Epoch 00060: val_loss improved from 0.13517 to 0.13488, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1300 - accuracy: 0.9629 - val_loss: 0.1349 - val_accuracy: 0.9608\n",
      "Epoch 61/500\n",
      "322/338 [===========================>..] - ETA: 0s - loss: 0.1296 - accuracy: 0.9636\n",
      "Epoch 00061: val_loss improved from 0.13488 to 0.13436, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1295 - accuracy: 0.9632 - val_loss: 0.1344 - val_accuracy: 0.9608\n",
      "Epoch 62/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1295 - accuracy: 0.9632\n",
      "Epoch 00062: val_loss improved from 0.13436 to 0.13367, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1298 - accuracy: 0.9630 - val_loss: 0.1337 - val_accuracy: 0.9592\n",
      "Epoch 63/500\n",
      "328/338 [============================>.] - ETA: 0s - loss: 0.1301 - accuracy: 0.9634\n",
      "Epoch 00063: val_loss did not improve from 0.13367\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1297 - accuracy: 0.9635 - val_loss: 0.1368 - val_accuracy: 0.9583\n",
      "Epoch 64/500\n",
      "324/338 [===========================>..] - ETA: 0s - loss: 0.1301 - accuracy: 0.9633\n",
      "Epoch 00064: val_loss did not improve from 0.13367\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1294 - accuracy: 0.9635 - val_loss: 0.1361 - val_accuracy: 0.9592\n",
      "Epoch 65/500\n",
      "329/338 [============================>.] - ETA: 0s - loss: 0.1297 - accuracy: 0.9632\n",
      "Epoch 00065: val_loss improved from 0.13367 to 0.13363, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1294 - accuracy: 0.9634 - val_loss: 0.1336 - val_accuracy: 0.9600\n",
      "Epoch 66/500\n",
      "338/338 [==============================] - ETA: 0s - loss: 0.1291 - accuracy: 0.9631\n",
      "Epoch 00066: val_loss did not improve from 0.13363\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1291 - accuracy: 0.9631 - val_loss: 0.1365 - val_accuracy: 0.9592\n",
      "Epoch 67/500\n",
      "337/338 [============================>.] - ETA: 0s - loss: 0.1287 - accuracy: 0.9639\n",
      "Epoch 00067: val_loss did not improve from 0.13363\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1290 - accuracy: 0.9638 - val_loss: 0.1357 - val_accuracy: 0.9592\n",
      "Epoch 68/500\n",
      "336/338 [============================>.] - ETA: 0s - loss: 0.1287 - accuracy: 0.9628\n",
      "Epoch 00068: val_loss did not improve from 0.13363\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1288 - accuracy: 0.9628 - val_loss: 0.1377 - val_accuracy: 0.9567\n",
      "Epoch 69/500\n",
      "310/338 [==========================>...] - ETA: 0s - loss: 0.1262 - accuracy: 0.9649\n",
      "Epoch 00069: val_loss did not improve from 0.13363\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1286 - accuracy: 0.9643 - val_loss: 0.1373 - val_accuracy: 0.9600\n",
      "Epoch 70/500\n",
      "331/338 [============================>.] - ETA: 0s - loss: 0.1295 - accuracy: 0.9631\n",
      "Epoch 00070: val_loss did not improve from 0.13363\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1292 - accuracy: 0.9631 - val_loss: 0.1360 - val_accuracy: 0.9583\n",
      "Epoch 71/500\n",
      "307/338 [==========================>...] - ETA: 0s - loss: 0.1277 - accuracy: 0.9633\n",
      "Epoch 00071: val_loss did not improve from 0.13363\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1287 - accuracy: 0.9634 - val_loss: 0.1370 - val_accuracy: 0.9583\n",
      "Epoch 72/500\n",
      "302/338 [=========================>....] - ETA: 0s - loss: 0.1291 - accuracy: 0.9633\n",
      "Epoch 00072: val_loss did not improve from 0.13363\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1287 - accuracy: 0.9635 - val_loss: 0.1352 - val_accuracy: 0.9608\n",
      "Epoch 73/500\n",
      "302/338 [=========================>....] - ETA: 0s - loss: 0.1312 - accuracy: 0.9629\n",
      "Epoch 00073: val_loss did not improve from 0.13363\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1285 - accuracy: 0.9637 - val_loss: 0.1364 - val_accuracy: 0.9567\n",
      "Epoch 74/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9640\n",
      "Epoch 00074: val_loss did not improve from 0.13363\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1283 - accuracy: 0.9637 - val_loss: 0.1362 - val_accuracy: 0.9600\n",
      "Epoch 75/500\n",
      "328/338 [============================>.] - ETA: 0s - loss: 0.1289 - accuracy: 0.9636\n",
      "Epoch 00075: val_loss did not improve from 0.13363\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1284 - accuracy: 0.9636 - val_loss: 0.1366 - val_accuracy: 0.9600\n",
      "Epoch 76/500\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 0.1269 - accuracy: 0.9636\n",
      "Epoch 00076: val_loss did not improve from 0.13363\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1283 - accuracy: 0.9632 - val_loss: 0.1360 - val_accuracy: 0.9600\n",
      "Epoch 77/500\n",
      "337/338 [============================>.] - ETA: 0s - loss: 0.1279 - accuracy: 0.9635\n",
      "Epoch 00077: val_loss did not improve from 0.13363\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1280 - accuracy: 0.9634 - val_loss: 0.1361 - val_accuracy: 0.9583\n",
      "Epoch 78/500\n",
      "327/338 [============================>.] - ETA: 0s - loss: 0.1289 - accuracy: 0.9635\n",
      "Epoch 00078: val_loss did not improve from 0.13363\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1281 - accuracy: 0.9638 - val_loss: 0.1347 - val_accuracy: 0.9592\n",
      "Epoch 79/500\n",
      "334/338 [============================>.] - ETA: 0s - loss: 0.1279 - accuracy: 0.9629\n",
      "Epoch 00079: val_loss did not improve from 0.13363\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1280 - accuracy: 0.9629 - val_loss: 0.1337 - val_accuracy: 0.9592\n",
      "Epoch 80/500\n",
      "331/338 [============================>.] - ETA: 0s - loss: 0.1263 - accuracy: 0.9642\n",
      "Epoch 00080: val_loss did not improve from 0.13363\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1280 - accuracy: 0.9637 - val_loss: 0.1366 - val_accuracy: 0.9592\n",
      "Epoch 81/500\n",
      "301/338 [=========================>....] - ETA: 0s - loss: 0.1274 - accuracy: 0.9634\n",
      "Epoch 00081: val_loss did not improve from 0.13363\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1277 - accuracy: 0.9639 - val_loss: 0.1340 - val_accuracy: 0.9583\n",
      "Epoch 82/500\n",
      "336/338 [============================>.] - ETA: 0s - loss: 0.1275 - accuracy: 0.9636\n",
      "Epoch 00082: val_loss did not improve from 0.13363\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1276 - accuracy: 0.9637 - val_loss: 0.1383 - val_accuracy: 0.9583\n",
      "Epoch 83/500\n",
      "336/338 [============================>.] - ETA: 0s - loss: 0.1270 - accuracy: 0.9642\n",
      "Epoch 00083: val_loss improved from 0.13363 to 0.13222, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1276 - accuracy: 0.9639 - val_loss: 0.1322 - val_accuracy: 0.9592\n",
      "Epoch 84/500\n",
      "307/338 [==========================>...] - ETA: 0s - loss: 0.1299 - accuracy: 0.9622\n",
      "Epoch 00084: val_loss did not improve from 0.13222\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1275 - accuracy: 0.9633 - val_loss: 0.1355 - val_accuracy: 0.9608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/500\n",
      "329/338 [============================>.] - ETA: 0s - loss: 0.1276 - accuracy: 0.9628\n",
      "Epoch 00085: val_loss did not improve from 0.13222\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1274 - accuracy: 0.9629 - val_loss: 0.1341 - val_accuracy: 0.9592\n",
      "Epoch 86/500\n",
      "333/338 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9635\n",
      "Epoch 00086: val_loss did not improve from 0.13222\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1274 - accuracy: 0.9634 - val_loss: 0.1336 - val_accuracy: 0.9600\n",
      "Epoch 87/500\n",
      "336/338 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9637\n",
      "Epoch 00087: val_loss did not improve from 0.13222\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1274 - accuracy: 0.9637 - val_loss: 0.1356 - val_accuracy: 0.9592\n",
      "Epoch 88/500\n",
      "337/338 [============================>.] - ETA: 0s - loss: 0.1273 - accuracy: 0.9629\n",
      "Epoch 00088: val_loss did not improve from 0.13222\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1272 - accuracy: 0.9630 - val_loss: 0.1352 - val_accuracy: 0.9592\n",
      "Epoch 89/500\n",
      "300/338 [=========================>....] - ETA: 0s - loss: 0.1257 - accuracy: 0.9640\n",
      "Epoch 00089: val_loss did not improve from 0.13222\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1271 - accuracy: 0.9634 - val_loss: 0.1342 - val_accuracy: 0.9583\n",
      "Epoch 90/500\n",
      "314/338 [==========================>...] - ETA: 0s - loss: 0.1269 - accuracy: 0.9629\n",
      "Epoch 00090: val_loss did not improve from 0.13222\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1270 - accuracy: 0.9630 - val_loss: 0.1344 - val_accuracy: 0.9592\n",
      "Epoch 91/500\n",
      "310/338 [==========================>...] - ETA: 0s - loss: 0.1278 - accuracy: 0.9634\n",
      "Epoch 00091: val_loss did not improve from 0.13222\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1271 - accuracy: 0.9639 - val_loss: 0.1336 - val_accuracy: 0.9608\n",
      "Epoch 92/500\n",
      "330/338 [============================>.] - ETA: 0s - loss: 0.1268 - accuracy: 0.9641\n",
      "Epoch 00092: val_loss did not improve from 0.13222\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1272 - accuracy: 0.9638 - val_loss: 0.1324 - val_accuracy: 0.9583\n",
      "Epoch 93/500\n",
      "333/338 [============================>.] - ETA: 0s - loss: 0.1269 - accuracy: 0.9638\n",
      "Epoch 00093: val_loss did not improve from 0.13222\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1271 - accuracy: 0.9635 - val_loss: 0.1351 - val_accuracy: 0.9608\n",
      "Epoch 94/500\n",
      "325/338 [===========================>..] - ETA: 0s - loss: 0.1279 - accuracy: 0.9627\n",
      "Epoch 00094: val_loss did not improve from 0.13222\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1269 - accuracy: 0.9631 - val_loss: 0.1331 - val_accuracy: 0.9592\n",
      "Epoch 95/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1264 - accuracy: 0.9640\n",
      "Epoch 00095: val_loss did not improve from 0.13222\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1265 - accuracy: 0.9640 - val_loss: 0.1370 - val_accuracy: 0.9575\n",
      "Epoch 96/500\n",
      "325/338 [===========================>..] - ETA: 0s - loss: 0.1251 - accuracy: 0.9638\n",
      "Epoch 00096: val_loss did not improve from 0.13222\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1267 - accuracy: 0.9636 - val_loss: 0.1328 - val_accuracy: 0.9600\n",
      "Epoch 97/500\n",
      "303/338 [=========================>....] - ETA: 0s - loss: 0.1276 - accuracy: 0.9629\n",
      "Epoch 00097: val_loss did not improve from 0.13222\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1267 - accuracy: 0.9635 - val_loss: 0.1344 - val_accuracy: 0.9567\n",
      "Epoch 98/500\n",
      "338/338 [==============================] - ETA: 0s - loss: 0.1267 - accuracy: 0.9639\n",
      "Epoch 00098: val_loss did not improve from 0.13222\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1267 - accuracy: 0.9639 - val_loss: 0.1361 - val_accuracy: 0.9567\n",
      "Epoch 99/500\n",
      "326/338 [===========================>..] - ETA: 0s - loss: 0.1281 - accuracy: 0.9633\n",
      "Epoch 00099: val_loss did not improve from 0.13222\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1266 - accuracy: 0.9637 - val_loss: 0.1338 - val_accuracy: 0.9592\n",
      "Epoch 100/500\n",
      "332/338 [============================>.] - ETA: 0s - loss: 0.1269 - accuracy: 0.9635\n",
      "Epoch 00100: val_loss did not improve from 0.13222\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1266 - accuracy: 0.9633 - val_loss: 0.1342 - val_accuracy: 0.9592\n",
      "Epoch 101/500\n",
      "334/338 [============================>.] - ETA: 0s - loss: 0.1259 - accuracy: 0.9634\n",
      "Epoch 00101: val_loss did not improve from 0.13222\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1262 - accuracy: 0.9633 - val_loss: 0.1336 - val_accuracy: 0.9592\n",
      "Epoch 102/500\n",
      "311/338 [==========================>...] - ETA: 0s - loss: 0.1269 - accuracy: 0.9632\n",
      "Epoch 00102: val_loss did not improve from 0.13222\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1265 - accuracy: 0.9634 - val_loss: 0.1335 - val_accuracy: 0.9608\n",
      "Epoch 103/500\n",
      "334/338 [============================>.] - ETA: 0s - loss: 0.1263 - accuracy: 0.9632\n",
      "Epoch 00103: val_loss did not improve from 0.13222\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1263 - accuracy: 0.9632 - val_loss: 0.1368 - val_accuracy: 0.9583\n",
      "Epoch 104/500\n",
      "310/338 [==========================>...] - ETA: 0s - loss: 0.1236 - accuracy: 0.9638\n",
      "Epoch 00104: val_loss did not improve from 0.13222\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1262 - accuracy: 0.9632 - val_loss: 0.1351 - val_accuracy: 0.9583\n",
      "Epoch 105/500\n",
      "332/338 [============================>.] - ETA: 0s - loss: 0.1269 - accuracy: 0.9635\n",
      "Epoch 00105: val_loss did not improve from 0.13222\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1261 - accuracy: 0.9637 - val_loss: 0.1362 - val_accuracy: 0.9575\n",
      "Epoch 106/500\n",
      "321/338 [===========================>..] - ETA: 0s - loss: 0.1272 - accuracy: 0.9632\n",
      "Epoch 00106: val_loss did not improve from 0.13222\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1264 - accuracy: 0.9633 - val_loss: 0.1325 - val_accuracy: 0.9592\n",
      "Epoch 107/500\n",
      "319/338 [===========================>..] - ETA: 0s - loss: 0.1267 - accuracy: 0.9625\n",
      "Epoch 00107: val_loss did not improve from 0.13222\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1259 - accuracy: 0.9630 - val_loss: 0.1353 - val_accuracy: 0.9592\n",
      "Epoch 108/500\n",
      "321/338 [===========================>..] - ETA: 0s - loss: 0.1266 - accuracy: 0.9642\n",
      "Epoch 00108: val_loss improved from 0.13222 to 0.13103, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1262 - accuracy: 0.9640 - val_loss: 0.1310 - val_accuracy: 0.9617\n",
      "Epoch 109/500\n",
      "334/338 [============================>.] - ETA: 0s - loss: 0.1267 - accuracy: 0.9634\n",
      "Epoch 00109: val_loss improved from 0.13103 to 0.13090, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1260 - accuracy: 0.9636 - val_loss: 0.1309 - val_accuracy: 0.9600\n",
      "Epoch 110/500\n",
      "317/338 [===========================>..] - ETA: 0s - loss: 0.1237 - accuracy: 0.9645\n",
      "Epoch 00110: val_loss did not improve from 0.13090\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1259 - accuracy: 0.9637 - val_loss: 0.1341 - val_accuracy: 0.9583\n",
      "Epoch 111/500\n",
      "338/338 [==============================] - ETA: 0s - loss: 0.1258 - accuracy: 0.9643\n",
      "Epoch 00111: val_loss did not improve from 0.13090\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1258 - accuracy: 0.9643 - val_loss: 0.1352 - val_accuracy: 0.9592\n",
      "Epoch 112/500\n",
      "338/338 [==============================] - ETA: 0s - loss: 0.1263 - accuracy: 0.9636\n",
      "Epoch 00112: val_loss did not improve from 0.13090\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1263 - accuracy: 0.9636 - val_loss: 0.1312 - val_accuracy: 0.9592\n",
      "Epoch 113/500\n",
      "333/338 [============================>.] - ETA: 0s - loss: 0.1265 - accuracy: 0.9631\n",
      "Epoch 00113: val_loss did not improve from 0.13090\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1259 - accuracy: 0.9634 - val_loss: 0.1316 - val_accuracy: 0.9600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/500\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 0.1282 - accuracy: 0.9631\n",
      "Epoch 00114: val_loss did not improve from 0.13090\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1260 - accuracy: 0.9638 - val_loss: 0.1322 - val_accuracy: 0.9583\n",
      "Epoch 115/500\n",
      "306/338 [==========================>...] - ETA: 0s - loss: 0.1274 - accuracy: 0.9642\n",
      "Epoch 00115: val_loss did not improve from 0.13090\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1260 - accuracy: 0.9642 - val_loss: 0.1356 - val_accuracy: 0.9575\n",
      "Epoch 116/500\n",
      "299/338 [=========================>....] - ETA: 0s - loss: 0.1248 - accuracy: 0.9636\n",
      "Epoch 00116: val_loss did not improve from 0.13090\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1259 - accuracy: 0.9635 - val_loss: 0.1324 - val_accuracy: 0.9592\n",
      "Epoch 117/500\n",
      "337/338 [============================>.] - ETA: 0s - loss: 0.1257 - accuracy: 0.9636\n",
      "Epoch 00117: val_loss did not improve from 0.13090\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1256 - accuracy: 0.9637 - val_loss: 0.1353 - val_accuracy: 0.9592\n",
      "Epoch 118/500\n",
      "318/338 [===========================>..] - ETA: 0s - loss: 0.1248 - accuracy: 0.9638\n",
      "Epoch 00118: val_loss did not improve from 0.13090\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1258 - accuracy: 0.9637 - val_loss: 0.1337 - val_accuracy: 0.9592\n",
      "Epoch 119/500\n",
      "327/338 [============================>.] - ETA: 0s - loss: 0.1246 - accuracy: 0.9647\n",
      "Epoch 00119: val_loss did not improve from 0.13090\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1256 - accuracy: 0.9643 - val_loss: 0.1332 - val_accuracy: 0.9583\n",
      "Epoch 120/500\n",
      "333/338 [============================>.] - ETA: 0s - loss: 0.1254 - accuracy: 0.9640\n",
      "Epoch 00120: val_loss did not improve from 0.13090\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1255 - accuracy: 0.9641 - val_loss: 0.1350 - val_accuracy: 0.9583\n",
      "Epoch 121/500\n",
      "332/338 [============================>.] - ETA: 0s - loss: 0.1245 - accuracy: 0.9639\n",
      "Epoch 00121: val_loss did not improve from 0.13090\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1252 - accuracy: 0.9636 - val_loss: 0.1332 - val_accuracy: 0.9600\n",
      "Epoch 122/500\n",
      "326/338 [===========================>..] - ETA: 0s - loss: 0.1263 - accuracy: 0.9637\n",
      "Epoch 00122: val_loss did not improve from 0.13090\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1256 - accuracy: 0.9638 - val_loss: 0.1328 - val_accuracy: 0.9600\n",
      "Epoch 123/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1255 - accuracy: 0.9634\n",
      "Epoch 00123: val_loss did not improve from 0.13090\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1255 - accuracy: 0.9635 - val_loss: 0.1316 - val_accuracy: 0.9592\n",
      "Epoch 124/500\n",
      "330/338 [============================>.] - ETA: 0s - loss: 0.1251 - accuracy: 0.9636\n",
      "Epoch 00124: val_loss did not improve from 0.13090\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1256 - accuracy: 0.9636 - val_loss: 0.1320 - val_accuracy: 0.9608\n",
      "Epoch 125/500\n",
      "338/338 [==============================] - ETA: 0s - loss: 0.1255 - accuracy: 0.9644\n",
      "Epoch 00125: val_loss did not improve from 0.13090\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1255 - accuracy: 0.9644 - val_loss: 0.1341 - val_accuracy: 0.9567\n",
      "Epoch 126/500\n",
      "332/338 [============================>.] - ETA: 0s - loss: 0.1259 - accuracy: 0.9637\n",
      "Epoch 00126: val_loss improved from 0.13090 to 0.13065, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1254 - accuracy: 0.9637 - val_loss: 0.1306 - val_accuracy: 0.9592\n",
      "Epoch 127/500\n",
      "331/338 [============================>.] - ETA: 0s - loss: 0.1238 - accuracy: 0.9637\n",
      "Epoch 00127: val_loss improved from 0.13065 to 0.13049, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1253 - accuracy: 0.9631 - val_loss: 0.1305 - val_accuracy: 0.9583\n",
      "Epoch 128/500\n",
      "337/338 [============================>.] - ETA: 0s - loss: 0.1258 - accuracy: 0.9632\n",
      "Epoch 00128: val_loss did not improve from 0.13049\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1257 - accuracy: 0.9632 - val_loss: 0.1314 - val_accuracy: 0.9600\n",
      "Epoch 129/500\n",
      "326/338 [===========================>..] - ETA: 0s - loss: 0.1253 - accuracy: 0.9640\n",
      "Epoch 00129: val_loss did not improve from 0.13049\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1250 - accuracy: 0.9642 - val_loss: 0.1349 - val_accuracy: 0.9592\n",
      "Epoch 130/500\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 0.1236 - accuracy: 0.9644\n",
      "Epoch 00130: val_loss did not improve from 0.13049\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1252 - accuracy: 0.9641 - val_loss: 0.1355 - val_accuracy: 0.9575\n",
      "Epoch 131/500\n",
      "321/338 [===========================>..] - ETA: 0s - loss: 0.1263 - accuracy: 0.9632\n",
      "Epoch 00131: val_loss did not improve from 0.13049\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1252 - accuracy: 0.9639 - val_loss: 0.1322 - val_accuracy: 0.9600\n",
      "Epoch 132/500\n",
      "318/338 [===========================>..] - ETA: 0s - loss: 0.1241 - accuracy: 0.9638\n",
      "Epoch 00132: val_loss did not improve from 0.13049\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1253 - accuracy: 0.9635 - val_loss: 0.1328 - val_accuracy: 0.9583\n",
      "Epoch 133/500\n",
      "337/338 [============================>.] - ETA: 0s - loss: 0.1248 - accuracy: 0.9641\n",
      "Epoch 00133: val_loss did not improve from 0.13049\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1250 - accuracy: 0.9641 - val_loss: 0.1346 - val_accuracy: 0.9583\n",
      "Epoch 134/500\n",
      "303/338 [=========================>....] - ETA: 0s - loss: 0.1252 - accuracy: 0.9634\n",
      "Epoch 00134: val_loss improved from 0.13049 to 0.13013, saving model to weights.h5\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1249 - accuracy: 0.9634 - val_loss: 0.1301 - val_accuracy: 0.9592\n",
      "Epoch 135/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1256 - accuracy: 0.9632\n",
      "Epoch 00135: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1253 - accuracy: 0.9633 - val_loss: 0.1346 - val_accuracy: 0.9567\n",
      "Epoch 136/500\n",
      "303/338 [=========================>....] - ETA: 0s - loss: 0.1255 - accuracy: 0.9640\n",
      "Epoch 00136: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1252 - accuracy: 0.9640 - val_loss: 0.1330 - val_accuracy: 0.9592\n",
      "Epoch 137/500\n",
      "315/338 [==========================>...] - ETA: 0s - loss: 0.1236 - accuracy: 0.9642\n",
      "Epoch 00137: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1249 - accuracy: 0.9641 - val_loss: 0.1366 - val_accuracy: 0.9575\n",
      "Epoch 138/500\n",
      "326/338 [===========================>..] - ETA: 0s - loss: 0.1250 - accuracy: 0.9635\n",
      "Epoch 00138: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1250 - accuracy: 0.9634 - val_loss: 0.1329 - val_accuracy: 0.9583\n",
      "Epoch 139/500\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 0.1283 - accuracy: 0.9631\n",
      "Epoch 00139: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1245 - accuracy: 0.9643 - val_loss: 0.1386 - val_accuracy: 0.9567\n",
      "Epoch 140/500\n",
      "322/338 [===========================>..] - ETA: 0s - loss: 0.1247 - accuracy: 0.9641\n",
      "Epoch 00140: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1249 - accuracy: 0.9640 - val_loss: 0.1352 - val_accuracy: 0.9575\n",
      "Epoch 141/500\n",
      "336/338 [============================>.] - ETA: 0s - loss: 0.1253 - accuracy: 0.9642\n",
      "Epoch 00141: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1249 - accuracy: 0.9643 - val_loss: 0.1344 - val_accuracy: 0.9575\n",
      "Epoch 142/500\n",
      "331/338 [============================>.] - ETA: 0s - loss: 0.1256 - accuracy: 0.9636\n",
      "Epoch 00142: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1249 - accuracy: 0.9637 - val_loss: 0.1354 - val_accuracy: 0.9558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/500\n",
      "337/338 [============================>.] - ETA: 0s - loss: 0.1251 - accuracy: 0.9639\n",
      "Epoch 00143: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1249 - accuracy: 0.9640 - val_loss: 0.1335 - val_accuracy: 0.9567\n",
      "Epoch 144/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1244 - accuracy: 0.9643\n",
      "Epoch 00144: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1246 - accuracy: 0.9643 - val_loss: 0.1373 - val_accuracy: 0.9575\n",
      "Epoch 145/500\n",
      "331/338 [============================>.] - ETA: 0s - loss: 0.1234 - accuracy: 0.9645\n",
      "Epoch 00145: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1247 - accuracy: 0.9639 - val_loss: 0.1314 - val_accuracy: 0.9575\n",
      "Epoch 146/500\n",
      "328/338 [============================>.] - ETA: 0s - loss: 0.1238 - accuracy: 0.9644\n",
      "Epoch 00146: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1247 - accuracy: 0.9643 - val_loss: 0.1324 - val_accuracy: 0.9575\n",
      "Epoch 147/500\n",
      "307/338 [==========================>...] - ETA: 0s - loss: 0.1239 - accuracy: 0.9636\n",
      "Epoch 00147: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1247 - accuracy: 0.9637 - val_loss: 0.1336 - val_accuracy: 0.9592\n",
      "Epoch 148/500\n",
      "302/338 [=========================>....] - ETA: 0s - loss: 0.1255 - accuracy: 0.9644\n",
      "Epoch 00148: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1247 - accuracy: 0.9645 - val_loss: 0.1331 - val_accuracy: 0.9583\n",
      "Epoch 149/500\n",
      "329/338 [============================>.] - ETA: 0s - loss: 0.1236 - accuracy: 0.9642\n",
      "Epoch 00149: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1247 - accuracy: 0.9639 - val_loss: 0.1328 - val_accuracy: 0.9575\n",
      "Epoch 150/500\n",
      "336/338 [============================>.] - ETA: 0s - loss: 0.1245 - accuracy: 0.9637\n",
      "Epoch 00150: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1243 - accuracy: 0.9638 - val_loss: 0.1342 - val_accuracy: 0.9592\n",
      "Epoch 151/500\n",
      "301/338 [=========================>....] - ETA: 0s - loss: 0.1253 - accuracy: 0.9644\n",
      "Epoch 00151: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1244 - accuracy: 0.9644 - val_loss: 0.1326 - val_accuracy: 0.9575\n",
      "Epoch 152/500\n",
      "321/338 [===========================>..] - ETA: 0s - loss: 0.1266 - accuracy: 0.9637\n",
      "Epoch 00152: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1246 - accuracy: 0.9643 - val_loss: 0.1360 - val_accuracy: 0.9583\n",
      "Epoch 153/500\n",
      "309/338 [==========================>...] - ETA: 0s - loss: 0.1261 - accuracy: 0.9639\n",
      "Epoch 00153: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 0s 1ms/step - loss: 0.1247 - accuracy: 0.9642 - val_loss: 0.1326 - val_accuracy: 0.9583\n",
      "Epoch 154/500\n",
      "323/338 [===========================>..] - ETA: 0s - loss: 0.1252 - accuracy: 0.9634\n",
      "Epoch 00154: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1247 - accuracy: 0.9636 - val_loss: 0.1320 - val_accuracy: 0.9575\n",
      "Epoch 155/500\n",
      "314/338 [==========================>...] - ETA: 0s - loss: 0.1254 - accuracy: 0.9634\n",
      "Epoch 00155: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1244 - accuracy: 0.9637 - val_loss: 0.1336 - val_accuracy: 0.9558\n",
      "Epoch 156/500\n",
      "337/338 [============================>.] - ETA: 0s - loss: 0.1247 - accuracy: 0.9638\n",
      "Epoch 00156: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1245 - accuracy: 0.9639 - val_loss: 0.1343 - val_accuracy: 0.9583\n",
      "Epoch 157/500\n",
      "332/338 [============================>.] - ETA: 0s - loss: 0.1248 - accuracy: 0.9640\n",
      "Epoch 00157: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1239 - accuracy: 0.9643 - val_loss: 0.1330 - val_accuracy: 0.9583\n",
      "Epoch 158/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1244 - accuracy: 0.9646\n",
      "Epoch 00158: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1245 - accuracy: 0.9646 - val_loss: 0.1317 - val_accuracy: 0.9592\n",
      "Epoch 159/500\n",
      "336/338 [============================>.] - ETA: 0s - loss: 0.1242 - accuracy: 0.9638\n",
      "Epoch 00159: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1244 - accuracy: 0.9638 - val_loss: 0.1332 - val_accuracy: 0.9575\n",
      "Epoch 160/500\n",
      "330/338 [============================>.] - ETA: 0s - loss: 0.1246 - accuracy: 0.9640\n",
      "Epoch 00160: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1243 - accuracy: 0.9639 - val_loss: 0.1333 - val_accuracy: 0.9567\n",
      "Epoch 161/500\n",
      "314/338 [==========================>...] - ETA: 0s - loss: 0.1204 - accuracy: 0.9656\n",
      "Epoch 00161: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1243 - accuracy: 0.9640 - val_loss: 0.1327 - val_accuracy: 0.9592\n",
      "Epoch 162/500\n",
      "299/338 [=========================>....] - ETA: 0s - loss: 0.1224 - accuracy: 0.9644\n",
      "Epoch 00162: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1242 - accuracy: 0.9644 - val_loss: 0.1335 - val_accuracy: 0.9558\n",
      "Epoch 163/500\n",
      "330/338 [============================>.] - ETA: 0s - loss: 0.1247 - accuracy: 0.9634\n",
      "Epoch 00163: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1243 - accuracy: 0.9635 - val_loss: 0.1348 - val_accuracy: 0.9567\n",
      "Epoch 164/500\n",
      "303/338 [=========================>....] - ETA: 0s - loss: 0.1255 - accuracy: 0.9636\n",
      "Epoch 00164: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1244 - accuracy: 0.9643 - val_loss: 0.1365 - val_accuracy: 0.9575\n",
      "Epoch 165/500\n",
      "301/338 [=========================>....] - ETA: 0s - loss: 0.1251 - accuracy: 0.9641\n",
      "Epoch 00165: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1243 - accuracy: 0.9642 - val_loss: 0.1346 - val_accuracy: 0.9575\n",
      "Epoch 166/500\n",
      "317/338 [===========================>..] - ETA: 0s - loss: 0.1250 - accuracy: 0.9640\n",
      "Epoch 00166: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1242 - accuracy: 0.9642 - val_loss: 0.1331 - val_accuracy: 0.9567\n",
      "Epoch 167/500\n",
      "330/338 [============================>.] - ETA: 0s - loss: 0.1242 - accuracy: 0.9643\n",
      "Epoch 00167: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1244 - accuracy: 0.9641 - val_loss: 0.1322 - val_accuracy: 0.9575\n",
      "Epoch 168/500\n",
      "329/338 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9648\n",
      "Epoch 00168: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1240 - accuracy: 0.9643 - val_loss: 0.1347 - val_accuracy: 0.9558\n",
      "Epoch 169/500\n",
      "325/338 [===========================>..] - ETA: 0s - loss: 0.1227 - accuracy: 0.9644\n",
      "Epoch 00169: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1240 - accuracy: 0.9640 - val_loss: 0.1338 - val_accuracy: 0.9575\n",
      "Epoch 170/500\n",
      "303/338 [=========================>....] - ETA: 0s - loss: 0.1232 - accuracy: 0.9645\n",
      "Epoch 00170: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1242 - accuracy: 0.9644 - val_loss: 0.1316 - val_accuracy: 0.9567\n",
      "Epoch 171/500\n",
      "338/338 [==============================] - ETA: 0s - loss: 0.1239 - accuracy: 0.9643\n",
      "Epoch 00171: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1239 - accuracy: 0.9643 - val_loss: 0.1340 - val_accuracy: 0.9575\n",
      "Epoch 172/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334/338 [============================>.] - ETA: 0s - loss: 0.1242 - accuracy: 0.9635\n",
      "Epoch 00172: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1244 - accuracy: 0.9635 - val_loss: 0.1329 - val_accuracy: 0.9575\n",
      "Epoch 173/500\n",
      "322/338 [===========================>..] - ETA: 0s - loss: 0.1258 - accuracy: 0.9634\n",
      "Epoch 00173: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1240 - accuracy: 0.9640 - val_loss: 0.1317 - val_accuracy: 0.9575\n",
      "Epoch 174/500\n",
      "324/338 [===========================>..] - ETA: 0s - loss: 0.1237 - accuracy: 0.9642\n",
      "Epoch 00174: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1242 - accuracy: 0.9640 - val_loss: 0.1339 - val_accuracy: 0.9567\n",
      "Epoch 175/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1241 - accuracy: 0.9636\n",
      "Epoch 00175: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1243 - accuracy: 0.9637 - val_loss: 0.1330 - val_accuracy: 0.9567\n",
      "Epoch 176/500\n",
      "317/338 [===========================>..] - ETA: 0s - loss: 0.1230 - accuracy: 0.9645\n",
      "Epoch 00176: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1239 - accuracy: 0.9643 - val_loss: 0.1345 - val_accuracy: 0.9567\n",
      "Epoch 177/500\n",
      "299/338 [=========================>....] - ETA: 0s - loss: 0.1260 - accuracy: 0.9635\n",
      "Epoch 00177: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1238 - accuracy: 0.9640 - val_loss: 0.1365 - val_accuracy: 0.9567\n",
      "Epoch 178/500\n",
      "319/338 [===========================>..] - ETA: 0s - loss: 0.1238 - accuracy: 0.9641\n",
      "Epoch 00178: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1242 - accuracy: 0.9638 - val_loss: 0.1348 - val_accuracy: 0.9558\n",
      "Epoch 179/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1234 - accuracy: 0.9645\n",
      "Epoch 00179: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1237 - accuracy: 0.9643 - val_loss: 0.1352 - val_accuracy: 0.9575\n",
      "Epoch 180/500\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 0.1246 - accuracy: 0.9646\n",
      "Epoch 00180: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1239 - accuracy: 0.9647 - val_loss: 0.1326 - val_accuracy: 0.9567\n",
      "Epoch 181/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1245 - accuracy: 0.9644\n",
      "Epoch 00181: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1243 - accuracy: 0.9644 - val_loss: 0.1333 - val_accuracy: 0.9567\n",
      "Epoch 182/500\n",
      "324/338 [===========================>..] - ETA: 0s - loss: 0.1232 - accuracy: 0.9640\n",
      "Epoch 00182: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1240 - accuracy: 0.9639 - val_loss: 0.1334 - val_accuracy: 0.9567\n",
      "Epoch 183/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1238 - accuracy: 0.9643\n",
      "Epoch 00183: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1238 - accuracy: 0.9643 - val_loss: 0.1341 - val_accuracy: 0.9583\n",
      "Epoch 184/500\n",
      "324/338 [===========================>..] - ETA: 0s - loss: 0.1233 - accuracy: 0.9644\n",
      "Epoch 00184: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1236 - accuracy: 0.9643 - val_loss: 0.1377 - val_accuracy: 0.9583\n",
      "Epoch 185/500\n",
      "331/338 [============================>.] - ETA: 0s - loss: 0.1226 - accuracy: 0.9648\n",
      "Epoch 00185: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1237 - accuracy: 0.9643 - val_loss: 0.1349 - val_accuracy: 0.9567\n",
      "Epoch 186/500\n",
      "316/338 [===========================>..] - ETA: 0s - loss: 0.1277 - accuracy: 0.9632\n",
      "Epoch 00186: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1238 - accuracy: 0.9643 - val_loss: 0.1360 - val_accuracy: 0.9567\n",
      "Epoch 187/500\n",
      "331/338 [============================>.] - ETA: 0s - loss: 0.1239 - accuracy: 0.9641\n",
      "Epoch 00187: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1236 - accuracy: 0.9643 - val_loss: 0.1345 - val_accuracy: 0.9567\n",
      "Epoch 188/500\n",
      "326/338 [===========================>..] - ETA: 0s - loss: 0.1250 - accuracy: 0.9640\n",
      "Epoch 00188: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1238 - accuracy: 0.9644 - val_loss: 0.1330 - val_accuracy: 0.9583\n",
      "Epoch 189/500\n",
      "318/338 [===========================>..] - ETA: 0s - loss: 0.1239 - accuracy: 0.9644\n",
      "Epoch 00189: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1239 - accuracy: 0.9644 - val_loss: 0.1342 - val_accuracy: 0.9575\n",
      "Epoch 190/500\n",
      "332/338 [============================>.] - ETA: 0s - loss: 0.1233 - accuracy: 0.9645\n",
      "Epoch 00190: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1236 - accuracy: 0.9643 - val_loss: 0.1352 - val_accuracy: 0.9575\n",
      "Epoch 191/500\n",
      "337/338 [============================>.] - ETA: 0s - loss: 0.1241 - accuracy: 0.9644\n",
      "Epoch 00191: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1240 - accuracy: 0.9644 - val_loss: 0.1372 - val_accuracy: 0.9567\n",
      "Epoch 192/500\n",
      "325/338 [===========================>..] - ETA: 0s - loss: 0.1207 - accuracy: 0.9651\n",
      "Epoch 00192: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1237 - accuracy: 0.9643 - val_loss: 0.1322 - val_accuracy: 0.9567\n",
      "Epoch 193/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1242 - accuracy: 0.9643\n",
      "Epoch 00193: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1239 - accuracy: 0.9643 - val_loss: 0.1342 - val_accuracy: 0.9567\n",
      "Epoch 194/500\n",
      "316/338 [===========================>..] - ETA: 0s - loss: 0.1250 - accuracy: 0.9640\n",
      "Epoch 00194: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1232 - accuracy: 0.9643 - val_loss: 0.1357 - val_accuracy: 0.9575\n",
      "Epoch 195/500\n",
      "325/338 [===========================>..] - ETA: 0s - loss: 0.1239 - accuracy: 0.9643\n",
      "Epoch 00195: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1237 - accuracy: 0.9645 - val_loss: 0.1347 - val_accuracy: 0.9567\n",
      "Epoch 196/500\n",
      "308/338 [==========================>...] - ETA: 0s - loss: 0.1247 - accuracy: 0.9633\n",
      "Epoch 00196: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1236 - accuracy: 0.9643 - val_loss: 0.1327 - val_accuracy: 0.9575\n",
      "Epoch 197/500\n",
      "330/338 [============================>.] - ETA: 0s - loss: 0.1227 - accuracy: 0.9643\n",
      "Epoch 00197: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1235 - accuracy: 0.9643 - val_loss: 0.1351 - val_accuracy: 0.9575\n",
      "Epoch 198/500\n",
      "321/338 [===========================>..] - ETA: 0s - loss: 0.1219 - accuracy: 0.9649\n",
      "Epoch 00198: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1235 - accuracy: 0.9644 - val_loss: 0.1322 - val_accuracy: 0.9583\n",
      "Epoch 199/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1236 - accuracy: 0.9652\n",
      "Epoch 00199: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1236 - accuracy: 0.9652 - val_loss: 0.1348 - val_accuracy: 0.9575\n",
      "Epoch 200/500\n",
      "308/338 [==========================>...] - ETA: 0s - loss: 0.1249 - accuracy: 0.9643\n",
      "Epoch 00200: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1233 - accuracy: 0.9645 - val_loss: 0.1355 - val_accuracy: 0.9583\n",
      "Epoch 201/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337/338 [============================>.] - ETA: 0s - loss: 0.1232 - accuracy: 0.9646\n",
      "Epoch 00201: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1231 - accuracy: 0.9646 - val_loss: 0.1344 - val_accuracy: 0.9575\n",
      "Epoch 202/500\n",
      "309/338 [==========================>...] - ETA: 0s - loss: 0.1245 - accuracy: 0.9643\n",
      "Epoch 00202: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1232 - accuracy: 0.9645 - val_loss: 0.1351 - val_accuracy: 0.9558\n",
      "Epoch 203/500\n",
      "336/338 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9650\n",
      "Epoch 00203: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1233 - accuracy: 0.9652 - val_loss: 0.1330 - val_accuracy: 0.9575\n",
      "Epoch 204/500\n",
      "314/338 [==========================>...] - ETA: 0s - loss: 0.1230 - accuracy: 0.9647\n",
      "Epoch 00204: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1234 - accuracy: 0.9646 - val_loss: 0.1332 - val_accuracy: 0.9592\n",
      "Epoch 205/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9644\n",
      "Epoch 00205: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1233 - accuracy: 0.9644 - val_loss: 0.1312 - val_accuracy: 0.9583\n",
      "Epoch 206/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1233 - accuracy: 0.9646\n",
      "Epoch 00206: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1230 - accuracy: 0.9647 - val_loss: 0.1328 - val_accuracy: 0.9600\n",
      "Epoch 207/500\n",
      "337/338 [============================>.] - ETA: 0s - loss: 0.1231 - accuracy: 0.9644\n",
      "Epoch 00207: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1229 - accuracy: 0.9644 - val_loss: 0.1339 - val_accuracy: 0.9567\n",
      "Epoch 208/500\n",
      "313/338 [==========================>...] - ETA: 0s - loss: 0.1216 - accuracy: 0.9657\n",
      "Epoch 00208: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1231 - accuracy: 0.9651 - val_loss: 0.1329 - val_accuracy: 0.9583\n",
      "Epoch 209/500\n",
      "301/338 [=========================>....] - ETA: 0s - loss: 0.1212 - accuracy: 0.9651\n",
      "Epoch 00209: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1232 - accuracy: 0.9652 - val_loss: 0.1353 - val_accuracy: 0.9575\n",
      "Epoch 210/500\n",
      "312/338 [==========================>...] - ETA: 0s - loss: 0.1218 - accuracy: 0.9650\n",
      "Epoch 00210: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1229 - accuracy: 0.9652 - val_loss: 0.1339 - val_accuracy: 0.9600\n",
      "Epoch 211/500\n",
      "328/338 [============================>.] - ETA: 0s - loss: 0.1227 - accuracy: 0.9653\n",
      "Epoch 00211: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1230 - accuracy: 0.9655 - val_loss: 0.1353 - val_accuracy: 0.9583\n",
      "Epoch 212/500\n",
      "332/338 [============================>.] - ETA: 0s - loss: 0.1242 - accuracy: 0.9649\n",
      "Epoch 00212: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1234 - accuracy: 0.9649 - val_loss: 0.1348 - val_accuracy: 0.9592\n",
      "Epoch 213/500\n",
      "328/338 [============================>.] - ETA: 0s - loss: 0.1228 - accuracy: 0.9645\n",
      "Epoch 00213: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1230 - accuracy: 0.9645 - val_loss: 0.1338 - val_accuracy: 0.9575\n",
      "Epoch 214/500\n",
      "331/338 [============================>.] - ETA: 0s - loss: 0.1224 - accuracy: 0.9650\n",
      "Epoch 00214: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1230 - accuracy: 0.9645 - val_loss: 0.1350 - val_accuracy: 0.9575\n",
      "Epoch 215/500\n",
      "325/338 [===========================>..] - ETA: 0s - loss: 0.1246 - accuracy: 0.9650\n",
      "Epoch 00215: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1229 - accuracy: 0.9654 - val_loss: 0.1359 - val_accuracy: 0.9583\n",
      "Epoch 216/500\n",
      "325/338 [===========================>..] - ETA: 0s - loss: 0.1247 - accuracy: 0.9642\n",
      "Epoch 00216: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1230 - accuracy: 0.9648 - val_loss: 0.1341 - val_accuracy: 0.9575\n",
      "Epoch 217/500\n",
      "308/338 [==========================>...] - ETA: 0s - loss: 0.1245 - accuracy: 0.9643\n",
      "Epoch 00217: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1228 - accuracy: 0.9643 - val_loss: 0.1363 - val_accuracy: 0.9575\n",
      "Epoch 218/500\n",
      "315/338 [==========================>...] - ETA: 0s - loss: 0.1263 - accuracy: 0.9635\n",
      "Epoch 00218: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1227 - accuracy: 0.9646 - val_loss: 0.1346 - val_accuracy: 0.9583\n",
      "Epoch 219/500\n",
      "338/338 [==============================] - ETA: 0s - loss: 0.1229 - accuracy: 0.9647\n",
      "Epoch 00219: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1229 - accuracy: 0.9647 - val_loss: 0.1339 - val_accuracy: 0.9592\n",
      "Epoch 220/500\n",
      "312/338 [==========================>...] - ETA: 0s - loss: 0.1244 - accuracy: 0.9644\n",
      "Epoch 00220: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1225 - accuracy: 0.9651 - val_loss: 0.1387 - val_accuracy: 0.9583\n",
      "Epoch 221/500\n",
      "331/338 [============================>.] - ETA: 0s - loss: 0.1218 - accuracy: 0.9641\n",
      "Epoch 00221: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1228 - accuracy: 0.9638 - val_loss: 0.1339 - val_accuracy: 0.9592\n",
      "Epoch 222/500\n",
      "324/338 [===========================>..] - ETA: 0s - loss: 0.1223 - accuracy: 0.9649\n",
      "Epoch 00222: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1230 - accuracy: 0.9647 - val_loss: 0.1366 - val_accuracy: 0.9583\n",
      "Epoch 223/500\n",
      "301/338 [=========================>....] - ETA: 0s - loss: 0.1233 - accuracy: 0.9648\n",
      "Epoch 00223: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1227 - accuracy: 0.9650 - val_loss: 0.1353 - val_accuracy: 0.9592\n",
      "Epoch 224/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1230 - accuracy: 0.9646\n",
      "Epoch 00224: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1228 - accuracy: 0.9645 - val_loss: 0.1359 - val_accuracy: 0.9600\n",
      "Epoch 225/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1228 - accuracy: 0.9642\n",
      "Epoch 00225: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1230 - accuracy: 0.9641 - val_loss: 0.1337 - val_accuracy: 0.9600\n",
      "Epoch 226/500\n",
      "333/338 [============================>.] - ETA: 0s - loss: 0.1224 - accuracy: 0.9650\n",
      "Epoch 00226: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1228 - accuracy: 0.9650 - val_loss: 0.1339 - val_accuracy: 0.9592\n",
      "Epoch 227/500\n",
      "334/338 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9653\n",
      "Epoch 00227: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1228 - accuracy: 0.9648 - val_loss: 0.1335 - val_accuracy: 0.9575\n",
      "Epoch 228/500\n",
      "327/338 [============================>.] - ETA: 0s - loss: 0.1219 - accuracy: 0.9651\n",
      "Epoch 00228: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1227 - accuracy: 0.9651 - val_loss: 0.1346 - val_accuracy: 0.9583\n",
      "Epoch 229/500\n",
      "312/338 [==========================>...] - ETA: 0s - loss: 0.1215 - accuracy: 0.9646\n",
      "Epoch 00229: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1229 - accuracy: 0.9645 - val_loss: 0.1337 - val_accuracy: 0.9583\n",
      "Epoch 230/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298/338 [=========================>....] - ETA: 0s - loss: 0.1224 - accuracy: 0.9655\n",
      "Epoch 00230: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 0s 1ms/step - loss: 0.1228 - accuracy: 0.9652 - val_loss: 0.1319 - val_accuracy: 0.9592\n",
      "Epoch 231/500\n",
      "326/338 [===========================>..] - ETA: 0s - loss: 0.1244 - accuracy: 0.9648\n",
      "Epoch 00231: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 0s 1ms/step - loss: 0.1227 - accuracy: 0.9656 - val_loss: 0.1367 - val_accuracy: 0.9583\n",
      "Epoch 232/500\n",
      "333/338 [============================>.] - ETA: 0s - loss: 0.1232 - accuracy: 0.9646\n",
      "Epoch 00232: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 0s 1ms/step - loss: 0.1227 - accuracy: 0.9648 - val_loss: 0.1358 - val_accuracy: 0.9575\n",
      "Epoch 233/500\n",
      "329/338 [============================>.] - ETA: 0s - loss: 0.1226 - accuracy: 0.9644\n",
      "Epoch 00233: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1226 - accuracy: 0.9642 - val_loss: 0.1343 - val_accuracy: 0.9583\n",
      "Epoch 234/500\n",
      "315/338 [==========================>...] - ETA: 0s - loss: 0.1229 - accuracy: 0.9660\n",
      "Epoch 00234: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 0s 1ms/step - loss: 0.1225 - accuracy: 0.9659 - val_loss: 0.1351 - val_accuracy: 0.9583\n",
      "Epoch 235/500\n",
      "332/338 [============================>.] - ETA: 0s - loss: 0.1233 - accuracy: 0.9651\n",
      "Epoch 00235: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 0s 1ms/step - loss: 0.1228 - accuracy: 0.9653 - val_loss: 0.1337 - val_accuracy: 0.9583\n",
      "Epoch 236/500\n",
      "317/338 [===========================>..] - ETA: 0s - loss: 0.1240 - accuracy: 0.9647\n",
      "Epoch 00236: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 0s 1ms/step - loss: 0.1229 - accuracy: 0.9650 - val_loss: 0.1343 - val_accuracy: 0.9583\n",
      "Epoch 237/500\n",
      "312/338 [==========================>...] - ETA: 0s - loss: 0.1240 - accuracy: 0.9651\n",
      "Epoch 00237: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 0s 1ms/step - loss: 0.1225 - accuracy: 0.9659 - val_loss: 0.1354 - val_accuracy: 0.9600\n",
      "Epoch 238/500\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 0.1202 - accuracy: 0.9653\n",
      "Epoch 00238: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 0s 1ms/step - loss: 0.1224 - accuracy: 0.9646 - val_loss: 0.1365 - val_accuracy: 0.9592\n",
      "Epoch 239/500\n",
      "309/338 [==========================>...] - ETA: 0s - loss: 0.1234 - accuracy: 0.9658\n",
      "Epoch 00239: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 0s 1ms/step - loss: 0.1225 - accuracy: 0.9659 - val_loss: 0.1352 - val_accuracy: 0.9592\n",
      "Epoch 240/500\n",
      "308/338 [==========================>...] - ETA: 0s - loss: 0.1220 - accuracy: 0.9647\n",
      "Epoch 00240: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1226 - accuracy: 0.9647 - val_loss: 0.1338 - val_accuracy: 0.9592\n",
      "Epoch 241/500\n",
      "321/338 [===========================>..] - ETA: 0s - loss: 0.1229 - accuracy: 0.9655\n",
      "Epoch 00241: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 0s 1ms/step - loss: 0.1228 - accuracy: 0.9656 - val_loss: 0.1342 - val_accuracy: 0.9575\n",
      "Epoch 242/500\n",
      "317/338 [===========================>..] - ETA: 0s - loss: 0.1219 - accuracy: 0.9650\n",
      "Epoch 00242: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 0s 1ms/step - loss: 0.1224 - accuracy: 0.9646 - val_loss: 0.1360 - val_accuracy: 0.9583\n",
      "Epoch 243/500\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 0.1230 - accuracy: 0.9652\n",
      "Epoch 00243: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 0s 1ms/step - loss: 0.1226 - accuracy: 0.9653 - val_loss: 0.1365 - val_accuracy: 0.9583\n",
      "Epoch 244/500\n",
      "336/338 [============================>.] - ETA: 0s - loss: 0.1227 - accuracy: 0.9656\n",
      "Epoch 00244: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1226 - accuracy: 0.9656 - val_loss: 0.1370 - val_accuracy: 0.9575\n",
      "Epoch 245/500\n",
      "309/338 [==========================>...] - ETA: 0s - loss: 0.1228 - accuracy: 0.9654\n",
      "Epoch 00245: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 0s 1ms/step - loss: 0.1228 - accuracy: 0.9650 - val_loss: 0.1360 - val_accuracy: 0.9575\n",
      "Epoch 246/500\n",
      "310/338 [==========================>...] - ETA: 0s - loss: 0.1237 - accuracy: 0.9653\n",
      "Epoch 00246: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 0s 1ms/step - loss: 0.1229 - accuracy: 0.9654 - val_loss: 0.1344 - val_accuracy: 0.9592\n",
      "Epoch 247/500\n",
      "317/338 [===========================>..] - ETA: 0s - loss: 0.1195 - accuracy: 0.9663\n",
      "Epoch 00247: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 0s 1ms/step - loss: 0.1225 - accuracy: 0.9654 - val_loss: 0.1354 - val_accuracy: 0.9567\n",
      "Epoch 248/500\n",
      "312/338 [==========================>...] - ETA: 0s - loss: 0.1213 - accuracy: 0.9656\n",
      "Epoch 00248: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 0s 1ms/step - loss: 0.1224 - accuracy: 0.9655 - val_loss: 0.1347 - val_accuracy: 0.9592\n",
      "Epoch 249/500\n",
      "321/338 [===========================>..] - ETA: 0s - loss: 0.1218 - accuracy: 0.9657\n",
      "Epoch 00249: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1227 - accuracy: 0.9657 - val_loss: 0.1366 - val_accuracy: 0.9592\n",
      "Epoch 250/500\n",
      "334/338 [============================>.] - ETA: 0s - loss: 0.1220 - accuracy: 0.9650\n",
      "Epoch 00250: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1222 - accuracy: 0.9648 - val_loss: 0.1348 - val_accuracy: 0.9600\n",
      "Epoch 251/500\n",
      "326/338 [===========================>..] - ETA: 0s - loss: 0.1224 - accuracy: 0.9655\n",
      "Epoch 00251: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1224 - accuracy: 0.9654 - val_loss: 0.1345 - val_accuracy: 0.9583\n",
      "Epoch 252/500\n",
      "320/338 [===========================>..] - ETA: 0s - loss: 0.1221 - accuracy: 0.9661\n",
      "Epoch 00252: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1225 - accuracy: 0.9655 - val_loss: 0.1357 - val_accuracy: 0.9583\n",
      "Epoch 253/500\n",
      "328/338 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9651\n",
      "Epoch 00253: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1225 - accuracy: 0.9656 - val_loss: 0.1353 - val_accuracy: 0.9583\n",
      "Epoch 254/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1225 - accuracy: 0.9656\n",
      "Epoch 00254: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1224 - accuracy: 0.9656 - val_loss: 0.1328 - val_accuracy: 0.9592\n",
      "Epoch 255/500\n",
      "315/338 [==========================>...] - ETA: 0s - loss: 0.1236 - accuracy: 0.9649\n",
      "Epoch 00255: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1224 - accuracy: 0.9658 - val_loss: 0.1369 - val_accuracy: 0.9592\n",
      "Epoch 256/500\n",
      "334/338 [============================>.] - ETA: 0s - loss: 0.1222 - accuracy: 0.9652\n",
      "Epoch 00256: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1223 - accuracy: 0.9652 - val_loss: 0.1370 - val_accuracy: 0.9592\n",
      "Epoch 257/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1227 - accuracy: 0.9641\n",
      "Epoch 00257: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1225 - accuracy: 0.9642 - val_loss: 0.1364 - val_accuracy: 0.9600\n",
      "Epoch 258/500\n",
      "336/338 [============================>.] - ETA: 0s - loss: 0.1219 - accuracy: 0.9659\n",
      "Epoch 00258: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1222 - accuracy: 0.9657 - val_loss: 0.1350 - val_accuracy: 0.9583\n",
      "Epoch 259/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/338 [===========================>..] - ETA: 0s - loss: 0.1212 - accuracy: 0.9659\n",
      "Epoch 00259: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1224 - accuracy: 0.9655 - val_loss: 0.1364 - val_accuracy: 0.9575\n",
      "Epoch 260/500\n",
      "337/338 [============================>.] - ETA: 0s - loss: 0.1225 - accuracy: 0.9654\n",
      "Epoch 00260: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1226 - accuracy: 0.9654 - val_loss: 0.1377 - val_accuracy: 0.9583\n",
      "Epoch 261/500\n",
      "325/338 [===========================>..] - ETA: 0s - loss: 0.1231 - accuracy: 0.9654\n",
      "Epoch 00261: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1222 - accuracy: 0.9657 - val_loss: 0.1349 - val_accuracy: 0.9592\n",
      "Epoch 262/500\n",
      "332/338 [============================>.] - ETA: 0s - loss: 0.1225 - accuracy: 0.9651\n",
      "Epoch 00262: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1222 - accuracy: 0.9651 - val_loss: 0.1368 - val_accuracy: 0.9592\n",
      "Epoch 263/500\n",
      "324/338 [===========================>..] - ETA: 0s - loss: 0.1210 - accuracy: 0.9653\n",
      "Epoch 00263: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1223 - accuracy: 0.9651 - val_loss: 0.1339 - val_accuracy: 0.9600\n",
      "Epoch 264/500\n",
      "319/338 [===========================>..] - ETA: 0s - loss: 0.1230 - accuracy: 0.9651\n",
      "Epoch 00264: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1223 - accuracy: 0.9655 - val_loss: 0.1339 - val_accuracy: 0.9583\n",
      "Epoch 265/500\n",
      "308/338 [==========================>...] - ETA: 0s - loss: 0.1226 - accuracy: 0.9650\n",
      "Epoch 00265: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1226 - accuracy: 0.9655 - val_loss: 0.1356 - val_accuracy: 0.9592\n",
      "Epoch 266/500\n",
      "325/338 [===========================>..] - ETA: 0s - loss: 0.1230 - accuracy: 0.9656\n",
      "Epoch 00266: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1225 - accuracy: 0.9658 - val_loss: 0.1364 - val_accuracy: 0.9583\n",
      "Epoch 267/500\n",
      "303/338 [=========================>....] - ETA: 0s - loss: 0.1201 - accuracy: 0.9658\n",
      "Epoch 00267: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1221 - accuracy: 0.9651 - val_loss: 0.1349 - val_accuracy: 0.9567\n",
      "Epoch 268/500\n",
      "325/338 [===========================>..] - ETA: 0s - loss: 0.1222 - accuracy: 0.9654\n",
      "Epoch 00268: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1221 - accuracy: 0.9653 - val_loss: 0.1363 - val_accuracy: 0.9600\n",
      "Epoch 269/500\n",
      "299/338 [=========================>....] - ETA: 0s - loss: 0.1230 - accuracy: 0.9656\n",
      "Epoch 00269: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1222 - accuracy: 0.9655 - val_loss: 0.1369 - val_accuracy: 0.9592\n",
      "Epoch 270/500\n",
      "317/338 [===========================>..] - ETA: 0s - loss: 0.1200 - accuracy: 0.9666\n",
      "Epoch 00270: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1224 - accuracy: 0.9655 - val_loss: 0.1359 - val_accuracy: 0.9583\n",
      "Epoch 271/500\n",
      "311/338 [==========================>...] - ETA: 0s - loss: 0.1232 - accuracy: 0.9658\n",
      "Epoch 00271: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 0s 1ms/step - loss: 0.1222 - accuracy: 0.9658 - val_loss: 0.1333 - val_accuracy: 0.9583\n",
      "Epoch 272/500\n",
      "299/338 [=========================>....] - ETA: 0s - loss: 0.1236 - accuracy: 0.9650\n",
      "Epoch 00272: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1224 - accuracy: 0.9655 - val_loss: 0.1356 - val_accuracy: 0.9583\n",
      "Epoch 273/500\n",
      "321/338 [===========================>..] - ETA: 0s - loss: 0.1221 - accuracy: 0.9658\n",
      "Epoch 00273: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9659 - val_loss: 0.1350 - val_accuracy: 0.9592\n",
      "Epoch 274/500\n",
      "327/338 [============================>.] - ETA: 0s - loss: 0.1236 - accuracy: 0.9644\n",
      "Epoch 00274: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1227 - accuracy: 0.9647 - val_loss: 0.1368 - val_accuracy: 0.9583\n",
      "Epoch 275/500\n",
      "322/338 [===========================>..] - ETA: 0s - loss: 0.1199 - accuracy: 0.9663\n",
      "Epoch 00275: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1223 - accuracy: 0.9656 - val_loss: 0.1363 - val_accuracy: 0.9600\n",
      "Epoch 276/500\n",
      "331/338 [============================>.] - ETA: 0s - loss: 0.1224 - accuracy: 0.9656\n",
      "Epoch 00276: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1221 - accuracy: 0.9655 - val_loss: 0.1364 - val_accuracy: 0.9600\n",
      "Epoch 277/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1226 - accuracy: 0.9656\n",
      "Epoch 00277: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1224 - accuracy: 0.9657 - val_loss: 0.1326 - val_accuracy: 0.9592\n",
      "Epoch 278/500\n",
      "330/338 [============================>.] - ETA: 0s - loss: 0.1214 - accuracy: 0.9651\n",
      "Epoch 00278: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1221 - accuracy: 0.9651 - val_loss: 0.1352 - val_accuracy: 0.9583\n",
      "Epoch 279/500\n",
      "338/338 [==============================] - ETA: 0s - loss: 0.1220 - accuracy: 0.9654\n",
      "Epoch 00279: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1220 - accuracy: 0.9654 - val_loss: 0.1370 - val_accuracy: 0.9583\n",
      "Epoch 280/500\n",
      "328/338 [============================>.] - ETA: 0s - loss: 0.1236 - accuracy: 0.9647\n",
      "Epoch 00280: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1223 - accuracy: 0.9652 - val_loss: 0.1360 - val_accuracy: 0.9592\n",
      "Epoch 281/500\n",
      "326/338 [===========================>..] - ETA: 0s - loss: 0.1230 - accuracy: 0.9657\n",
      "Epoch 00281: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1223 - accuracy: 0.9659 - val_loss: 0.1344 - val_accuracy: 0.9600\n",
      "Epoch 282/500\n",
      "336/338 [============================>.] - ETA: 0s - loss: 0.1222 - accuracy: 0.9656\n",
      "Epoch 00282: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1223 - accuracy: 0.9656 - val_loss: 0.1365 - val_accuracy: 0.9592\n",
      "Epoch 283/500\n",
      "315/338 [==========================>...] - ETA: 0s - loss: 0.1211 - accuracy: 0.9660\n",
      "Epoch 00283: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1222 - accuracy: 0.9656 - val_loss: 0.1352 - val_accuracy: 0.9592\n",
      "Epoch 284/500\n",
      "336/338 [============================>.] - ETA: 0s - loss: 0.1219 - accuracy: 0.9661\n",
      "Epoch 00284: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1223 - accuracy: 0.9660 - val_loss: 0.1368 - val_accuracy: 0.9592\n",
      "Epoch 285/500\n",
      "326/338 [===========================>..] - ETA: 0s - loss: 0.1219 - accuracy: 0.9658\n",
      "Epoch 00285: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1222 - accuracy: 0.9656 - val_loss: 0.1351 - val_accuracy: 0.9608\n",
      "Epoch 286/500\n",
      "329/338 [============================>.] - ETA: 0s - loss: 0.1222 - accuracy: 0.9655\n",
      "Epoch 00286: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1221 - accuracy: 0.9656 - val_loss: 0.1364 - val_accuracy: 0.9592\n",
      "Epoch 287/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1224 - accuracy: 0.9659\n",
      "Epoch 00287: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1220 - accuracy: 0.9660 - val_loss: 0.1382 - val_accuracy: 0.9575\n",
      "Epoch 288/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/338 [==========================>...] - ETA: 0s - loss: 0.1220 - accuracy: 0.9657\n",
      "Epoch 00288: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1223 - accuracy: 0.9653 - val_loss: 0.1363 - val_accuracy: 0.9583\n",
      "Epoch 289/500\n",
      "321/338 [===========================>..] - ETA: 0s - loss: 0.1229 - accuracy: 0.9654\n",
      "Epoch 00289: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1221 - accuracy: 0.9655 - val_loss: 0.1348 - val_accuracy: 0.9600\n",
      "Epoch 290/500\n",
      "305/338 [==========================>...] - ETA: 0s - loss: 0.1217 - accuracy: 0.9658\n",
      "Epoch 00290: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1220 - accuracy: 0.9659 - val_loss: 0.1366 - val_accuracy: 0.9583\n",
      "Epoch 291/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1226 - accuracy: 0.9662\n",
      "Epoch 00291: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1219 - accuracy: 0.9665 - val_loss: 0.1369 - val_accuracy: 0.9583\n",
      "Epoch 292/500\n",
      "333/338 [============================>.] - ETA: 0s - loss: 0.1219 - accuracy: 0.9656\n",
      "Epoch 00292: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9657 - val_loss: 0.1358 - val_accuracy: 0.9608\n",
      "Epoch 293/500\n",
      "332/338 [============================>.] - ETA: 0s - loss: 0.1228 - accuracy: 0.9657\n",
      "Epoch 00293: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1225 - accuracy: 0.9657 - val_loss: 0.1349 - val_accuracy: 0.9600\n",
      "Epoch 294/500\n",
      "319/338 [===========================>..] - ETA: 0s - loss: 0.1211 - accuracy: 0.9654\n",
      "Epoch 00294: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1220 - accuracy: 0.9651 - val_loss: 0.1389 - val_accuracy: 0.9583\n",
      "Epoch 295/500\n",
      "314/338 [==========================>...] - ETA: 0s - loss: 0.1223 - accuracy: 0.9656\n",
      "Epoch 00295: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1219 - accuracy: 0.9656 - val_loss: 0.1345 - val_accuracy: 0.9608\n",
      "Epoch 296/500\n",
      "307/338 [==========================>...] - ETA: 0s - loss: 0.1259 - accuracy: 0.9649\n",
      "Epoch 00296: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1221 - accuracy: 0.9663 - val_loss: 0.1346 - val_accuracy: 0.9575\n",
      "Epoch 297/500\n",
      "323/338 [===========================>..] - ETA: 0s - loss: 0.1225 - accuracy: 0.9651\n",
      "Epoch 00297: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1223 - accuracy: 0.9652 - val_loss: 0.1365 - val_accuracy: 0.9567\n",
      "Epoch 298/500\n",
      "338/338 [==============================] - ETA: 0s - loss: 0.1218 - accuracy: 0.9660\n",
      "Epoch 00298: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9660 - val_loss: 0.1373 - val_accuracy: 0.9575\n",
      "Epoch 299/500\n",
      "321/338 [===========================>..] - ETA: 0s - loss: 0.1206 - accuracy: 0.9654\n",
      "Epoch 00299: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1221 - accuracy: 0.9656 - val_loss: 0.1371 - val_accuracy: 0.9583\n",
      "Epoch 300/500\n",
      "337/338 [============================>.] - ETA: 0s - loss: 0.1216 - accuracy: 0.9655\n",
      "Epoch 00300: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 3ms/step - loss: 0.1220 - accuracy: 0.9655 - val_loss: 0.1350 - val_accuracy: 0.9600\n",
      "Epoch 301/500\n",
      "312/338 [==========================>...] - ETA: 0s - loss: 0.1228 - accuracy: 0.9656\n",
      "Epoch 00301: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1221 - accuracy: 0.9658 - val_loss: 0.1368 - val_accuracy: 0.9592\n",
      "Epoch 302/500\n",
      "314/338 [==========================>...] - ETA: 0s - loss: 0.1232 - accuracy: 0.9657\n",
      "Epoch 00302: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1220 - accuracy: 0.9661 - val_loss: 0.1340 - val_accuracy: 0.9592\n",
      "Epoch 303/500\n",
      "327/338 [============================>.] - ETA: 0s - loss: 0.1218 - accuracy: 0.9651\n",
      "Epoch 00303: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1221 - accuracy: 0.9650 - val_loss: 0.1366 - val_accuracy: 0.9608\n",
      "Epoch 304/500\n",
      "308/338 [==========================>...] - ETA: 0s - loss: 0.1219 - accuracy: 0.9656\n",
      "Epoch 00304: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1219 - accuracy: 0.9655 - val_loss: 0.1362 - val_accuracy: 0.9575\n",
      "Epoch 305/500\n",
      "332/338 [============================>.] - ETA: 0s - loss: 0.1232 - accuracy: 0.9658\n",
      "Epoch 00305: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1220 - accuracy: 0.9662 - val_loss: 0.1373 - val_accuracy: 0.9583\n",
      "Epoch 306/500\n",
      "336/338 [============================>.] - ETA: 0s - loss: 0.1221 - accuracy: 0.9654\n",
      "Epoch 00306: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1219 - accuracy: 0.9655 - val_loss: 0.1365 - val_accuracy: 0.9592\n",
      "Epoch 307/500\n",
      "327/338 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 0.9654\n",
      "Epoch 00307: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9653 - val_loss: 0.1393 - val_accuracy: 0.9575\n",
      "Epoch 308/500\n",
      "317/338 [===========================>..] - ETA: 0s - loss: 0.1225 - accuracy: 0.9656\n",
      "Epoch 00308: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1220 - accuracy: 0.9659 - val_loss: 0.1357 - val_accuracy: 0.9592\n",
      "Epoch 309/500\n",
      "305/338 [==========================>...] - ETA: 0s - loss: 0.1221 - accuracy: 0.9653\n",
      "Epoch 00309: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1220 - accuracy: 0.9653 - val_loss: 0.1379 - val_accuracy: 0.9592\n",
      "Epoch 310/500\n",
      "315/338 [==========================>...] - ETA: 0s - loss: 0.1229 - accuracy: 0.9660\n",
      "Epoch 00310: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 0s 1ms/step - loss: 0.1220 - accuracy: 0.9661 - val_loss: 0.1379 - val_accuracy: 0.9600\n",
      "Epoch 311/500\n",
      "307/338 [==========================>...] - ETA: 0s - loss: 0.1230 - accuracy: 0.9651\n",
      "Epoch 00311: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1222 - accuracy: 0.9653 - val_loss: 0.1388 - val_accuracy: 0.9583\n",
      "Epoch 312/500\n",
      "310/338 [==========================>...] - ETA: 0s - loss: 0.1232 - accuracy: 0.9661\n",
      "Epoch 00312: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1221 - accuracy: 0.9658 - val_loss: 0.1356 - val_accuracy: 0.9592\n",
      "Epoch 313/500\n",
      "302/338 [=========================>....] - ETA: 0s - loss: 0.1223 - accuracy: 0.9657\n",
      "Epoch 00313: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1219 - accuracy: 0.9655 - val_loss: 0.1362 - val_accuracy: 0.9600\n",
      "Epoch 314/500\n",
      "309/338 [==========================>...] - ETA: 0s - loss: 0.1202 - accuracy: 0.9669\n",
      "Epoch 00314: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1220 - accuracy: 0.9663 - val_loss: 0.1372 - val_accuracy: 0.9583\n",
      "Epoch 315/500\n",
      "310/338 [==========================>...] - ETA: 0s - loss: 0.1212 - accuracy: 0.9664\n",
      "Epoch 00315: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1218 - accuracy: 0.9663 - val_loss: 0.1360 - val_accuracy: 0.9592\n",
      "Epoch 316/500\n",
      "336/338 [============================>.] - ETA: 0s - loss: 0.1218 - accuracy: 0.9659 ETA: 0s - loss: 0.1124 - \n",
      "Epoch 00316: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1219 - accuracy: 0.9658 - val_loss: 0.1374 - val_accuracy: 0.9583\n",
      "Epoch 317/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301/338 [=========================>....] - ETA: 0s - loss: 0.1194 - accuracy: 0.9661\n",
      "Epoch 00317: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1220 - accuracy: 0.9656 - val_loss: 0.1375 - val_accuracy: 0.9592\n",
      "Epoch 318/500\n",
      "306/338 [==========================>...] - ETA: 0s - loss: 0.1194 - accuracy: 0.9662\n",
      "Epoch 00318: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1219 - accuracy: 0.9654 - val_loss: 0.1382 - val_accuracy: 0.9583\n",
      "Epoch 319/500\n",
      "303/338 [=========================>....] - ETA: 0s - loss: 0.1196 - accuracy: 0.9656\n",
      "Epoch 00319: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1222 - accuracy: 0.9651 - val_loss: 0.1361 - val_accuracy: 0.9600\n",
      "Epoch 320/500\n",
      "332/338 [============================>.] - ETA: 0s - loss: 0.1225 - accuracy: 0.9657\n",
      "Epoch 00320: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1219 - accuracy: 0.9660 - val_loss: 0.1368 - val_accuracy: 0.9575\n",
      "Epoch 321/500\n",
      "330/338 [============================>.] - ETA: 0s - loss: 0.1227 - accuracy: 0.9652\n",
      "Epoch 00321: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1220 - accuracy: 0.9653 - val_loss: 0.1366 - val_accuracy: 0.9575\n",
      "Epoch 322/500\n",
      "306/338 [==========================>...] - ETA: 0s - loss: 0.1198 - accuracy: 0.9654\n",
      "Epoch 00322: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9654 - val_loss: 0.1396 - val_accuracy: 0.9567\n",
      "Epoch 323/500\n",
      "337/338 [============================>.] - ETA: 0s - loss: 0.1220 - accuracy: 0.9654\n",
      "Epoch 00323: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1221 - accuracy: 0.9654 - val_loss: 0.1366 - val_accuracy: 0.9583\n",
      "Epoch 324/500\n",
      "305/338 [==========================>...] - ETA: 0s - loss: 0.1218 - accuracy: 0.9652\n",
      "Epoch 00324: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9653 - val_loss: 0.1373 - val_accuracy: 0.9592\n",
      "Epoch 325/500\n",
      "303/338 [=========================>....] - ETA: 0s - loss: 0.1223 - accuracy: 0.9661\n",
      "Epoch 00325: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1218 - accuracy: 0.9663 - val_loss: 0.1356 - val_accuracy: 0.9575\n",
      "Epoch 326/500\n",
      "309/338 [==========================>...] - ETA: 0s - loss: 0.1203 - accuracy: 0.9653\n",
      "Epoch 00326: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1220 - accuracy: 0.9647 - val_loss: 0.1394 - val_accuracy: 0.9575\n",
      "Epoch 327/500\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 0.1231 - accuracy: 0.9652\n",
      "Epoch 00327: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1219 - accuracy: 0.9656 - val_loss: 0.1345 - val_accuracy: 0.9583\n",
      "Epoch 328/500\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 0.1229 - accuracy: 0.9654\n",
      "Epoch 00328: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1219 - accuracy: 0.9657 - val_loss: 0.1358 - val_accuracy: 0.9575\n",
      "Epoch 329/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 0.9653\n",
      "Epoch 00329: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1216 - accuracy: 0.9653 - val_loss: 0.1342 - val_accuracy: 0.9592\n",
      "Epoch 330/500\n",
      "308/338 [==========================>...] - ETA: 0s - loss: 0.1213 - accuracy: 0.9661\n",
      "Epoch 00330: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1224 - accuracy: 0.9655 - val_loss: 0.1360 - val_accuracy: 0.9583\n",
      "Epoch 331/500\n",
      "338/338 [==============================] - ETA: 0s - loss: 0.1219 - accuracy: 0.9656\n",
      "Epoch 00331: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1219 - accuracy: 0.9656 - val_loss: 0.1357 - val_accuracy: 0.9592\n",
      "Epoch 332/500\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 0.1230 - accuracy: 0.9653\n",
      "Epoch 00332: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1219 - accuracy: 0.9657 - val_loss: 0.1375 - val_accuracy: 0.9567\n",
      "Epoch 333/500\n",
      "303/338 [=========================>....] - ETA: 0s - loss: 0.1210 - accuracy: 0.9654\n",
      "Epoch 00333: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1222 - accuracy: 0.9656 - val_loss: 0.1369 - val_accuracy: 0.9592\n",
      "Epoch 334/500\n",
      "336/338 [============================>.] - ETA: 0s - loss: 0.1214 - accuracy: 0.9661\n",
      "Epoch 00334: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9660 - val_loss: 0.1338 - val_accuracy: 0.9592\n",
      "Epoch 335/500\n",
      "307/338 [==========================>...] - ETA: 0s - loss: 0.1218 - accuracy: 0.9653\n",
      "Epoch 00335: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9653 - val_loss: 0.1357 - val_accuracy: 0.9567\n",
      "Epoch 336/500\n",
      "305/338 [==========================>...] - ETA: 0s - loss: 0.1207 - accuracy: 0.9664\n",
      "Epoch 00336: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1222 - accuracy: 0.9656 - val_loss: 0.1368 - val_accuracy: 0.9583\n",
      "Epoch 337/500\n",
      "302/338 [=========================>....] - ETA: 0s - loss: 0.1195 - accuracy: 0.9661\n",
      "Epoch 00337: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9656 - val_loss: 0.1373 - val_accuracy: 0.9575\n",
      "Epoch 338/500\n",
      "303/338 [=========================>....] - ETA: 0s - loss: 0.1222 - accuracy: 0.9656\n",
      "Epoch 00338: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9656 - val_loss: 0.1401 - val_accuracy: 0.9575\n",
      "Epoch 339/500\n",
      "337/338 [============================>.] - ETA: 0s - loss: 0.1221 - accuracy: 0.9651\n",
      "Epoch 00339: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1221 - accuracy: 0.9651 - val_loss: 0.1382 - val_accuracy: 0.9575\n",
      "Epoch 340/500\n",
      "314/338 [==========================>...] - ETA: 0s - loss: 0.1226 - accuracy: 0.9653\n",
      "Epoch 00340: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9655 - val_loss: 0.1383 - val_accuracy: 0.9575\n",
      "Epoch 341/500\n",
      "331/338 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9652\n",
      "Epoch 00341: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1221 - accuracy: 0.9650 - val_loss: 0.1358 - val_accuracy: 0.9592\n",
      "Epoch 342/500\n",
      "300/338 [=========================>....] - ETA: 0s - loss: 0.1216 - accuracy: 0.9655\n",
      "Epoch 00342: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1219 - accuracy: 0.9657 - val_loss: 0.1357 - val_accuracy: 0.9583\n",
      "Epoch 343/500\n",
      "328/338 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9658\n",
      "Epoch 00343: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9659 - val_loss: 0.1365 - val_accuracy: 0.9583\n",
      "Epoch 344/500\n",
      "302/338 [=========================>....] - ETA: 0s - loss: 0.1201 - accuracy: 0.9662\n",
      "Epoch 00344: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1216 - accuracy: 0.9658 - val_loss: 0.1392 - val_accuracy: 0.9575\n",
      "Epoch 345/500\n",
      "338/338 [==============================] - ETA: 0s - loss: 0.1219 - accuracy: 0.9653\n",
      "Epoch 00345: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1219 - accuracy: 0.9653 - val_loss: 0.1357 - val_accuracy: 0.9583\n",
      "Epoch 346/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304/338 [=========================>....] - ETA: 0s - loss: 0.1198 - accuracy: 0.9657\n",
      "Epoch 00346: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1219 - accuracy: 0.9651 - val_loss: 0.1386 - val_accuracy: 0.9583\n",
      "Epoch 347/500\n",
      "327/338 [============================>.] - ETA: 0s - loss: 0.1227 - accuracy: 0.9653\n",
      "Epoch 00347: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1221 - accuracy: 0.9656 - val_loss: 0.1373 - val_accuracy: 0.9583\n",
      "Epoch 348/500\n",
      "308/338 [==========================>...] - ETA: 0s - loss: 0.1227 - accuracy: 0.9645\n",
      "Epoch 00348: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1216 - accuracy: 0.9652 - val_loss: 0.1347 - val_accuracy: 0.9600\n",
      "Epoch 349/500\n",
      "324/338 [===========================>..] - ETA: 0s - loss: 0.1237 - accuracy: 0.9658\n",
      "Epoch 00349: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1220 - accuracy: 0.9663 - val_loss: 0.1379 - val_accuracy: 0.9558\n",
      "Epoch 350/500\n",
      "303/338 [=========================>....] - ETA: 0s - loss: 0.1197 - accuracy: 0.9658\n",
      "Epoch 00350: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1220 - accuracy: 0.9645 - val_loss: 0.1353 - val_accuracy: 0.9583\n",
      "Epoch 351/500\n",
      "317/338 [===========================>..] - ETA: 0s - loss: 0.1196 - accuracy: 0.9661\n",
      "Epoch 00351: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9655 - val_loss: 0.1357 - val_accuracy: 0.9583\n",
      "Epoch 352/500\n",
      "305/338 [==========================>...] - ETA: 0s - loss: 0.1254 - accuracy: 0.9639\n",
      "Epoch 00352: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1219 - accuracy: 0.9651 - val_loss: 0.1368 - val_accuracy: 0.9567\n",
      "Epoch 353/500\n",
      "331/338 [============================>.] - ETA: 0s - loss: 0.1216 - accuracy: 0.9658\n",
      "Epoch 00353: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9656 - val_loss: 0.1366 - val_accuracy: 0.9583\n",
      "Epoch 354/500\n",
      "313/338 [==========================>...] - ETA: 0s - loss: 0.1215 - accuracy: 0.9655\n",
      "Epoch 00354: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 0s 1ms/step - loss: 0.1219 - accuracy: 0.9656 - val_loss: 0.1391 - val_accuracy: 0.9592\n",
      "Epoch 355/500\n",
      "322/338 [===========================>..] - ETA: 0s - loss: 0.1201 - accuracy: 0.9655\n",
      "Epoch 00355: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1215 - accuracy: 0.9651 - val_loss: 0.1349 - val_accuracy: 0.9592\n",
      "Epoch 356/500\n",
      "306/338 [==========================>...] - ETA: 0s - loss: 0.1202 - accuracy: 0.9664\n",
      "Epoch 00356: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1217 - accuracy: 0.9658 - val_loss: 0.1394 - val_accuracy: 0.9583\n",
      "Epoch 357/500\n",
      "327/338 [============================>.] - ETA: 0s - loss: 0.1205 - accuracy: 0.9649\n",
      "Epoch 00357: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9646 - val_loss: 0.1378 - val_accuracy: 0.9583\n",
      "Epoch 358/500\n",
      "311/338 [==========================>...] - ETA: 0s - loss: 0.1225 - accuracy: 0.9657\n",
      "Epoch 00358: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 0s 1ms/step - loss: 0.1220 - accuracy: 0.9656 - val_loss: 0.1374 - val_accuracy: 0.9583\n",
      "Epoch 359/500\n",
      "337/338 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 0.9655\n",
      "Epoch 00359: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1216 - accuracy: 0.9656 - val_loss: 0.1378 - val_accuracy: 0.9583\n",
      "Epoch 360/500\n",
      "313/338 [==========================>...] - ETA: 0s - loss: 0.1220 - accuracy: 0.9655\n",
      "Epoch 00360: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 0s 1ms/step - loss: 0.1217 - accuracy: 0.9657 - val_loss: 0.1358 - val_accuracy: 0.9600\n",
      "Epoch 361/500\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 0.1210 - accuracy: 0.9649\n",
      "Epoch 00361: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1218 - accuracy: 0.9649 - val_loss: 0.1344 - val_accuracy: 0.9575\n",
      "Epoch 362/500\n",
      "314/338 [==========================>...] - ETA: 0s - loss: 0.1188 - accuracy: 0.9664\n",
      "Epoch 00362: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 0s 1ms/step - loss: 0.1221 - accuracy: 0.9656 - val_loss: 0.1360 - val_accuracy: 0.9592\n",
      "Epoch 363/500\n",
      "338/338 [==============================] - ETA: 0s - loss: 0.1218 - accuracy: 0.9653\n",
      "Epoch 00363: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9653 - val_loss: 0.1350 - val_accuracy: 0.9592\n",
      "Epoch 364/500\n",
      "338/338 [==============================] - ETA: 0s - loss: 0.1218 - accuracy: 0.9653\n",
      "Epoch 00364: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9653 - val_loss: 0.1373 - val_accuracy: 0.9583\n",
      "Epoch 365/500\n",
      "305/338 [==========================>...] - ETA: 0s - loss: 0.1202 - accuracy: 0.9656\n",
      "Epoch 00365: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9659 - val_loss: 0.1343 - val_accuracy: 0.9583\n",
      "Epoch 366/500\n",
      "302/338 [=========================>....] - ETA: 0s - loss: 0.1193 - accuracy: 0.9656\n",
      "Epoch 00366: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9651 - val_loss: 0.1361 - val_accuracy: 0.9575\n",
      "Epoch 367/500\n",
      "310/338 [==========================>...] - ETA: 0s - loss: 0.1212 - accuracy: 0.9652\n",
      "Epoch 00367: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 0s 1ms/step - loss: 0.1218 - accuracy: 0.9652 - val_loss: 0.1405 - val_accuracy: 0.9575\n",
      "Epoch 368/500\n",
      "313/338 [==========================>...] - ETA: 0s - loss: 0.1218 - accuracy: 0.9650\n",
      "Epoch 00368: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 0s 1ms/step - loss: 0.1217 - accuracy: 0.9651 - val_loss: 0.1358 - val_accuracy: 0.9600\n",
      "Epoch 369/500\n",
      "337/338 [============================>.] - ETA: 0s - loss: 0.1219 - accuracy: 0.9662\n",
      "Epoch 00369: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9662 - val_loss: 0.1373 - val_accuracy: 0.9592\n",
      "Epoch 370/500\n",
      "331/338 [============================>.] - ETA: 0s - loss: 0.1223 - accuracy: 0.9650\n",
      "Epoch 00370: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9652 - val_loss: 0.1331 - val_accuracy: 0.9600\n",
      "Epoch 371/500\n",
      "299/338 [=========================>....] - ETA: 0s - loss: 0.1199 - accuracy: 0.9659\n",
      "Epoch 00371: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9653 - val_loss: 0.1384 - val_accuracy: 0.9575\n",
      "Epoch 372/500\n",
      "328/338 [============================>.] - ETA: 0s - loss: 0.1238 - accuracy: 0.9648\n",
      "Epoch 00372: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1219 - accuracy: 0.9656 - val_loss: 0.1365 - val_accuracy: 0.9583\n",
      "Epoch 373/500\n",
      "300/338 [=========================>....] - ETA: 0s - loss: 0.1226 - accuracy: 0.9649\n",
      "Epoch 00373: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9656 - val_loss: 0.1381 - val_accuracy: 0.9583\n",
      "Epoch 374/500\n",
      "313/338 [==========================>...] - ETA: 0s - loss: 0.1200 - accuracy: 0.9662\n",
      "Epoch 00374: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 0s 1ms/step - loss: 0.1217 - accuracy: 0.9653 - val_loss: 0.1369 - val_accuracy: 0.9583\n",
      "Epoch 375/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299/338 [=========================>....] - ETA: 0s - loss: 0.1216 - accuracy: 0.9655\n",
      "Epoch 00375: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9659 - val_loss: 0.1391 - val_accuracy: 0.9583\n",
      "Epoch 376/500\n",
      "332/338 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 0.9651\n",
      "Epoch 00376: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1219 - accuracy: 0.9652 - val_loss: 0.1369 - val_accuracy: 0.9575\n",
      "Epoch 377/500\n",
      "306/338 [==========================>...] - ETA: 0s - loss: 0.1230 - accuracy: 0.9652\n",
      "Epoch 00377: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1219 - accuracy: 0.9656 - val_loss: 0.1368 - val_accuracy: 0.9600\n",
      "Epoch 378/500\n",
      "306/338 [==========================>...] - ETA: 0s - loss: 0.1228 - accuracy: 0.9650\n",
      "Epoch 00378: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1216 - accuracy: 0.9656 - val_loss: 0.1360 - val_accuracy: 0.9592\n",
      "Epoch 379/500\n",
      "305/338 [==========================>...] - ETA: 0s - loss: 0.1213 - accuracy: 0.9652\n",
      "Epoch 00379: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1217 - accuracy: 0.9653 - val_loss: 0.1360 - val_accuracy: 0.9600\n",
      "Epoch 380/500\n",
      "303/338 [=========================>....] - ETA: 0s - loss: 0.1243 - accuracy: 0.9651\n",
      "Epoch 00380: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9658 - val_loss: 0.1378 - val_accuracy: 0.9567\n",
      "Epoch 381/500\n",
      "301/338 [=========================>....] - ETA: 0s - loss: 0.1202 - accuracy: 0.9649\n",
      "Epoch 00381: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1214 - accuracy: 0.9649 - val_loss: 0.1373 - val_accuracy: 0.9583\n",
      "Epoch 382/500\n",
      "337/338 [============================>.] - ETA: 0s - loss: 0.1208 - accuracy: 0.9661\n",
      "Epoch 00382: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9658 - val_loss: 0.1364 - val_accuracy: 0.9583\n",
      "Epoch 383/500\n",
      "307/338 [==========================>...] - ETA: 0s - loss: 0.1240 - accuracy: 0.9646\n",
      "Epoch 00383: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1216 - accuracy: 0.9656 - val_loss: 0.1365 - val_accuracy: 0.9575\n",
      "Epoch 384/500\n",
      "336/338 [============================>.] - ETA: 0s - loss: 0.1222 - accuracy: 0.9648\n",
      "Epoch 00384: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1220 - accuracy: 0.9649 - val_loss: 0.1378 - val_accuracy: 0.9583\n",
      "Epoch 385/500\n",
      "310/338 [==========================>...] - ETA: 0s - loss: 0.1203 - accuracy: 0.9662\n",
      "Epoch 00385: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1218 - accuracy: 0.9655 - val_loss: 0.1368 - val_accuracy: 0.9592\n",
      "Epoch 386/500\n",
      "299/338 [=========================>....] - ETA: 0s - loss: 0.1195 - accuracy: 0.9659\n",
      "Epoch 00386: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9655 - val_loss: 0.1381 - val_accuracy: 0.9567\n",
      "Epoch 387/500\n",
      "328/338 [============================>.] - ETA: 0s - loss: 0.1214 - accuracy: 0.9653\n",
      "Epoch 00387: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9650 - val_loss: 0.1368 - val_accuracy: 0.9567\n",
      "Epoch 388/500\n",
      "300/338 [=========================>....] - ETA: 0s - loss: 0.1187 - accuracy: 0.9666\n",
      "Epoch 00388: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1216 - accuracy: 0.9656 - val_loss: 0.1386 - val_accuracy: 0.9567\n",
      "Epoch 389/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1220 - accuracy: 0.9655\n",
      "Epoch 00389: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9656 - val_loss: 0.1367 - val_accuracy: 0.9583\n",
      "Epoch 390/500\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 0.1197 - accuracy: 0.9657\n",
      "Epoch 00390: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1217 - accuracy: 0.9648 - val_loss: 0.1387 - val_accuracy: 0.9575\n",
      "Epoch 391/500\n",
      "326/338 [===========================>..] - ETA: 0s - loss: 0.1215 - accuracy: 0.9657\n",
      "Epoch 00391: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1215 - accuracy: 0.9657 - val_loss: 0.1378 - val_accuracy: 0.9575\n",
      "Epoch 392/500\n",
      "301/338 [=========================>....] - ETA: 0s - loss: 0.1222 - accuracy: 0.9651\n",
      "Epoch 00392: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1216 - accuracy: 0.9650 - val_loss: 0.1365 - val_accuracy: 0.9567\n",
      "Epoch 393/500\n",
      "306/338 [==========================>...] - ETA: 0s - loss: 0.1233 - accuracy: 0.9650\n",
      "Epoch 00393: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1215 - accuracy: 0.9654 - val_loss: 0.1399 - val_accuracy: 0.9567\n",
      "Epoch 394/500\n",
      "302/338 [=========================>....] - ETA: 0s - loss: 0.1203 - accuracy: 0.9654\n",
      "Epoch 00394: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1219 - accuracy: 0.9655 - val_loss: 0.1372 - val_accuracy: 0.9575\n",
      "Epoch 395/500\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 0.1199 - accuracy: 0.9657\n",
      "Epoch 00395: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1217 - accuracy: 0.9654 - val_loss: 0.1378 - val_accuracy: 0.9592\n",
      "Epoch 396/500\n",
      "332/338 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9655\n",
      "Epoch 00396: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9653 - val_loss: 0.1372 - val_accuracy: 0.9600\n",
      "Epoch 397/500\n",
      "300/338 [=========================>....] - ETA: 0s - loss: 0.1222 - accuracy: 0.9653\n",
      "Epoch 00397: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1219 - accuracy: 0.9656 - val_loss: 0.1382 - val_accuracy: 0.9567\n",
      "Epoch 398/500\n",
      "323/338 [===========================>..] - ETA: 0s - loss: 0.1223 - accuracy: 0.9657\n",
      "Epoch 00398: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9656 - val_loss: 0.1361 - val_accuracy: 0.9575\n",
      "Epoch 399/500\n",
      "306/338 [==========================>...] - ETA: 0s - loss: 0.1201 - accuracy: 0.9659\n",
      "Epoch 00399: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1218 - accuracy: 0.9655 - val_loss: 0.1359 - val_accuracy: 0.9583\n",
      "Epoch 400/500\n",
      "314/338 [==========================>...] - ETA: 0s - loss: 0.1228 - accuracy: 0.9665\n",
      "Epoch 00400: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9665 - val_loss: 0.1388 - val_accuracy: 0.9575\n",
      "Epoch 401/500\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 0.1216 - accuracy: 0.9660\n",
      "Epoch 00401: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1216 - accuracy: 0.9657 - val_loss: 0.1425 - val_accuracy: 0.9583\n",
      "Epoch 402/500\n",
      "323/338 [===========================>..] - ETA: 0s - loss: 0.1200 - accuracy: 0.9662\n",
      "Epoch 00402: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9656 - val_loss: 0.1403 - val_accuracy: 0.9575\n",
      "Epoch 403/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1207 - accuracy: 0.9662\n",
      "Epoch 00403: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1216 - accuracy: 0.9658 - val_loss: 0.1390 - val_accuracy: 0.9583\n",
      "Epoch 404/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329/338 [============================>.] - ETA: 0s - loss: 0.1229 - accuracy: 0.9650\n",
      "Epoch 00404: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9656 - val_loss: 0.1401 - val_accuracy: 0.9592\n",
      "Epoch 405/500\n",
      "329/338 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9657 ETA: 0s - loss: 0.1292 - ac\n",
      "Epoch 00405: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1216 - accuracy: 0.9656 - val_loss: 0.1370 - val_accuracy: 0.9583\n",
      "Epoch 406/500\n",
      "299/338 [=========================>....] - ETA: 0s - loss: 0.1241 - accuracy: 0.9642\n",
      "Epoch 00406: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9649 - val_loss: 0.1359 - val_accuracy: 0.9575\n",
      "Epoch 407/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1214 - accuracy: 0.9653\n",
      "Epoch 00407: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9653 - val_loss: 0.1365 - val_accuracy: 0.9583\n",
      "Epoch 408/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1221 - accuracy: 0.9653\n",
      "Epoch 00408: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9654 - val_loss: 0.1386 - val_accuracy: 0.9575\n",
      "Epoch 409/500\n",
      "322/338 [===========================>..] - ETA: 0s - loss: 0.1203 - accuracy: 0.9659\n",
      "Epoch 00409: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1219 - accuracy: 0.9654 - val_loss: 0.1367 - val_accuracy: 0.9575\n",
      "Epoch 410/500\n",
      "305/338 [==========================>...] - ETA: 0s - loss: 0.1203 - accuracy: 0.9663\n",
      "Epoch 00410: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1217 - accuracy: 0.9656 - val_loss: 0.1388 - val_accuracy: 0.9592\n",
      "Epoch 411/500\n",
      "302/338 [=========================>....] - ETA: 0s - loss: 0.1247 - accuracy: 0.9649\n",
      "Epoch 00411: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1216 - accuracy: 0.9657 - val_loss: 0.1363 - val_accuracy: 0.9567\n",
      "Epoch 412/500\n",
      "324/338 [===========================>..] - ETA: 0s - loss: 0.1209 - accuracy: 0.9662\n",
      "Epoch 00412: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1216 - accuracy: 0.9658 - val_loss: 0.1359 - val_accuracy: 0.9583\n",
      "Epoch 413/500\n",
      "299/338 [=========================>....] - ETA: 0s - loss: 0.1211 - accuracy: 0.9659\n",
      "Epoch 00413: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9655 - val_loss: 0.1391 - val_accuracy: 0.9575\n",
      "Epoch 414/500\n",
      "303/338 [=========================>....] - ETA: 0s - loss: 0.1235 - accuracy: 0.9650\n",
      "Epoch 00414: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1221 - accuracy: 0.9655 - val_loss: 0.1373 - val_accuracy: 0.9575\n",
      "Epoch 415/500\n",
      "302/338 [=========================>....] - ETA: 0s - loss: 0.1177 - accuracy: 0.9673\n",
      "Epoch 00415: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1214 - accuracy: 0.9656 - val_loss: 0.1378 - val_accuracy: 0.9583\n",
      "Epoch 416/500\n",
      "299/338 [=========================>....] - ETA: 0s - loss: 0.1250 - accuracy: 0.9645\n",
      "Epoch 00416: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9654 - val_loss: 0.1363 - val_accuracy: 0.9575\n",
      "Epoch 417/500\n",
      "301/338 [=========================>....] - ETA: 0s - loss: 0.1202 - accuracy: 0.9661\n",
      "Epoch 00417: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9655 - val_loss: 0.1377 - val_accuracy: 0.9567\n",
      "Epoch 418/500\n",
      "301/338 [=========================>....] - ETA: 0s - loss: 0.1225 - accuracy: 0.9650\n",
      "Epoch 00418: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1219 - accuracy: 0.9652 - val_loss: 0.1376 - val_accuracy: 0.9575\n",
      "Epoch 419/500\n",
      "332/338 [============================>.] - ETA: 0s - loss: 0.1220 - accuracy: 0.9652\n",
      "Epoch 00419: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9653 - val_loss: 0.1380 - val_accuracy: 0.9575\n",
      "Epoch 420/500\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 0.1210 - accuracy: 0.9652\n",
      "Epoch 00420: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9651 - val_loss: 0.1368 - val_accuracy: 0.9583\n",
      "Epoch 421/500\n",
      "338/338 [==============================] - ETA: 0s - loss: 0.1217 - accuracy: 0.9659\n",
      "Epoch 00421: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9659 - val_loss: 0.1381 - val_accuracy: 0.9575\n",
      "Epoch 422/500\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 0.1239 - accuracy: 0.9644\n",
      "Epoch 00422: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9654 - val_loss: 0.1370 - val_accuracy: 0.9567\n",
      "Epoch 423/500\n",
      "303/338 [=========================>....] - ETA: 0s - loss: 0.1212 - accuracy: 0.9646\n",
      "Epoch 00423: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1217 - accuracy: 0.9650 - val_loss: 0.1383 - val_accuracy: 0.9575\n",
      "Epoch 424/500\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 0.1216 - accuracy: 0.9659\n",
      "Epoch 00424: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1219 - accuracy: 0.9659 - val_loss: 0.1401 - val_accuracy: 0.9583\n",
      "Epoch 425/500\n",
      "307/338 [==========================>...] - ETA: 0s - loss: 0.1210 - accuracy: 0.9660\n",
      "Epoch 00425: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1217 - accuracy: 0.9656 - val_loss: 0.1368 - val_accuracy: 0.9583\n",
      "Epoch 426/500\n",
      "328/338 [============================>.] - ETA: 0s - loss: 0.1201 - accuracy: 0.9660\n",
      "Epoch 00426: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1219 - accuracy: 0.9653 - val_loss: 0.1386 - val_accuracy: 0.9575\n",
      "Epoch 427/500\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 0.1205 - accuracy: 0.9656\n",
      "Epoch 00427: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1213 - accuracy: 0.9649 - val_loss: 0.1368 - val_accuracy: 0.9575\n",
      "Epoch 428/500\n",
      "337/338 [============================>.] - ETA: 0s - loss: 0.1216 - accuracy: 0.9654\n",
      "Epoch 00428: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1216 - accuracy: 0.9654 - val_loss: 0.1398 - val_accuracy: 0.9583\n",
      "Epoch 429/500\n",
      "303/338 [=========================>....] - ETA: 0s - loss: 0.1230 - accuracy: 0.9654\n",
      "Epoch 00429: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9659 - val_loss: 0.1372 - val_accuracy: 0.9583\n",
      "Epoch 430/500\n",
      "329/338 [============================>.] - ETA: 0s - loss: 0.1220 - accuracy: 0.9655\n",
      "Epoch 00430: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9656 - val_loss: 0.1372 - val_accuracy: 0.9575\n",
      "Epoch 431/500\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 0.1211 - accuracy: 0.9658\n",
      "Epoch 00431: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1216 - accuracy: 0.9655 - val_loss: 0.1371 - val_accuracy: 0.9575\n",
      "Epoch 432/500\n",
      "331/338 [============================>.] - ETA: 0s - loss: 0.1198 - accuracy: 0.9656\n",
      "Epoch 00432: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1212 - accuracy: 0.9654 - val_loss: 0.1428 - val_accuracy: 0.9575\n",
      "Epoch 433/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299/338 [=========================>....] - ETA: 0s - loss: 0.1214 - accuracy: 0.9656\n",
      "Epoch 00433: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1219 - accuracy: 0.9656 - val_loss: 0.1373 - val_accuracy: 0.9600\n",
      "Epoch 434/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1204 - accuracy: 0.9667\n",
      "Epoch 00434: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9664 - val_loss: 0.1367 - val_accuracy: 0.9583\n",
      "Epoch 435/500\n",
      "301/338 [=========================>....] - ETA: 0s - loss: 0.1242 - accuracy: 0.9645\n",
      "Epoch 00435: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1217 - accuracy: 0.9648 - val_loss: 0.1371 - val_accuracy: 0.9575\n",
      "Epoch 436/500\n",
      "334/338 [============================>.] - ETA: 0s - loss: 0.1214 - accuracy: 0.9661\n",
      "Epoch 00436: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1216 - accuracy: 0.9662 - val_loss: 0.1392 - val_accuracy: 0.9567\n",
      "Epoch 437/500\n",
      "337/338 [============================>.] - ETA: 0s - loss: 0.1214 - accuracy: 0.9658\n",
      "Epoch 00437: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1213 - accuracy: 0.9658 - val_loss: 0.1442 - val_accuracy: 0.9575\n",
      "Epoch 438/500\n",
      "298/338 [=========================>....] - ETA: 0s - loss: 0.1208 - accuracy: 0.9643\n",
      "Epoch 00438: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9643 - val_loss: 0.1363 - val_accuracy: 0.9575\n",
      "Epoch 439/500\n",
      "336/338 [============================>.] - ETA: 0s - loss: 0.1222 - accuracy: 0.9657\n",
      "Epoch 00439: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9658 - val_loss: 0.1387 - val_accuracy: 0.9583\n",
      "Epoch 440/500\n",
      "299/338 [=========================>....] - ETA: 0s - loss: 0.1209 - accuracy: 0.9655\n",
      "Epoch 00440: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1216 - accuracy: 0.9656 - val_loss: 0.1361 - val_accuracy: 0.9592\n",
      "Epoch 441/500\n",
      "307/338 [==========================>...] - ETA: 0s - loss: 0.1203 - accuracy: 0.9657\n",
      "Epoch 00441: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1217 - accuracy: 0.9654 - val_loss: 0.1367 - val_accuracy: 0.9592\n",
      "Epoch 442/500\n",
      "303/338 [=========================>....] - ETA: 0s - loss: 0.1242 - accuracy: 0.9648\n",
      "Epoch 00442: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9655 - val_loss: 0.1359 - val_accuracy: 0.9583\n",
      "Epoch 443/500\n",
      "305/338 [==========================>...] - ETA: 0s - loss: 0.1211 - accuracy: 0.9660\n",
      "Epoch 00443: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1214 - accuracy: 0.9658 - val_loss: 0.1362 - val_accuracy: 0.9575\n",
      "Epoch 444/500\n",
      "299/338 [=========================>....] - ETA: 0s - loss: 0.1225 - accuracy: 0.9653\n",
      "Epoch 00444: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1214 - accuracy: 0.9654 - val_loss: 0.1386 - val_accuracy: 0.9575\n",
      "Epoch 445/500\n",
      "302/338 [=========================>....] - ETA: 0s - loss: 0.1205 - accuracy: 0.9660\n",
      "Epoch 00445: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1214 - accuracy: 0.9657 - val_loss: 0.1394 - val_accuracy: 0.9583\n",
      "Epoch 446/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1212 - accuracy: 0.9648\n",
      "Epoch 00446: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9647 - val_loss: 0.1362 - val_accuracy: 0.9583\n",
      "Epoch 447/500\n",
      "301/338 [=========================>....] - ETA: 0s - loss: 0.1233 - accuracy: 0.9648\n",
      "Epoch 00447: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1219 - accuracy: 0.9652 - val_loss: 0.1407 - val_accuracy: 0.9583\n",
      "Epoch 448/500\n",
      "337/338 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9652\n",
      "Epoch 00448: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9651 - val_loss: 0.1377 - val_accuracy: 0.9575\n",
      "Epoch 449/500\n",
      "299/338 [=========================>....] - ETA: 0s - loss: 0.1226 - accuracy: 0.9640\n",
      "Epoch 00449: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1219 - accuracy: 0.9646 - val_loss: 0.1374 - val_accuracy: 0.9575\n",
      "Epoch 450/500\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 0.1205 - accuracy: 0.9658\n",
      "Epoch 00450: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1219 - accuracy: 0.9652 - val_loss: 0.1375 - val_accuracy: 0.9592\n",
      "Epoch 451/500\n",
      "338/338 [==============================] - ETA: 0s - loss: 0.1219 - accuracy: 0.9649\n",
      "Epoch 00451: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1219 - accuracy: 0.9649 - val_loss: 0.1373 - val_accuracy: 0.9575\n",
      "Epoch 452/500\n",
      "307/338 [==========================>...] - ETA: 0s - loss: 0.1217 - accuracy: 0.9656\n",
      "Epoch 00452: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1216 - accuracy: 0.9656 - val_loss: 0.1382 - val_accuracy: 0.9575\n",
      "Epoch 453/500\n",
      "300/338 [=========================>....] - ETA: 0s - loss: 0.1211 - accuracy: 0.9656\n",
      "Epoch 00453: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9659 - val_loss: 0.1396 - val_accuracy: 0.9558\n",
      "Epoch 454/500\n",
      "338/338 [==============================] - ETA: 0s - loss: 0.1216 - accuracy: 0.9654\n",
      "Epoch 00454: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1216 - accuracy: 0.9654 - val_loss: 0.1384 - val_accuracy: 0.9575\n",
      "Epoch 455/500\n",
      "306/338 [==========================>...] - ETA: 0s - loss: 0.1211 - accuracy: 0.9658\n",
      "Epoch 00455: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1221 - accuracy: 0.9654 - val_loss: 0.1384 - val_accuracy: 0.9575\n",
      "Epoch 456/500\n",
      "337/338 [============================>.] - ETA: 0s - loss: 0.1216 - accuracy: 0.9652\n",
      "Epoch 00456: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1214 - accuracy: 0.9653 - val_loss: 0.1366 - val_accuracy: 0.9583\n",
      "Epoch 457/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1211 - accuracy: 0.9653\n",
      "Epoch 00457: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1214 - accuracy: 0.9653 - val_loss: 0.1373 - val_accuracy: 0.9583\n",
      "Epoch 458/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1214 - accuracy: 0.9659\n",
      "Epoch 00458: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1216 - accuracy: 0.9657 - val_loss: 0.1376 - val_accuracy: 0.9575\n",
      "Epoch 459/500\n",
      "299/338 [=========================>....] - ETA: 0s - loss: 0.1214 - accuracy: 0.9652\n",
      "Epoch 00459: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9649 - val_loss: 0.1386 - val_accuracy: 0.9567\n",
      "Epoch 460/500\n",
      "336/338 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9663\n",
      "Epoch 00460: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9661 - val_loss: 0.1376 - val_accuracy: 0.9558\n",
      "Epoch 461/500\n",
      "331/338 [============================>.] - ETA: 0s - loss: 0.1216 - accuracy: 0.9650\n",
      "Epoch 00461: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9649 - val_loss: 0.1358 - val_accuracy: 0.9575\n",
      "Epoch 462/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328/338 [============================>.] - ETA: 0s - loss: 0.1225 - accuracy: 0.9654\n",
      "Epoch 00462: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1215 - accuracy: 0.9657 - val_loss: 0.1354 - val_accuracy: 0.9583\n",
      "Epoch 463/500\n",
      "303/338 [=========================>....] - ETA: 0s - loss: 0.1211 - accuracy: 0.9649\n",
      "Epoch 00463: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1218 - accuracy: 0.9648 - val_loss: 0.1380 - val_accuracy: 0.9575\n",
      "Epoch 464/500\n",
      "309/338 [==========================>...] - ETA: 0s - loss: 0.1202 - accuracy: 0.9660\n",
      "Epoch 00464: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1217 - accuracy: 0.9651 - val_loss: 0.1367 - val_accuracy: 0.9583\n",
      "Epoch 465/500\n",
      "327/338 [============================>.] - ETA: 0s - loss: 0.1232 - accuracy: 0.9657\n",
      "Epoch 00465: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1219 - accuracy: 0.9660 - val_loss: 0.1392 - val_accuracy: 0.9575\n",
      "Epoch 466/500\n",
      "301/338 [=========================>....] - ETA: 0s - loss: 0.1272 - accuracy: 0.9636\n",
      "Epoch 00466: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1215 - accuracy: 0.9653 - val_loss: 0.1350 - val_accuracy: 0.9575\n",
      "Epoch 467/500\n",
      "305/338 [==========================>...] - ETA: 0s - loss: 0.1179 - accuracy: 0.9667\n",
      "Epoch 00467: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1215 - accuracy: 0.9652 - val_loss: 0.1397 - val_accuracy: 0.9575\n",
      "Epoch 468/500\n",
      "305/338 [==========================>...] - ETA: 0s - loss: 0.1210 - accuracy: 0.9657\n",
      "Epoch 00468: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9655 - val_loss: 0.1397 - val_accuracy: 0.9575\n",
      "Epoch 469/500\n",
      "338/338 [==============================] - ETA: 0s - loss: 0.1217 - accuracy: 0.9654\n",
      "Epoch 00469: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9654 - val_loss: 0.1404 - val_accuracy: 0.9575\n",
      "Epoch 470/500\n",
      "305/338 [==========================>...] - ETA: 0s - loss: 0.1198 - accuracy: 0.9656\n",
      "Epoch 00470: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1216 - accuracy: 0.9654 - val_loss: 0.1382 - val_accuracy: 0.9583\n",
      "Epoch 471/500\n",
      "336/338 [============================>.] - ETA: 0s - loss: 0.1210 - accuracy: 0.9666\n",
      "Epoch 00471: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1214 - accuracy: 0.9665 - val_loss: 0.1353 - val_accuracy: 0.9592\n",
      "Epoch 472/500\n",
      "305/338 [==========================>...] - ETA: 0s - loss: 0.1196 - accuracy: 0.9662\n",
      "Epoch 00472: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1214 - accuracy: 0.9658 - val_loss: 0.1381 - val_accuracy: 0.9583\n",
      "Epoch 473/500\n",
      "302/338 [=========================>....] - ETA: 0s - loss: 0.1187 - accuracy: 0.9663\n",
      "Epoch 00473: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1216 - accuracy: 0.9654 - val_loss: 0.1360 - val_accuracy: 0.9583\n",
      "Epoch 474/500\n",
      "303/338 [=========================>....] - ETA: 0s - loss: 0.1232 - accuracy: 0.9656\n",
      "Epoch 00474: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1216 - accuracy: 0.9660 - val_loss: 0.1404 - val_accuracy: 0.9575\n",
      "Epoch 475/500\n",
      "337/338 [============================>.] - ETA: 0s - loss: 0.1218 - accuracy: 0.9654\n",
      "Epoch 00475: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9655 - val_loss: 0.1376 - val_accuracy: 0.9575\n",
      "Epoch 476/500\n",
      "333/338 [============================>.] - ETA: 0s - loss: 0.1220 - accuracy: 0.9652\n",
      "Epoch 00476: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9653 - val_loss: 0.1356 - val_accuracy: 0.9583\n",
      "Epoch 477/500\n",
      "306/338 [==========================>...] - ETA: 0s - loss: 0.1226 - accuracy: 0.9649\n",
      "Epoch 00477: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1216 - accuracy: 0.9655 - val_loss: 0.1376 - val_accuracy: 0.9583\n",
      "Epoch 478/500\n",
      "336/338 [============================>.] - ETA: 0s - loss: 0.1220 - accuracy: 0.9648\n",
      "Epoch 00478: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1216 - accuracy: 0.9649 - val_loss: 0.1389 - val_accuracy: 0.9567\n",
      "Epoch 479/500\n",
      "304/338 [=========================>....] - ETA: 0s - loss: 0.1225 - accuracy: 0.9650\n",
      "Epoch 00479: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1217 - accuracy: 0.9653 - val_loss: 0.1380 - val_accuracy: 0.9567\n",
      "Epoch 480/500\n",
      "302/338 [=========================>....] - ETA: 0s - loss: 0.1201 - accuracy: 0.9655\n",
      "Epoch 00480: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9647 - val_loss: 0.1373 - val_accuracy: 0.9583\n",
      "Epoch 481/500\n",
      "303/338 [=========================>....] - ETA: 0s - loss: 0.1226 - accuracy: 0.9652\n",
      "Epoch 00481: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1215 - accuracy: 0.9654 - val_loss: 0.1374 - val_accuracy: 0.9567\n",
      "Epoch 482/500\n",
      "305/338 [==========================>...] - ETA: 0s - loss: 0.1219 - accuracy: 0.9645\n",
      "Epoch 00482: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1215 - accuracy: 0.9646 - val_loss: 0.1409 - val_accuracy: 0.9575\n",
      "Epoch 483/500\n",
      "335/338 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9659\n",
      "Epoch 00483: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1219 - accuracy: 0.9657 - val_loss: 0.1374 - val_accuracy: 0.9583\n",
      "Epoch 484/500\n",
      "301/338 [=========================>....] - ETA: 0s - loss: 0.1160 - accuracy: 0.9671\n",
      "Epoch 00484: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1216 - accuracy: 0.9656 - val_loss: 0.1403 - val_accuracy: 0.9575\n",
      "Epoch 485/500\n",
      "302/338 [=========================>....] - ETA: 0s - loss: 0.1204 - accuracy: 0.9661\n",
      "Epoch 00485: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9652 - val_loss: 0.1376 - val_accuracy: 0.9583\n",
      "Epoch 486/500\n",
      "337/338 [============================>.] - ETA: 0s - loss: 0.1216 - accuracy: 0.9660\n",
      "Epoch 00486: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1215 - accuracy: 0.9660 - val_loss: 0.1381 - val_accuracy: 0.9583\n",
      "Epoch 487/500\n",
      "337/338 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 0.9648\n",
      "Epoch 00487: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1216 - accuracy: 0.9648 - val_loss: 0.1377 - val_accuracy: 0.9592\n",
      "Epoch 488/500\n",
      "317/338 [===========================>..] - ETA: 0s - loss: 0.1228 - accuracy: 0.9655\n",
      "Epoch 00488: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1214 - accuracy: 0.9660 - val_loss: 0.1382 - val_accuracy: 0.9575\n",
      "Epoch 489/500\n",
      "333/338 [============================>.] - ETA: 0s - loss: 0.1223 - accuracy: 0.9646\n",
      "Epoch 00489: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1216 - accuracy: 0.9650 - val_loss: 0.1408 - val_accuracy: 0.9567\n",
      "Epoch 490/500\n",
      "328/338 [============================>.] - ETA: 0s - loss: 0.1224 - accuracy: 0.9652\n",
      "Epoch 00490: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1215 - accuracy: 0.9655 - val_loss: 0.1371 - val_accuracy: 0.9575\n",
      "Epoch 491/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "335/338 [============================>.] - ETA: 0s - loss: 0.1206 - accuracy: 0.9654\n",
      "Epoch 00491: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1213 - accuracy: 0.9652 - val_loss: 0.1378 - val_accuracy: 0.9575\n",
      "Epoch 492/500\n",
      "302/338 [=========================>....] - ETA: 0s - loss: 0.1208 - accuracy: 0.9646\n",
      "Epoch 00492: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1215 - accuracy: 0.9651 - val_loss: 0.1380 - val_accuracy: 0.9575\n",
      "Epoch 493/500\n",
      "301/338 [=========================>....] - ETA: 0s - loss: 0.1218 - accuracy: 0.9653\n",
      "Epoch 00493: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1216 - accuracy: 0.9653 - val_loss: 0.1369 - val_accuracy: 0.9583\n",
      "Epoch 494/500\n",
      "330/338 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9652\n",
      "Epoch 00494: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1215 - accuracy: 0.9653 - val_loss: 0.1361 - val_accuracy: 0.9575\n",
      "Epoch 495/500\n",
      "338/338 [==============================] - ETA: 0s - loss: 0.1217 - accuracy: 0.9656\n",
      "Epoch 00495: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9656 - val_loss: 0.1372 - val_accuracy: 0.9583\n",
      "Epoch 496/500\n",
      "308/338 [==========================>...] - ETA: 0s - loss: 0.1223 - accuracy: 0.9660\n",
      "Epoch 00496: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1215 - accuracy: 0.9656 - val_loss: 0.1362 - val_accuracy: 0.9583\n",
      "Epoch 497/500\n",
      "332/338 [============================>.] - ETA: 0s - loss: 0.1218 - accuracy: 0.9661\n",
      "Epoch 00497: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1219 - accuracy: 0.9659 - val_loss: 0.1360 - val_accuracy: 0.9583\n",
      "Epoch 498/500\n",
      "300/338 [=========================>....] - ETA: 0s - loss: 0.1199 - accuracy: 0.9657\n",
      "Epoch 00498: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1214 - accuracy: 0.9657 - val_loss: 0.1367 - val_accuracy: 0.9583\n",
      "Epoch 499/500\n",
      "326/338 [===========================>..] - ETA: 0s - loss: 0.1213 - accuracy: 0.9651\n",
      "Epoch 00499: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9650 - val_loss: 0.1369 - val_accuracy: 0.9583\n",
      "Epoch 500/500\n",
      "308/338 [==========================>...] - ETA: 0s - loss: 0.1189 - accuracy: 0.9666\n",
      "Epoch 00500: val_loss did not improve from 0.13013\n",
      "338/338 [==============================] - 1s 1ms/step - loss: 0.1212 - accuracy: 0.9657 - val_loss: 0.1423 - val_accuracy: 0.9575\n"
     ]
    }
   ],
   "source": [
    "#checkpoint to hold the best weights found yet\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=\"weights.h5\", verbose=1, save_best_only=True)\n",
    "#here we train the model\n",
    "historic = model.fit(X_train, y_train, validation_split=0.1, batch_size=32, epochs=500, verbose=1, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "17438f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_26 (Dense)            (None, 5)                 90        \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 4)                 24        \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 119\n",
      "Trainable params: 119\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "709a4f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAuaElEQVR4nO3deZxcVZ338c+vtt6zJxASIAHDlhCSEAKIsoggiwIKIxmVGZwBBkZG8RkdcJlR5xmfwVEZXFBExdERQQTZNAgiuwImwRCyEBIgkE4g+9Zrbb/nj3Oru7pTCZ2l0knu9/169aur7lbn1HK/95xz65a5OyIiEl+J/i6AiIj0LwWBiEjMKQhERGJOQSAiEnMKAhGRmFMQiIjEnIJApI/M7H/M7D/6uOxSM3vvzm5HZHdQEIiIxJyCQEQk5hQEsk+JumQ+a2ZzzazVzH5sZvuZ2YNmttnMHjGzwWXLn2dm881sg5k9bmZHls2bbGbPR+v9Eqjt9VjvN7M50bp/MrOJO1jmy81siZmtM7P7zeyAaLqZ2X+b2Soz2xjVaUI07xwzWxCVbbmZfWaHnjARFASyb7oQOAM4DPgA8CDweWAY4T3/SQAzOwy4HbgGGA7MAB4ws4yZZYB7gf8FhgC/irZLtO4U4FbgH4ChwA+A+82sZnsKambvAf4T+DAwEngduCOafSZwclSPQcDFwNpo3o+Bf3D3JmAC8Oj2PK5IOQWB7Iu+4+4r3X058BTwnLv/xd07gXuAydFyFwO/dfffu3sO+AZQB7wTOAFIAze6e87d7wJmlj3G5cAP3P05dy+4+0+Bzmi97fFR4FZ3fz4q3+eAE81sDJADmoAjAHP3he7+ZrReDjjKzAa4+3p3f347H1eki4JA9kUry263V7jfGN0+gHAEDoC7F4FlwKho3nLveVXG18tuHwz8c9QttMHMNgAHRuttj95laCEc9Y9y90eB7wI3ASvN7BYzGxAteiFwDvC6mT1hZidu5+OKdFEQSJytIOzQgdAnT9iZLwfeBEZF00oOKru9DPiquw8q+6t399t3sgwNhK6m5QDu/m13PxYYT+gi+mw0faa7nw+MIHRh3bmdjyvSRUEgcXYncK6ZnW5maeCfCd07fwKeAfLAJ80sZWYfAqaVrftD4EozOz4a1G0ws3PNrGk7y/AL4ONmNikaX/h/hK6spWZ2XLT9NNAKdACFaAzjo2Y2MOrS2gQUduJ5kJhTEEhsufsi4GPAd4A1hIHlD7h71t2zwIeAS4H1hPGEX5etO4swTvDdaP6SaNntLcMfgH8F7ia0Qg4FpkezBxACZz2h+2gtYRwD4BJgqZltAq6M6iGyQ0w/TCMiEm9qEYiIxJyCQEQk5hQEIiIxpyAQEYm5VH8XYHsNGzbMx4wZ09/FEBHZq8yePXuNuw+vNG+vC4IxY8Ywa9as/i6GiMhexcxe39o8dQ2JiMScgkBEJOYUBCIiMbfXjRGIyL4ll8vR3NxMR0dHfxdln1BbW8vo0aNJp9N9XkdBICL9qrm5maamJsaMGUPPi73K9nJ31q5dS3NzM2PHju3zeuoaEpF+1dHRwdChQxUCu4CZMXTo0O1uXSkIRKTfKQR2nR15LmMTBIve2swNDy9iTUtnfxdFRGSPEpsgWLKqhW8/uoR1rdn+LoqI7EE2bNjA9773ve1e75xzzmHDhg27vkD9IDZBkIhaS4Wifn9BRLptLQgKhW3/6NuMGTMYNGhQlUq1e8XmrKFElARF/RCPiJS57rrreOWVV5g0aRLpdJrGxkZGjhzJnDlzWLBgARdccAHLli2jo6ODT33qU1xxxRVA9+VuWlpaOPvss3nXu97Fn/70J0aNGsV9991HXV1dP9es7+ITBNEASrHYzwURka36ygPzWbBi0y7d5lEHDOBLHxi/1fnXX3898+bNY86cOTz++OOce+65zJs3r+v0y1tvvZUhQ4bQ3t7Occcdx4UXXsjQoUN7bGPx4sXcfvvt/PCHP+TDH/4wd999Nx/72N7z66GxCYJk1AmmFoGIbMu0adN6nIP/7W9/m3vuuQeAZcuWsXjx4i2CYOzYsUyaNAmAY489lqVLl+6u4u4SsQmC0ilVBQWByB5rW0fuu0tDQ0PX7ccff5xHHnmEZ555hvr6ek499dSK5+jX1NR03U4mk7S3t++Wsu4qsRksTkZB4AoCESnT1NTE5s2bK87buHEjgwcPpr6+npdeeolnn312N5du94hNi6A0RlDQGIGIlBk6dCgnnXQSEyZMoK6ujv32269r3llnncXNN9/MxIkTOfzwwznhhBP6saTVE58g0BiBiGzFL37xi4rTa2pqePDBByvOK40DDBs2jHnz5nVN/8xnPrPLy1dtseka6j5rSEEgIlIufkGgHBAR6SE2QaDTR0VEKotNEOj0URGRymITBDp9VESkstgEgU4fFRGpLD5BoDECEdkFGhsbAVixYgUXXXRRxWVOPfVUZs2atc3t3HjjjbS1tXXd78/LWscnCHT6qIjsQgcccAB33XXXDq/fOwj687LWsQmCZEKnj4rIlq699toev0fw5S9/ma985SucfvrpTJkyhaOPPpr77rtvi/WWLl3KhAkTAGhvb2f69OlMnDiRiy++uMe1hq666iqmTp3K+PHj+dKXvgSEC9mtWLGC0047jdNOOw0Il7Ves2YNADfccAMTJkxgwoQJ3HjjjV2Pd+SRR3L55Zczfvx4zjzzzF12TaP4fLO49MM06hoS2XM9eB289eKu3eb+R8PZ12919vTp07nmmmv4x3/8RwDuvPNOfve73/HpT3+aAQMGsGbNGk444QTOO++8rf4e8Pe//33q6+uZO3cuc+fOZcqUKV3zvvrVrzJkyBAKhQKnn346c+fO5ZOf/CQ33HADjz32GMOGDeuxrdmzZ/OTn/yE5557Dnfn+OOP55RTTmHw4MFVu9x1VVsEZnaWmS0ysyVmdt1WljnVzOaY2Xwze6JaZUnorCERqWDy5MmsWrWKFStW8MILLzB48GBGjhzJ5z//eSZOnMh73/teli9fzsqVK7e6jSeffLJrhzxx4kQmTpzYNe/OO+9kypQpTJ48mfnz57NgwYJtlufpp5/mgx/8IA0NDTQ2NvKhD32Ip556Cqje5a6r1iIwsyRwE3AG0AzMNLP73X1B2TKDgO8BZ7n7G2Y2olrl6T5rSEEgssfaxpF7NV100UXcddddvPXWW0yfPp3bbruN1atXM3v2bNLpNGPGjKl4+elylVoLr732Gt/4xjeYOXMmgwcP5tJLL33b7WzrYLVal7uuZotgGrDE3V919yxwB3B+r2U+Avza3d8AcPdV1SqMxghEZGumT5/OHXfcwV133cVFF13Exo0bGTFiBOl0mscee4zXX399m+uffPLJ3HbbbQDMmzePuXPnArBp0yYaGhoYOHAgK1eu7HEBu61d/vrkk0/m3nvvpa2tjdbWVu655x7e/e5378LabqmaYwSjgGVl95uB43stcxiQNrPHgSbgW+7+s94bMrMrgCsADjrooB0qTCmsddaQiPQ2fvx4Nm/ezKhRoxg5ciQf/ehH+cAHPsDUqVOZNGkSRxxxxDbXv+qqq/j4xz/OxIkTmTRpEtOmTQPgmGOOYfLkyYwfP55DDjmEk046qWudK664grPPPpuRI0fy2GOPdU2fMmUKl156adc2LrvsMiZPnlzVXz2zavWZm9lfAe9z98ui+5cA09z9n8qW+S4wFTgdqAOeAc5195e3tt2pU6f6252fW8mbG9s58T8f5foPHc30aTsWJiKy6y1cuJAjjzyyv4uxT6n0nJrZbHefWmn5arYImoEDy+6PBlZUWGaNu7cCrWb2JHAMsNUg2FEJXWtIRKSiao4RzATGmdlYM8sA04H7ey1zH/BuM0uZWT2h62hhNQqjy1CLiFRWtRaBu+fN7GrgISAJ3Oru883symj+ze6+0Mx+B8wFisCP3H3e1re64xIaIxDZY7n7Vs/Rl+2zI939Vf1CmbvPAGb0mnZzr/tfB75ezXJA+VlDCgKRPUltbS1r165l6NChCoOd5O6sXbuW2tra7VovNt8sNn2PQGSPNHr0aJqbm1m9enV/F2WfUFtby+jRo7drndgEQalrSA0CkT1LOp1m7Nix/V2MWIvhReeUBCIi5WITBDp9VESkstgFgXJARKSnGAVB+K/BYhGRnmITBBojEBGpLDZBYPqpShGRimITBBBaBcoBEZGeYhUECdNZQyIivcUsCExjBCIivcQvCNQ3JCLSQ6yCQGMEIiJbilUQmOl7BCIivcUqCJIJ26FrdYuI7MtiFQQJM501JCLSS+yCQD1DIiI9xScIXn2cnxQ+x8CO5f1dEhGRPUp8gqB9A0f7y6QLbf1dEhGRPUp8giAR/RhbodC/5RAR2cPELwhcQSAiUi52QWCe7+eCiIjsWWIUBMnwv6gWgYhIuRgFQalFoCAQESkXvyAoqmtIRKRc/IJAYwQiIj3EKAg0RiAiUkmMgiC0CBIKAhGRHqoaBGZ2lpktMrMlZnZdhfmnmtlGM5sT/f1b1QrT9T0CdQ2JiJRLVWvDZpYEbgLOAJqBmWZ2v7sv6LXoU+7+/mqVo4vOGhIRqaiaLYJpwBJ3f9Xds8AdwPlVfLxti8YIEjprSESkh2oGwShgWdn95mhabyea2Qtm9qCZja+0ITO7wsxmmdms1atX71hpoiBQi0BEpKdqBoFVmNb71wCeBw5292OA7wD3VtqQu9/i7lPdferw4cN3rDQ6fVREpKJqBkEzcGDZ/dHAivIF3H2Tu7dEt2cAaTMbVpXSdAVBsSqbFxHZW1UzCGYC48xsrJllgOnA/eULmNn+ZmbR7WlRedZWpTQaLBYRqahqZw25e97MrgYeApLAre4+38yujObfDFwEXGVmeaAdmO7V+nX50hiBBotFRHqoWhBAV3fPjF7Tbi67/V3gu9UsQxeNEYiIVBS7bxbrEhMiIj3FMAjUIhARKRefIDB9j0BEpJL4BEEiQZGELjonItJLfIIAKFpSF50TEekldkGg00dFRHqKVxCQJKExAhGRHmIVBG5JDRaLiPQSqyAoKghERLYQqyBwC11D1bqKhYjI3iheQZBIkqRIvqggEBEpiVcQWIqkFcgXFAQiIiUxC4IkKYrkivpNAhGRkngFQSJFkgK5vIJARKQkZkEQWgQaIxAR6RarIMCiFkFBLQIRkZJYBUFoEWiwWESkXKyCgEQqOn1ULQIRkZLYBUGKAjm1CEREusQuCJJWVNeQiEiZmAVBkjR5shosFhHpErMgiMYIFAQiIl1iFQSWSJOmoO8RiIiUiVUQkEyTIq/vEYiIlIlVEFiy1DWkFoGISEnMgiBNmry+RyAiUiZ2QZAyfY9ARKRcVYPAzM4ys0VmtsTMrtvGcseZWcHMLqpqeZKlwWK1CERESvoUBGb2KTMbYMGPzex5MzvzbdZJAjcBZwNHAX9tZkdtZbmvAQ9tf/G3jyXT4ZvFebUIRERK+toi+Dt33wScCQwHPg5c/zbrTAOWuPur7p4F7gDOr7DcPwF3A6v6WJYdZqkoCNQiEBHp0tcgsOj/OcBP3P2FsmlbMwpYVna/OZrWvVGzUcAHgZv7WI6dkigNFmuMQESkS1+DYLaZPUwIgofMrAl4u8PqSkHRew98I3Ctuxe2uSGzK8xslpnNWr16dR+LXGE7qUx00Tm1CERESlJ9XO7vgUnAq+7eZmZDCN1D29IMHFh2fzSwotcyU4E7zAxgGHCOmeXd/d7yhdz9FuAWgKlTp+7w4XwymSZlusSEiEi5vgbBicAcd281s48BU4Bvvc06M4FxZjYWWA5MBz5SvoC7jy3dNrP/AX7TOwR2pUQqDUAxn6vWQ4iI7HX62jX0faDNzI4B/gV4HfjZtlZw9zxwNeFsoIXAne4+38yuNLMrd6LMOyyRDEFQUBCIiHTpa4sg7+5uZucD33L3H5vZ377dSu4+A5jRa1rFgWF3v7SPZdlhFgVBsZCt9kOJiOw1+hoEm83sc8AlwLujc//T1StWlSTVNSQi0ltfu4YuBjoJ3yd4i3Aa6NerVqpqSYTcU4tARKRbn4Ig2vnfBgw0s/cDHe6+zTGCPVLUInC1CEREuvT1EhMfBv4M/BXwYeC5al8XqCoSURAUFAQiIiV9HSP4AnCcu68CMLPhwCPAXdUqWFV0DRYrCERESvo6RpAohUBk7Xasu+eIxghQ15CISJe+tgh+Z2YPAbdH9y+m12mhe4XSYHEx388FERHZc/QpCNz9s2Z2IXAS4RpCt7j7PVUtWTVEXUOoa0hEpEtfWwS4+92Ey0XvvaIWgQaLRUS6bTMIzGwzW14xFEKrwN19QFVKVS1JnTUkItLbNoPA3Zt2V0F2i+j0UVMQiIh02fvO/NkZpTECVxCIiJTEKwhKp48WdNaQiEhJvIIgahFYUS0CEZGSeAVBaYxA3yMQEekSsyBIAuAKAhGRLvEKgmQGgIS6hkREusQyCNQ1JCLSLWZBEMYI1CIQEekWsyCIuob0PQIRkS6xDIKkWgQiIl1iFgShayipFoGISJd4BYEZBUuRdA0Wi4iUxCsIgIKlSXqeYrHSRVVFROIndkFQTKRJkydbKPZ3UURE9gixDIIMeXIKAhERIIZB4KUWQV5BICICMQyCYiJD2tQ1JCJSUtUgMLOzzGyRmS0xs+sqzD/fzOaa2Rwzm2Vm76pmeSC0CDLkyOU1WCwiAtvx4/Xby8ySwE3AGUAzMNPM7nf3BWWL/QG4393dzCYCdwJHVKtMAJ7MkKFAtlCo5sOIiOw1qtkimAYscfdX3T0L3AGcX76Au7e4e+nQvAGo+mF6aYygU2MEIiJAdYNgFLCs7H5zNK0HM/ugmb0E/Bb4u0obMrMroq6jWatXr965UiUzpMmTK6hrSEQEqhsEVmHaFntfd7/H3Y8ALgD+b6UNufst7j7V3acOHz5850qVSofBYrUIRESA6gZBM3Bg2f3RwIqtLezuTwKHmtmwKpYJkhl9j0BEpEw1g2AmMM7MxppZBpgO3F++gJm9w8wsuj0FyABrq1gmLOoaUotARCSo2llD7p43s6uBh4AkcKu7zzezK6P5NwMXAn9jZjmgHbi4bPC4OpIaLBYRKVe1IABw9xnAjF7Tbi67/TXga9UsQ2+JVGmwWEEgIgIx/GaxpWpJW0FdQyIikRgGQRgs1iUmRESC2AWBuoZERHqKXxCkddaQiEi5qg4W74mSqRrQWUMiIl1iGAQZUpYnl9fvFouIQAy7hixTB0Ax19HPJRER2TPELghIRUGQbe/ngoiI7BliGAQ14b9aBCIiQByDIB1aBJ5XEIiIQByDIFULQCHb1s8FERHZM8QvCKIWQaFTYwQiIhDHIIjGCDynIBARgVgGgU4fFREpF78gSIcxAp01JCISxC8IohYBeXUNiYhALIMgjBGYTh8VEQHiGATRWUMUOvu3HCIie4j4BUH0PYKkWgQiIkAcgyBqEaQ9q98kEBEhjkGQSFEkQa1lac8W+rs0IiL9Ln5BYEYhWUMtWdpy+k0CEZH4BQFQTNZSQ04tAhER4hoEqXoarIM2BYGISEyDoGYATbSxqSPX30UREel3sQwCqx3AAGtjfauCQEQklkGQrB/EANpY16ovlYmIxDII0vWDaKKNdWoRiIhUNwjM7CwzW2RmS8zsugrzP2pmc6O/P5nZMdUsT0mibhADE22sb8vujocTEdmjVS0IzCwJ3AScDRwF/LWZHdVrsdeAU9x9IvB/gVuqVZ4eagfQSBvrWnSZCRGRarYIpgFL3P1Vd88CdwDnly/g7n9y9/XR3WeB0VUsT7fagSRw2ls27paHExHZk1UzCEYBy8ruN0fTtubvgQcrzTCzK8xslpnNWr169c6XrHYgAB0t63Z+WyIie7lqBoFVmOYVFzQ7jRAE11aa7+63uPtUd586fPjwnS9ZzQAA2jauw71ikUREYqOaQdAMHFh2fzSwovdCZjYR+BFwvruvrWJ5ujUMA6A2t44NbTpzSETirZpBMBMYZ2ZjzSwDTAfuL1/AzA4Cfg1c4u4vV7EsPQ0MQxGjbA1vrGvbbQ8rIrInqloQuHseuBp4CFgI3Onu883sSjO7Mlrs34ChwPfMbI6ZzapWeXoYMArHGGVreF1BICIxl6rmxt19BjCj17Sby25fBlxWzTJUlExD0/6M2rCWJatadvvDi4jsSWL5zWIAG3QQ78isZ95ynUIqIvEW2yBgyCEcwnJebN7Q3yUREelX8Q2CAyYzoLCOZMubLNM4gYjEWHyDYNSxAExKvMJTi9f0c2FERPpPfINg/6PxmgG8v3Yujy1a1d+lERHpN/ENglQNdtR5nMGzPLtoGRt0JVIRian4BgHAMX9NTbGN03wW33/8lf4ujYhIv4h3EBz0Thh0MF9uuIvHnn6al97a1N8lEhHZ7eIdBIkEfPinDEq083DmM8z8wT8ye+nuudyRiMieIt5BAHDAZBKXPULr2DO5xB9g8a2Xc/+c5f1dKhGR3UZBADD8MBr+5k46Jn2c6Yk/cNw97+Kr980mVyj2d8lERKpOQVBiRu37vkxx1FRG2jrOnn0FX7zhJl5dtbm/SyYiUlW2t/0wy9SpU33WrOpepNRn3krHY9+gri10EWUzA8mMPw/O/25VH1dEpFrMbLa7T600Ty2CCuy4v6PuE0/SNu48CiTIZDfCX/6X/MyfwOaV/V08EamWYgFWL+rvUux2CoKtaRhG/Uf/l+Il3b+lk/rtNRS/cyw8eC08cxOsWggLH4A1i/uxoCIxtGkFPPJl6NwFXbfLn4d7PwGFPDz8r3DTNFj/+s5vty8KeejdK7O1Xprf/jMs/E1ViqEgeBvpQ98NX97IAx94ngsL/8lzHQfBczfDQ5+H750Av/wYfHcqvPEsPPApmHtn98rusH4pPPVNaF/fPf21J+HR/9j6C74t2bZw1LInWPfqjtVhT7J5JbTs4ZcYKeR6Ps8tq2DVS1tffvnz4T1WyIfb5evmO8P9lQtg1q1QLMKffwgtqytv68mvw6NfhVxHWO/lh2Dda5Bt7bncyw/Bi3fB0j9uuY2OjXDLafDqE6Eu+Wz4v/j34XYl7rBhWajnxuXh4GvTm6G8xSL8/t/g6f+GG8bDfVeHz8SGZbD2lfBZe/UJaF3bc3vFAiyfDXdfBm/OheduCcv96lKY83NY9iz8+Zaw/Iu/6n7elj4Nv/k0/Neh8NaLYdri38Mz34PNb4X7614Ln02AzhZ4/mcw41/gh++Bx78WtvXANfD49SG8Fj8CHZvgW8fAVwaFUCsW4Z6r4LaLtvxcbVgGM38E66rzxVeNEWyHZevauOWJV1jx/G85wFdybfpO1g8az4Hrn+u54OAxMOQQeP0ZyLd3T3/He6GYh1cfD/fP+w4ceDwsmgEHTAlvwpXzYOCBcMBkGH4EtK+DQQfBwSfB4ofh3qvg2Evh/TeGN3YxD+nasD338OEYd0aYt//RYfqsW8PPcx54PNQP6VnWbBskkpDMgFnlihcL4MXwgz4QPmhPfB2evQne80U44RPw4GfhnZ+E4Yd3r7fqpXC/0nYXPQhD3wHDxlV+zJdmwH7jYfDB4f4jXw4ftlOvgxFHQvMsWHg/nPp5KHRC7cCorMXw/ZDe3LvL0bIaOjdBx4bwQQW44PvhQoSl8rvD3F+G8kUXKOS1pyBdByMnwcL7wut71PnhtXnhdjjk1PA8dm6CP34rvG7pOpjzCzj0PXDSp2DtEmhbC6sWwJBDYdyZ8NJvwhHuqGNh47Kwkx10UDiI8GI48NjvaDjklLDuzB+F/9N/AbN+Ao0jwnNiCVgxB168s0fVmfQxaFsDLStDAAwcFUK83MHvgot+DHNuCzupEz8RdppP3xDmN+4PEy4MrzlAsgbqBsFxl8OaRWHHWdI0Eo67DA4/O7z3NrwR3rsA9UND2UtGHRuew9k/hf0nhPfja0+G17TEEuF5AEjVQt1g2PwmJFLh/Q8w7n2w+KEtX/dEOrznW/rQpVs7KLwnSoYcCkedFz5TJam68NqsKes+GvqOEEADRsHoqbDg3rd/rL44YEp4rQ46EebfC81/hqtnbf0z8za2NUagINgBzevb+Pmzb/DUopXMf6uFybaYLwx8EK8fTu2AoUx4/eeQTGP7HRU+TOPeBw3Dw1FHF4NMQ/hgbOhDMzRdD7myy2U37h92GMVc2AGNeRc8/MWe6wwdB0PGdn8IIeyQDj8ntFL++vZwdLR2SXjTN40Mb7yzroeV8yHXHgLtf84NH/pxZ4ayLrivchkPORWm/E042nvqm7DmZXjf/wvlHjAaDjo+NLlfeRT+9G2oGxJ2jkufhiPOCR/yxv1g9v+EnTzA2JPhmI/AvVd2P85xl8PMH3bfT9WFHUWqJjp6LsLBJ8J53w1HUI/+BzTPDMsOHht2IvmOynU46vzwv7MFXvlDtP3a8OF760WwJAw9NNRtT9D7fVFu4EGw8Y3u+/VDw46x5a2+b//Q00MA/eHfu3e62zL8yHDEu6l528sl0mF7mQbItoSdfaoOcq1QMwAK2fAa1Q6CmiZoXRNe05ULQvkzjfBPs0P977yk++Cq3LDDQrftoe8JrZK1i0OINwyHeXdBw4hw0FbMh+fmrbnh/dayMoRRyeCxsP618FkYfVwIspGT4KAT4PH/DJ/D/SfCkkfCgVu565bB9QduWbYTrw5hXDsoHKBkGsJBxIBR4fNQNzh8bluj1moiBe84Az5yx9u/BluhIKiiVZs6+Nkzr/P4y6tYsqqFjlyRAbTQlmhkSEMNRwwqMnLEfowaUs/RLX+k4cCjGb36KeyQUxj+zH+Qeu2xsINd+jSc/Bk4/h9CM/D1P4YdzpBDw0754S/2PAqB8ME5+ER484WeR1m9ZRrDUX9Hr19ja9y/8k7BkuBR99PISfDmnK1v+/BzYdFv+/JU9XT0X4U6b36z8vyh7wgf8rfmhvt1Q+Cws2DZc2HnPvxIWL0wzCs/MqwbEj5UG5dtuc0R48MOqm5I6CKB7sef9JHQyuj9HB/6nnDkn0iFo+Q1L4cP6Lv/Twik33w6tJAyDd1HrS/9Bo54f3gt35oHA0aGboQVf4HxF4Swm3Bh6N5Y+lTYKYw7E352XtgBnHItvPTbsAN2YMKHQhdP29oQti/8IoxP1QyAc78JL9wRDjKGHQYf+FbYca5cEN4/betgY3M4ik3VhJbT5jdD6JuFea8/A3+8MZT92EthxFHwq7+FM78Kx1wcpq99JbRQDpgMm5aH2yvnQ8Ow0NKF0PI55drwPDz8xVDfYeNCl84ZX4F5d4ej6+m3h+CHENxvzQ1lT2ZCS6zUwi1X3qJb+0q4PeSQcL9YCK/L0HGQTIXPTyIJAw4ILYxMfeX32NYUC+H90b4udB2OOBLw0ApKbuPXfTc2wyuPhdd4+ezw2EecE8rburq7vC0ru1vr25JtDX9t66Bpv/De2AkKgt2kI1dgXWuW2a+vZ8Gbm1i5qYOla1p5eWULLZ1bHk2ZQX2ySDKVYXh9goOGD2RQfYY1LZ2MHlzPoPo0jTUpGmtSNNUkaaxJMDjRwcFLfkrr1KupTSepqW+iLgmZBXdC00iSww4NR91jTw4fnpaV4Q1U6mJ46pvh6GvCheGI9/Bzwof1sLPDGz+fDd0Ba1/p3tGe9oWwI1w+O+zkjvkIvPPqsPMbPCZ8cObfA8/9AE7+bGilFLLhcRY/DOM/GPpk35oLR10QWhdjTwk7lOd+AAdOC10cY06C5tlhp3zY+8L6c38ZwnDkxNDNUsiHJvLoafDSA2HHNurYcES47rWwbuOI0I3zu8/BAZPCYx78zrBD2FrXUUm2LYTgwgfCB/CdV0f9y/nurrHynVIlrzwGI4/Zshuut3zpqHdAuP/qE2Gn0zhi2+vtDm9Xxx3d5qoFoctPdjsFwR5gY1uOtlyeN9a2sbE9x4b2HMvXt9PSmadQdFZt7uCNdW2s2ZylLpNkY3uOTe058sXte30OHFJHbSpJvujUppM01aRoqElSX5Nic0eeQekCjRnD0/W0ZQs01KRwh9p0gtp0krp0ktp0gkwywYDcKizbSnK/I6hLJykWixy4/jmWDT6OproahjfW0Jot4O4Mqs+QSSXI5YskE8bIgbV05Its7sjRUJOiIZMiVyiSShjJhGHRTsbdK94WkV1rW0GwjXaO7EoD69MMJM3IgXXbtV5nvkBLR57NHXlaOvNs6sjR0pGnNZunI1ekI1egI1ckVyiSLxR5bW0b+UKRVDJBe7ZAS2eONS1ZWta2kTB4rVAkX3A2tW+gLpMi9D0YnbkCHfkCuUKl4PlL2e00MGe7659MGIWyUMskE6SSRjZfZFhjDXWZJG9ubKc2naQtW2C/ATXkC87Qxgxt2QLpRIL6miT5gtORK5BOJigUnbpMkvpMkqI79ZlUuJ9OAnSFYV10P5mAbL5IU204sh9UnyaZMFIJo6WzQENNEndIGHTkimRSCRpqUmRSCVZt6mBgXZqm2jSd+QLt2QIduQKd+SL7D6zFHYY31dAZPYel4EsnE7R05hjaWEMuX2R1SycjmmpprEnRmS+QzRfJFooUPaxTm06STBht2QI1qQQ1USiva81Sn0kxoC5F8/p2Rg6sZUNbjpp0aN3UpJIUis6A2hRFh1whbDdXKFKbSlKTTpA0Y0N7jmy+yICoHsMaazCjqxyb2vMMach0vU7ludwjoq30z7qWG1yfYXNHDoD6TIp8sUi+6F3PqZmRMEhEG02YYdF9i7bRli3Q2pmnviZFwsL7plS3zR053CFXLHY9V/likULRKb21ktE23WFzZ44h9RnWtWVJJxKkUwkKBSdfLOLA0IYMhaKTTBjZQthOfSZFoei058IBTjoZDpCKRWdzR55EIrw36jNJUkljY1uOEQNqyUfPd77o5AuOAQPr0rRm8zRkUiQS3Qc7pYO7QtGpSSXY1JHvOvjqfSDk7mQLRRIW3qfVOlBSEOzhalJJahqTDG2s2S2PVyiGHW02X6QzH/q839zYTirqTlnb2snQhho2d+ZoXtfO0MYMCTPWtHTiTtfOfcWGdhprUzRELZHX17ZyQBSCYSfl5Aqh9bCmpZNcwTnhkCGA0ViTZPmGdgzr+iDli0VaOwukkkZtKklrNk8qYWxsz7GuNcuA2jSrNnfQli3Q1hnGN9Ipoz0bwtLd6cwXSSWtq157WWM4ttJJo+j0OJDoq1IoVNJYk6KlM08yEQIqV3DqM0kMaM12n6I9sC7dFfqV1KQSFeeVHtuMrvdwR67ncqmEdQVDJpkgXyxS9J4HSoWoAo01KT75nnFcfvIh2/08vB0FgfSQTFjoyinLnf0HVhi8Azh095RpV+ndDbWpPU9nvkDRobE2RUeuQMKMfLFITSpJvlCkLTryH95Uw4a2HK3ZPLXpZPhLJcikEqzY0EGh6KxrzdJYmyKVMFqjHUypVbKpPUc6mWBYY4bmDe3kC05tOoFhrG/LMqKphvZcaE2UWgfZQmjtdeYLDKrL0JErsL4t13WkObAuzca20PWWLYQdV0tngaQZ6aSRSYWjzNZsgc58AffQCkpYKF9NOsmqTR0kzEinErR15hkcHSWH56jsucPLnsfStG6FQpF1rVkG1Yf1O/MFUskEqUT3EWzRw5G7e7jt7tHt6D6QSRoD69K0ZQs4kMsXaenMY2YMbwpvykwqQXs2T3s2BHtpRx4eozswisVwND2iqYZcwbuO/lNJoz1b4I11bQxrrCFbKNLamWf/gbWsbcmSzRc5cEhd14HI+tYsmVSCEU3hc1CbTtCeK9AWhUVrZ57GmjQ16VDfVCIE15qWzui1Cq36ZALqMinSUWETCWNTR47GTGjFtWXzpJMJLAqlUss+mYBCMWxv5KCtfBZ3koJAYqO8WW1mDKxPE7q6gsaaLT8OQ8tuD6rPbDEf4PD90xWnb824/Zq2a3mRatM3i0VEYk5BICIScwoCEZGYq2oQmNlZZrbIzJaY2XUV5h9hZs+YWaeZfaaaZRERkcqqNlhsZkngJuAMoBmYaWb3u/uCssXWAZ8ELqhWOUREZNuq2SKYBixx91fdPQvcAZxfvoC7r3L3mUCuiuUQEZFtqGYQjALKr/zVHE3bbmZ2hZnNMrNZq1dv5brpIiKyQ6oZBJW+C71D3+V091vcfaq7Tx0+fPhOFktERMpV8wtlzUD5hbhHAyt2dqOzZ89eY2Y7+jtyw4A1O1uGvYzqHA+qczzsTJ0P3tqMagbBTGCcmY0FlgPTgY/s7EbdfYebBGY2a2tX39tXqc7xoDrHQ7XqXLUgcPe8mV0NPAQkgVvdfb6ZXRnNv9nM9gdmAQOAopldAxzl7puqVS4REempqtcacvcZwIxe024uu/0WoctIRET6Sdy+WXxLfxegH6jO8aA6x0NV6rzX/UKZiIjsWnFrEYiISC8KAhGRmItNELzdBfD2VmZ2q5mtMrN5ZdOGmNnvzWxx9H9w2bzPRc/BIjN7X/+UeueY2YFm9piZLTSz+Wb2qWj6PltvM6s1sz+b2QtRnb8STd9n6wzhmmVm9hcz+010f5+uL4CZLTWzF81sjpnNiqZVt97e9ZNx++4f4fTVV4BDgAzwAuE01X4v2y6o28nAFGBe2bT/Aq6Lbl8HfC26fVRU9xpgbPScJPu7DjtQ55HAlOh2E/ByVLd9tt6Eb+o3RrfTwHPACftynaN6/B/gF8Bvovv7dH2juiwFhvWaVtV6x6VF8LYXwNtbufuThKu4ljsf+Gl0+6d0X931fOAOd+9099eAJYTnZq/i7m+6+/PR7c3AQsJ1rPbZenvQEt0t/camsw/X2cxGA+cCPyqbvM/W921Utd5xCYJddgG8vcR+7v4mhJ0mMCKavs89D2Y2BphMOELep+sddZPMAVYBv3f3fb3ONwL/AhTLpu3L9S1x4GEzm21mV0TTqlrvuPx4/S67AN5ebp96HsysEbgbuMbdN5X/OH3vRStM2+vq7e4FYJKZDQLuMbMJ21h8r66zmb0fWOXus83s1L6sUmHaXlPfXk5y9xVmNgL4vZm9tI1ld0m949IiqMoF8PZgK81sJED0f1U0fZ95HswsTQiB29z919Hkfb7eAO6+AXgcOIt9t84nAeeZ2VJCV+57zOzn7Lv17eLuK6L/q4B7CF09Va13XIKg6wJ4ZpYhXADv/n4uUzXdD/xtdPtvgfvKpk83s5roYoDjgD/3Q/l2ioVD/x8DC939hrJZ+2y9zWx41BLAzOqA9wIvsY/W2d0/5+6j3X0M4fP6qLt/jH20viVm1mBmTaXbwJnAPKpd7/4eId+NI/HnEM4ueQX4Qn+XZxfW63bgTcKvvDUDfw8MBf4ALI7+Dylb/gvRc7AIOLu/y7+DdX4Xofk7F5gT/Z2zL9cbmAj8JarzPODfoun7bJ3L6nEq3WcN7dP1JZzZ+EL0N7+0r6p2vXWJCRGRmItL15CIiGyFgkBEJOYUBCIiMacgEBGJOQWBiEjMKQhEdiMzO7V0JU2RPYWCQEQk5hQEIhWY2cei6//PMbMfRBd8azGzb5rZ82b2BzMbHi07ycyeNbO5ZnZP6VrxZvYOM3sk+g2B583s0GjzjWZ2l5m9ZGa32TYukiSyOygIRHoxsyOBiwkX/5oEFICPAg3A8+4+BXgC+FK0ys+Aa919IvBi2fTbgJvc/RjgnYRvgEO4Wuo1hGvJH0K4ro5Iv4nL1UdFtsfpwLHAzOhgvY5wka8i8MtomZ8DvzazgcAgd38imv5T4FfR9WJGufs9AO7eARBt78/u3hzdnwOMAZ6ueq1EtkJBILIlA37q7p/rMdHsX3stt63rs2yru6ez7HYBfQ6ln6lrSGRLfwAuiq4HX/q92IMJn5eLomU+Ajzt7huB9Wb27mj6JcAT7r4JaDazC6Jt1JhZ/e6shEhf6UhEpBd3X2BmXyT8SlSCcGXXTwCtwHgzmw1sJIwjQLgs8M3Rjv5V4OPR9EuAH5jZv0fb+KvdWA2RPtPVR0X6yMxa3L2xv8shsqupa0hEJObUIhARiTm1CEREYk5BICIScwoCEZGYUxCIiMScgkBEJOb+P3fn3KNy9B58AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(historic.history['loss'])\n",
    "plt.plot(historic.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dc9550ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(historic.history['accuracy'])\n",
    "plt.plot(historic.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e880ce51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.961\n"
     ]
    }
   ],
   "source": [
    "# Model Recall: what percentage of positive tuples are labelled as such?\n",
    "\n",
    "y_pred = np.around(model.predict(X_test))\n",
    "\n",
    "recall_average = recall_score(y_test,y_pred, average=\"micro\")\n",
    "\n",
    "print(\"Recall:\",recall_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3183293e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.961\n",
      "Precision: 0.9524340770791075\n"
     ]
    }
   ],
   "source": [
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Model Precision: what percentage of positive tuples are labeled as such?\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3c1fb455",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "import pandas as pd\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "conf_matrix = pd.DataFrame(conf_matrix, index=['stayed', 'left'], columns=['stayed', 'left'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4ce3de27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWcAAAD4CAYAAAAw/yevAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAd0ElEQVR4nO3de7xVVb338c+Xi1xUUo+XgxsUULyAFQapeUkNQbxrqQczQy1RsUw9laI+eUnSDK18PKJ4SfECYsoBbyV5evKxJEEhEZXklm5AECzFJC57/84fa25Y4GbttTZ7rzX35Pv2NV7MPeaca4yVu5+D3xxzDEUEZmaWLq0q3QEzM/s0B2czsxRycDYzSyEHZzOzFHJwNjNLoTZlaMPTQcysWNrcD1izbF7RMaftjj02u73mUo7gzJpl88rRjLUQbXfsAUCbraoq3BNLk7WrF1a6C6lSluBsZlY2tTWV7kGTcHA2s2ypWVvpHjQJB2czy5SI2kp3oUk4OJtZttQ6OJuZpY9HzmZmKeQHgmZmKeSRs5lZ+oRna5iZpZAfCJqZpZDTGmZmKeQHgmZmKeSRs5lZCvmBoJlZCvmBoJlZ+kQ452xmlj7OOZuZpZDTGmZmKeSRs5lZCtWsqXQPmoR33zazbKmtLb4UIKmrpN9LelPSLEnfS+p3kDRZ0tvJn9vn3TNc0hxJsyUdnVffV9LM5NxtkhrcWNbB2cyyJWqLL4WtBf4zIvYFDgIuktQLuAJ4PiJ6As8nP5OcGwz0BgYBd0hqnXzWKGAo0DMpgxpq3MHZzLKliUbOEbE4Il5NjlcAbwJVwEnAA8llDwAnJ8cnAeMiYlVEzAfmAAdI6gx0ioiXIiKAMXn3bJJzzmaWLc0wW0NSN2B/4M/ALhGxGHIBXNLOyWVVwJS826qTujXJ8cb1BTk4m1mmRAkPBCUNJZduqDM6IkZvdM02wOPAJRHxUYF0cX0nokB9QQ7OZpYtJUylSwLx6E2dl9SWXGB+OCKeSKqXSOqcjJo7A0uT+mqga97tXYBFSX2XeuoLcs7ZzLKl6WZrCLgXeDMibs07NQkYkhwPASbm1Q+W1E5Sd3IP/l5OUiArJB2UfOY38+7ZJI+czSxbmu4llEOAs4CZkmYkdVcCNwHjJX0LeAc4DSAiZkkaD7xBbqbHRbF+oY8LgfuBDsCzSSnIwdnMsqWJHghGxIvUny8G6L+Je0YAI+qpnwbsV0r7Ds5mli1+fdvMLIXWerF9M7P08cjZzCyFvGSomVkKeeRsZpZCHjmbmaWQR85mZink2RpmZikUDa4p1CI4OJtZtjjnbGaWQg7OZmYp5AeCZmYpVFPT8DUtgIOzmWWL0xpmZink4GxmlkIZyTl7myozy5SojaJLQyTdJ2mppNfz6h6VNCMpC+p2SZHUTdLKvHN35t3TV9JMSXMk3aYCu8TW8cjZzLKladMa9wO3A2PqKiLiP+qOJd0CfJh3/dyI6FPP54wit8v3FOAZYBANbFXlkbOZZUtNTfGlARHxAvBBfeeS0e/pwNhCn5Hs0N0pIl6KiCAX6E9uqG0HZzPLlhJ235Y0VNK0vDK0hJYOA5ZExNt5dd0lTZf0B0mHJXVVQHXeNdVJXUFOa5hZtpSQ1oiI0cDoRrZ0BhuOmhcDu0XEckl9gf+W1Jv6N4ltMOHt4LwZFi95nyt/PJJlH/ydVhKnnnQMZ51+MiNvv4c//PHPtGnbhq5VnbnhysvotO026+97byknfuN8hp17Jud8/VQA1qxZw4hb72Dq9Jm0krh46BAGHHlopb6aNaNWrVrx5ynPsmjhe5x0yhCuu/YHnHDCQGprg/eXLuPcb1/K4sVLKt3NlqsMCx9JagN8Fei7vtlYBaxKjl+RNBfYi9xIuUve7V2ARQ214bTGZmjTujU/+O55PPnIaB4Z/XPGPfEUc+f/jS99cX8mPHgnE8aMolvXKu558NEN7vvpbaM57KB+G9Td9cA4dth+O54edw8TH76Lfvt/tpxfxcro4u9+m7feWv834ZG3jOILfQfQ74sDefqZ33H1VZdWsHcZUEJaYzMcBbwVEevSFZJ2ktQ6Oe4B9ATmRcRiYIWkg5I89TeBiQ014OC8GXbacQd67b0nAFtv3ZEeu3dlyfvLOeTAvrRp0xqAz/XehyVLl6275/kX/kSXXf+dPbrvvsFnTXj6Ob59Vu4hcKtWrdh+u8+U6VtYOVVVdebYY/pz333r/za8YsXH64633rojkZElLyumNoovDZA0FngJ2FtStaRvJacG8+kHgV8GXpP0F+DXwAURUfcw8ULgHmAOMJcGZmpAA2kNSU9SIDcSESc21MCWYuHiJbz59lw+13vvDeonPP0cg/ofDsAnK//FfQ89xt2/+Am/Gvv4ums+Sv7PefvdY5g6/TW6VnXmysuGseMO25fvC1hZ3HrLdVwx/Aa2zUtzAfz4+sv5xpmn8uFHH3HUgNMq1LuMaMK1NSLijE3Un11P3ePA45++GiJiGrBfKW03NHIeCdwCzAdWAncn5WPg9U3dlP8EdPToxubaW45PPlnJpVfdwOUXn882W2+9rv6uB8bSunVrjh94JAD/de+DnPUfp9CxY4cN7q+pqWHJ0mXs/9lePPar2/n8fvsy8vZ7yvodrPkdd+xRLF26jFenz/zUuf/zo5/SfY8vMnbsBC4adk4FepcdUVtbdEmzgiPniPgDgKQfR8SX8049KemFAvflPwGNNcvmbXZH02rN2rVcctUNHDfwSAYccci6+onPTOaFP77MPbfdSN3LQDNnzWby71/k1jvuZcXH/0QS7bbaijO+dgId2rej/+EHAzDwyMN44snfVuT7WPM5+OB+nHD8QI4Z9BXat29Hp07b8sD9tzHk7IvXXTN23AQmTRzDddffUsGetnBFpCtagmJna+wkqUdEzAOQ1B3Yqfm61TJEBD+68Rf02L0rQwZ/dV39i1Omce/Dj3H/7TfToX37dfVjRo1cd/xf9z5Exw7t+fqpuczQ4YccyNTpr3Fg3z78edoM9ui+W/m+iJXFVVffxFVX3wTA4V/+EpddegFDzr6YPffszpw58wE44fiBzJ49t5LdbPkysrZGscH5UuD/SaobAncDzm+WHrUg01+bxZO/eZ6ee3Tja0MuAuB75w/hxl/cyeo1azjvkquA3EPBa3743YKfddmwcxl+/Uhu+uVd7LDdZ7jhysuavf+WDj8ZMZy99tqD2tpa3nlnIcMuuqLSXWrZMjJyVrFPhiW1A/ZJfnwrmdNXjEynNax0bXfsAUCbrRp8Scq2IGtXL4T6X9goyT9/NLjo6Lz19eM2u73mUtTIWVJH4DJg94g4T1JPSXtHxFPN2z0zsxJlJK1R7DznXwGrgS8lP1cDNzRLj8zMNkcTznOupGKD8x4RcTOwBiAiVtIEf/0wM2tqW8RUujyrJXUgeSFF0h4k75CbmaVKykfExSo2OF8L/AboKulh4BDg7Gbqk5lZ421JwTkinpP0CnAQuXTG9yJiWQO3mZmVXxO+vl1Jxc7W+DVwH/BsREYehZpZJhWzN2BLUOwDwTuBM4G3Jd0kaZ+GbjAzq4gtabZGRPwuIs4EvgAsACZL+pOkcyS1bc4OmpmVpDzrOTe7otdzlvRv5B4CfhuYDvySXLCe3Cw9MzNrjIyMnIvNOT9B7tXtB4ETkpX9AR6VNK25OmdmVrKUB91iFTtyvj0iekXEjXmBGYCI6Lepm8zMyi1qaosuDZF0n6Slkl7Pq7tW0kJJM5JybN654ZLmSJot6ei8+r6SZibnblPdOsIFFDuV7n8k7Qf0Atrn1Y8p5n4zs7Jp2pHz/cDtwMax7ucRMTK/QlIvcttX9QZ2BX4naa+IqAFGAUOBKcAzwCAa2Kqq2LTGNcAR5ILzM8AxwIv1dNjMrKKacipdRLwgqVuRl58EjEtW7JwvaQ5wgKQFQKeIeAlA0hjgZBoIzsWmNU4F+gPvRcQ5wOeBdkXea2ZWPiU8EMzfUi8pQ4ts5TuSXkvSHnWbfVYB7+ZdU53UVSXHG9cXVGxwXpm8fLJWUidgKdCjyHvNzMqntvgSEaMjol9eKWbT01HAHkAfYDG5fVah/sXgokB9QcWurTFN0nbkNnd9hdwGry8Xea+ZWdnE2uadvxwRS+qOJd0N1K1rXw10zbu0C7Aoqe9ST31Bxb6EMiwi/hERdwIDgCFJesPMLF1KGDk3hqTOeT+eAtTN5JgEDJbULtlntSfwcjLDbYWkg5JZGt8EJjbUTrEPBJ+PiP4AEbFg4zozs7RoygeCksaSmwyxo6Rq4BrgCEl9yKUmFpDspxoRsySNB94A1gIXJTM1AC4kN/OjA7kHgQUfBkIDwVlSe6Bj0rHtWZ876URuqoiZWbo0YVYjIs6op/reAtePAEbUUz8N2K+UthsaOZ8PXEIuEL9CLjgHsILc3D8zs1TZIlali4hfRkR3cv8l6JMc/wqYB7xUhv6ZmZWmmXPO5VL0POeI+EjSoeQeCN5PbjqJmVmqxNriS5oVG5zrktrHAXdGxERgq+bpkplZ40Vt8SXNig3OCyXdBZwOPCOpXQn3mpmVzxaW1jgd+C0wKCL+AewA/KC5OmVm1lhZGTkXuyrdJ8ATeT8vJvfaoplZqqQ96Bar2Ne3zcxahKhpcKnkFsHB2cwyxSNnM7MUilqPnM3MUscjZzOzFIrwyNnMLHU8cjYzS6Faz9YwM0sfPxA0M0uhrARnr49hZpkSUXxpSLK79lJJr+fV/UzSW8nu2xOS/VWR1E3SSkkzknJn3j19Jc2UNEfSbcl2VQU5OJtZpkStii5FuB8YtFHdZGC/iPgc8FdgeN65uRHRJykX5NWPAoaS21ewZz2f+SkOzmaWKREqujT8WfEC8MFGdc9FrFsNegob7qz9KcmGsJ0i4qWICGAMcHJDbTs4m1mm1NSo6CJpqKRpeWVoic2dy4abtXaXNF3SHyQdltRVAdV511QndQX5gaCZZUopL6FExGhgdGPakXQVuV22H06qFgO7RcRySX2B/5bUm/UbY2/QdEOf7+BsZplSjtkakoYAxwP9k1QFEbEKWJUcvyJpLrAXuZFyfuqjC7CooTac1jCzTGnK2Rr1kTQIuBw4MVnrvq5+J0mtk+Me5B78zUvWv18h6aBklsY3gYkNteORs5llSlOOnCWNBY4AdpRUDVxDbnZGO2ByMiNuSjIz48vA9ZLWktt39YKIqHuYeCG5mR8dyOWo8/PU9XJwNrNMqaltuoRARJxRT/W9m7j2ceDxTZybBuxXStsOzmaWKY1NV6SNg7OZZUqtlww1M0sfr+dcgrY79ihHM9bCrF29sNJdsAxyWsPMLIWc1ihBu/Zdy9GMtRCr/vUuAMO6nV7hnlia3LFgfJN8TlPO1qgkj5zNLFMyktVwcDazbHFaw8wshTxbw8wshTKy+baDs5llS9S7QmfL4+BsZpmy1mkNM7P08cjZzCyFnHM2M0uhrIycs/EqjZlZoraE0hBJ90laKun1vLodJE2W9Hby5/Z554ZLmiNptqSj8+r7SpqZnLst2RGlIAdnM8uUGlR0KcL9wKCN6q4Ano+InsDzyc9I6gUMBnon99xRt20VMAoYSm7rqp71fOanODibWabUqvjSkIh4Afhgo+qTgAeS4weAk/Pqx0XEqoiYD8wBDpDUGegUES8lm8GOybtnkxyczSxTalHRRdJQSdPyytAimtgl2bSV5M+dk/oq4N2866qTuqrkeOP6gvxA0MwypZSFjyJiNDC6iZqubyweBeoL8sjZzDKlKR8IbsKSJFVB8ufSpL4ayF8fuQuwKKnvUk99QQ7OZpYptVLRpZEmAUOS4yHAxLz6wZLaSepO7sHfy0nqY4Wkg5JZGt/Mu2eTnNYws0ypacLPkjQWOALYUVI1cA1wEzBe0reAd4DTACJilqTxwBvAWuCiiKjrzoXkZn50AJ5NSkEOzmaWKcXMwihWRJyxiVP9N3H9CGBEPfXTgP1KadvB2cwypTYjbwg6OJtZpnibKjOzFGrKtEYlOTibWaZ4VTozsxSq8cjZzCx9PHI2M0shB2czsxTKyBaCDs5mli0eOZuZpVBTvr5dSQ7OZpYpnudsZpZCTmuYmaWQg7OZWQp5bQ0zsxRyztnMLIWyMlvD21SZWabUEkWXQiTtLWlGXvlI0iWSrpW0MK/+2Lx7hkuaI2m2pKM353t45GxmmdJUDwQjYjbQB0BSa2AhMAE4B/h5RIzMv15SL2Aw0BvYFfidpL3ytqoqiUfOZpYpUUIpQX9gbkT8rcA1JwHjImJVRMwH5gAHlNj9dRyczSxTaksokoZKmpZXhm7iYwcDY/N+/o6k1yTdJ2n7pK4KeDfvmuqkrlEcnM0sU9Yqii4RMToi+uWV0Rt/nqStgBOBx5KqUcAe5FIei4Fb6i6tpzuNntnn4GxmmdIMaY1jgFcjYglARCyJiJqIqAXuZn3qohromndfF2BRY7+Hg7OZZUopaY0inUFeSkNS57xzpwCvJ8eTgMGS2knqDvQEXm7k1/BsDTPLloamyJVCUkdgAHB+XvXNkvqQG3wvqDsXEbMkjQfeANYCFzV2pgY4OJtZxjTl69sR8QnwbxvVnVXg+hHAiKZo28HZzDLFCx+ZmaVQTUaWPnJwNrNM8cjZzCyFwiNnM7P08cjZCtqrZw8eeuiOdT93774b119/C//39nsBuPSS87nppqvZtepzLF/+90p108qgQ6eOnHnTBey6d1eI4MEfjqL3EV/g8wP6URvBx8s+ZMz37+DDpX/niycdylHnn7ju3qp9duOm4y+n+o1CSzpYvqacSldJDs7N5K9vz+OAAwcB0KpVK+bPm8rESb8BoEuXzvTvfxh/e6e6kl20MjntmnN44w8zuGfYrbRu25qtOrRj8V+reerWRwE44uxjOPZ7pzL2qruZOvFFpk58EYBd9+7KBXf/0IG5RNkIzX5DsCy+8pVDmTf/b7zzzkIAfnbzNQy/cgQRWfk1sk1pv00H9jxgX/706P8AULOmhpUffcK/Pl657pp2HdvV+7vQ78RDmTbpj2Xra1asJYouaVbUyFlS92QJvIJ1Vr/TTjuR8Y9OBOD44wawaNF7zJz5ZoV7ZeWw42478/Hyjzhr5DC67Ls778ycx2PX3c/qlas48fuDOfCrX2blik/4xRnXferevsd/iTvP+1kFet2yZeWBYLEj58frqfv1pi7OX4Zv9OhPLfK0RWnbti3HHzeAx594mg4d2nP55d/luutvafhGy4RWrVvTdb/u/P+HnuPG4y5n9cpVDLzwZAAmjRzHVQcPY+rEFzl8yKAN7uvWZ09Wr1zN4r++W8+nWiHNsLZGRRQMzpL2kfQ14DOSvppXzgbab+q+/GX4hg7d1PKoW4ZBRx/JjBmvs3TpMnr06Ea3bl2ZOvW3zJ79J7pUdWbKlGfZZZedKt1Nayb/eG85/3hvOQtmzAHg1WemsNt+3Te4ZurEF9l/0IEb1PU94RCnNBopSvgnzRpKa+wNHA9sB5yQV78COK+Z+pQpp59+Eo+Oz6U0Zs16i6677b/u3OzZf+Lgg4/zbI0M++j9D/n7ouXs3KMzS+ctZp9DPsvit6vZqdu/8/6C9wD43FH9eG/u+pUlJfGFYw/i1tOvqVS3W7S0j4iL1VBwPjUizpJ0ZUT8pCw9ypAOHdrTv/9hXPSdKyrdFaug8dfexzm/uJg2bduw7N2ljPn+HXzjpxewS4/ORG3wwcJlPHLV+vTfngfuyz/eW87yd5dWsNctV01GHrSr0IwBSW+QW2h6EnAEG630HxEfFNFGtGvfteGrbIux6l+5POqwbqdXuCeWJncsGA/17yZSkq/vfkrR0fmRv03Y7PaaS0Mj5zuB3wA9gFc3OhdJvZlZaqQ9l1ysgsE5Im4DbpM0KiIuLFOfzMwaLSs556Km0kXEhZIOlXQOgKQdk21YzMxSpZYoujRE0gJJMyXNkDQtqdtB0mRJbyd/bp93/XBJcyTNlnT05nyPooKzpGuAy4HhSdVWwEOb07CZWXNohql0R0ZEn4jol/x8BfB8RPQEnk9+RlIvYDDQGxgE3CGpdWO/R7EvoZxCbmvwfwJExCJg28Y2ambWXGoiii6NdBLwQHL8AHByXv24iFiVvD09h/U7c5es2OC8OnLTOgJA0taNbdDMrDmVktbIf5s5KRu/NRfAc5JeyTu3S0QsBkj+3DmprwLyX+msTuoapdhV6cZLugvYTtJ5wLnA3Y1t1MysuZTyQDAiRgOF1pg4JCIWSdoZmCzprQLX1jctr9HD86KCc0SMlDQA+IjcW4M/iojJjW3UzKy5NOVUuiSFS0QslTSBXJpiiaTOEbFYUmeg7m2haiD/pY4uwCIaqeglQyNickT8ICK+78BsZmnVVLM1JG0tadu6Y2Ag8Dq5l/KGJJcNASYmx5OAwZLaJbPZegIvN/Z7FBw5S1pB/cNyARERnRrbsJlZc2jCddJ3ASZIglysfCQifiNpKrlU77eAd4DTknZnSRoPvAGsBS6KiJrGNt7QSyiekWFmLUpNE6U1ImIe8Pl66pcD/TdxzwhgRFO0722qzCxTvIegmVkKZWX7NwdnM8sUj5zNzFJoi1iVzsyspcnKYvsOzmaWKU5rmJmlkIOzmVkKebaGmVkKeeRsZpZCnq1hZpZCNZGNXQQdnM0sU5xzNjNLIeeczcxSyDlnM7MUqs1IWqPonVDMzFqCKOGfQiR1lfR7SW9KmiXpe0n9tZIWSpqRlGPz7hkuaY6k2ZKO3pzv4ZGzmWVKE87WWAv8Z0S8mmxX9Yqkui36fh4RI/MvltQLGAz0BnYFfidpr8buhuLgbGaZ0lRpjYhYDCxOjldIehOoKnDLScC4iFgFzJc0h9yGsC81pn2nNcwsU0pJa0gaKmlaXhla32dK6gbsD/w5qfqOpNck3Sdp+6SuCng377ZqCgfzghyczSxTaiOKLhExOiL65ZXRG3+epG2Ax4FLIuIjYBSwB9CH3Mj6lrpL6+lOo4fxTmuYWaY05VQ6SW3JBeaHI+IJgIhYknf+buCp5MdqoGve7V2ARY1t2yNnM8uUmqgpuhQiScC9wJsRcWtefee8y04BXk+OJwGDJbWT1B3oCbzc2O/hkbOZZUoTvr59CHAWMFPSjKTuSuAMSX3IpSwWAOcn7c6SNB54g9xMj4saO1MDHJzNLGOa6vXtiHiR+vPIzxS4ZwQwoinad3A2s0zxwkdmZimUlde3HZzNLFO88JGZWQp5sX0zsxRyztnMLIWcczYzSyGPnM3MUsjbVJmZpZBHzmZmKeTZGmZmKeQHgmZmKeS0hplZCvkNQTOzFMrKyFll+CLZ+F/KzMqhviU6S9Jmq6qiY87a1Qs3u73mUo7gbAlJQ+vbo8y2bP69sPp4m6ryqndnX9vi+ffCPsXB2cwshRyczcxSyMG5vJxXtPr498I+xQ8EzcxSyCNnM7MUcnA2M0shB+fNJOkSSR2buY0jJD3VnG1Y85L0cRHXXCzpTUkPSzpZUq9y9M3SycF5810CNGtwti3GMODYiDgTOBlwcN6COTiXQNLWkp6W9BdJr0u6BtgV+L2k3yfXjJI0TdIsSdcldf0lTcj7nAGSnkiOB0p6SdKrkh6TtE1SP0jSW5JeBL5a9i9rzUbSDyRNlfRa3u/InUAPYJKkq4ATgZ9JmiFpj0r21yrDszVKIOlrwKCIOC/5+TPAX4B+EbEsqdshIj6Q1Bp4HrgYmAm8CRwWEe9LegQYC7wEPAEcExH/lHQ50A64GXgb+AowB3gU6BgRx5fx61oTkvRxRGwjaSBwKnA+uXUkJgE3R8QLkhaQ/C5Juh94KiJ+XbFOW0V55FyamcBRkn4q6bCI+LCea06X9CowHegN9IrcfwEfBL4haTvgS8CzwEHk/ur6R0kzgCHA7sA+wPyIeDu596Fm/l5WPgOTMh14ldy/654V7ZGlkpcMLUFE/FVSX+BY4EZJz+Wfl9Qd+D7wxYj4ezL6aZ+c/hXwJPAv4LGIWCtJwOSIOGOjz+mDV/PLKgE3RsRdle6IpZtHziWQtCvwSUQ8BIwEvgCsALZNLukE/BP4UNIuwDF190bEImARcDVwf1I9BThE0p7J53eUtBfwFtA9L9e4QfC2Fu23wLl5zxaqJO1cz3X5v1e2BfLIuTSfJfeQphZYA1xIkqKQtDgijpQ0HZgFzAP+uNH9DwM7RcQbAEn++WxgrKR2yTVXJyP0ocDTkpYBLwL7NfeXs+YXEc9J2hd4KfcXJz4GvgEs3ejSccDdki4GTo2IueXtqVWaHwiWkaTbgekRcW+l+2Jm6ebgXCaSXiGX8hgQEasq3R8zSzcHZzOzFPIDQTOzFHJwNjNLIQdnM7MUcnA2M0shB2czsxT6X4BCymimZ6PqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(conf_matrix, annot=True, linewidths=1, fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "23a6b729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#roc_auc_score\n",
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, y_pred) #fpr = False Positive Rate, tpr = True Positive Rate\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.title('ُROC')\n",
    "plt.plot(fpr, tpr, label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1])\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b7fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
